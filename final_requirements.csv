Final_ID,Raw_Text,Req_Type,Hierarchy_ID
1,"Russell and Peter Norvig to be identified as the authors of this work have been asserted by them in accordance with the Copyright, Designs and Patents Act 1988.",Constraint,4
2,"No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, without either the prior written permission of the publisher or a license permitting restricted copying in the United Kingdom issued by the Copyright Licensing Agency Ltd, Saffron House, 6–10 Kirby Street, London EC1N 8TS.",Safety,1
3,"Each such agent im- plements a function that maps percept sequences to actions, and we cover different ways to represent these functions, such as reactive agents, real-time planners, and decision-theoretic systems.",Functional,1
4,"We explain the role of learning as extending the reach of the designer into unknown environments, and we show how that role constrains agent design, favoring explicit knowl- edge representation and reasoning.",Safety,4
5,We stress the importance of the task environment in determining the appropriate agent design.,Safety,4
6,"The book has 27 chapters, each requiring about a week’s worth of lectures, so working through the whole book requires a two-semester sequence.",Safety,4
7,Freshman calculus and linear algebra are useful for some of the topics; the required mathematical back- ground is supplied in Appendix A.,Safety,4
8,A number of exercises require some investigation of the literature; these are marked with a book icon.,Constraint,4
9,"About the Web site aima.cs.berkeley.edu, the Web site for the book, contains • implementations of the algorithms in the book in several programming languages, • a list of over 1000 schools that have used the book, many with links to online course materials and syllabi, • an annotated list of over 800 links to sites around the Web with useful AI content, • a chapter-by-chapter list of supplementary material and links, • instructions on how to join a discussion group for the book, NEW TERM Preface ix • instructions on how to contact the authors with questions or comments, • instructions on how to report errors in the book, in the likely event that some exist, and • slides and other materials for instructors.",Constraint,3
10,"Most helpful of all has been Julie Sussman, P.P.A., who read every chapter and provided extensive improvements.",Constraint,4
11,"He hopes that Gordon, Lucy, George, and Isaac will soon be reading this book after they have forgiven him for working so long on it.",Constraint,4
12,His other books are Paradigms of AI Programming: Case Studies in Common Lisp and Verbmobil: A Translation System for Face- to-Face Dialog and Intelligent Help Systems for UNIX.,Safety,1
13,12.5 Reasoning Systems for Categories .,Constraint,1
14,16.7 Decision-Theoretic Expert Systems .,Constraint,1
15,17.6 Mechanism Design .,Constraint,4
16,"A system is rational if it does the “right thing,” given what it knows.",Safety,1
17,"A human-centered approach must be in part an empirical science, in- 1 2 Chapter 1.",Interface,4
18,".” (Bellman, 1978) “The study of the computations that make it possible to perceive, reason, and act.” (Winston, 1992) Acting Humanly Acting Rationally “The art of creating machines that per- form functions that require intelligence when performed by people.” (Kurzweil, 1990) “The study of how to make computers do things at which, at the moment, people are better.” (Rich and Knight, 1991) “Computational Intelligence is the study of the design of intelligent agents.” (Poole et al., 1998) “AI .",Safety,3
19,"1.1.1 Acting humanly: The Turing Test approach The Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactory operational deﬁnition of intelligence.",Interface,4
20,"For now, we note that programming a computer to pass a rigorously applied test provides plenty to work on.",Constraint,4
21,The computer would need to possess the following capabilities: NATURAL LANGUAGE PROCESSING KNOWLEDGE REPRESENTATION AUTOMATED REASONING • natural language processing to enable it to communicate successfully in English; • knowledge representation to store what it knows or hears; • automated reasoning to use the stored information to answer questions and to draw new conclusions; MACHINE LEARNING • machine learning to adapt to new circumstances and to detect and extrapolate patterns.,Safety,4
22,Some systematic errors in human reasoning are cataloged by Kahneman et al.,Constraint,1
23,"However, the so-called total Turing Test includes a video signal so that the interrogator can test the subject’s perceptual abilities, as well as the opportunity for the interrogator to pass physical objects “through the hatch.” To pass the total Turing Test, the computer will need COMPUTER VISION ROBOTICS • computer vision to perceive objects, and • robotics to manipulate objects and move about.",Safety,4
24,"These six disciplines compose most of AI, and Turing deserves credit for designing a test that remains relevant 60 years later.",Constraint,4
25,"Aeronautical engineering texts do not deﬁne the goal of their ﬁeld as making “machines that ﬂy so exactly like pigeons that they can fool even other pigeons.” 1.1.2 Thinking humanly: The cognitive modeling approach If we are going to say that a given program thinks like a human, we must have some way of determining how humans think.",Safety,4
26,We will occasionally comment on similarities or differences between AI techniques and human cognition.,Constraint,4
27,"We will leave that for other books, as we assume the reader has only a computer for experimentation.",Safety,4
28,"His syllogisms provided patterns for argument structures that always yielded correct conclusions when given correct premises—for example, “Socrates is a man; all men are mortal; therefore, Socrates is mortal.” These laws of thought were supposed to govern the operation of the mind; their study initiated the ﬁeld called logic.",Safety,4
29,"(Contrast this with ordinary arithmetic notation, which provides only for statements about numbers.) By 1965, programs existed that could, in principle, solve any solvable problem described in logical notation.",Constraint,4
30,"(Although if no solution exists, the program might loop forever.) The so-called logicist tradition within artiﬁcial intelligence hopes to build on such programs to create intelligent systems.",Safety,1
31,"First, it is not easy to take informal knowledge and state it in the formal terms required by logical notation, particularly when the knowledge is less than 100% certain.",Safety,4
32,"Although both of these obstacles apply to any attempt to build computational reasoning systems, they appeared ﬁrst in the logicist tradition.",Safety,1
33,"Mak- ing correct inferences is sometimes part of being a rational agent, because one way to act rationally is to reason logically to the conclusion that a given action will achieve one’s goals and then to act on that conclusion.",Safety,4
34,"On the other hand, correct inference is not all of ration- ality; in some situations, there is no provably correct thing to do, but something must still be done.",Safety,4
35,Knowledge representation and reasoning enable agents to reach good decisions.,Constraint,4
36,"Second, it is more amenable to Section 1.2.",Constraint,4
37,"The standard of rationality is mathematically well deﬁned and completely general, and can be “unpacked” to generate agent designs that provably achieve it.",Safety,4
38,"We will see that despite the apparent simplicity with which the problem can be stated, an enormous variety of issues come up when we try to solve it.",Constraint,4
39,One important point to keep in mind: We will see before too long that achieving perfect rationality—always doing the right thing—is not feasible in complicated environments.,Safety,4
40,"For most of the book, however, we will adopt the working hypothesis that perfect rationality is a good starting point for analysis.",Safety,4
41,It simpliﬁes the problem and provides the appropriate setting for most of the foundational material in the ﬁeld.,Safety,4
42,"LIMITED RATIONALITY 1.2 THE FOUNDATIONS OF ARTIFICIAL INTELLIGENCE In this section, we provide a brief history of the disciplines that contributed ideas, viewpoints, and techniques to AI.",Interface,4
43,"He developed an informal system of syllogisms for proper reasoning, which in principle allowed one to gener- ate conclusions mechanically, given initial premises.",Interface,1
44,"Around 1500, Leonardo da Vinci (1452–1519) designed but did not build a me- chanical calculator; recent reconstructions have shown the design to be functional.",Safety,3
45,"In his 1651 book Leviathan, Thomas Hobbes suggested the idea of an “artiﬁcial animal,” arguing “For what is the heart but a spring; and the nerves, but so many strings; and the joints, but so many wheels.” It’s one thing to say that the mind operates, at least in part, according to logical rules, and to build physical systems that emulate some of those rules; it’s another to say that the mind itself is such a physical system.",Interface,1
46,"One problem with a purely physical conception of the mind is that it seems to leave little room for free will: if the mind is governed entirely by physical laws, then it has no more free will than a rock “deciding” to fall toward the center of the earth.",Constraint,4
47,Free will is simply the way that the perception of available choices appears to the choosing entity.,Safety,4
48,This question is vital to AI because intelligence requires action as well as reasoning.,Constraint,4
49,"For a doctor does not deliberate whether he shall heal, nor an orator whether he shall persuade, .",Interface,4
50,"They assume the end and consider how and by what means it is attained, and if it seems easily and best produced thereby; while if it is achieved by one means only they consider how it will be achieved by this and by what means this will be achieved, till they come to the ﬁrst cause, .",Safety,4
51,Aristotle’s algorithm was implemented 2300 years later by Newell and Simon in their GPS program.,Constraint,3
52,We would now call it a regression planning system (see Chapter 10).,Safety,1
53,"Goal-based analysis is useful, but does not say what to do when several actions will achieve the goal or when no action will achieve it completely.",Safety,4
54,"Philosophers staked out some of the fundamental ideas of AI, but the leap to a formal science required a level of mathematical formalization in three fundamental areas: logic, computa- tion, and probability.",Safety,4
55,This motivated Alan Turing (1912–1954) to try to characterize exactly which functions are com- putable—capable of being computed.,Constraint,3
56,"However, the Church–Turing thesis, which states that the Turing machine (Turing, 1936) is capable of computing any computable function, is generally accepted as providing a sufﬁcient deﬁnition.",Functional,3
57,"For example, no machine can tell in general whether a given program will return an answer on a given input or run forever.",Interface,4
58,"Roughly speaking, a problem is called intractable if the time required to solve instances of the problem grows exponentially with the size of the instances.",Constraint,4
59,"Therefore, one should strive to divide the overall problem of generating intelligent behavior into tractable subproblems rather than intractable ones.",Safety,4
60,"The theory of NP-completeness, pio- neered by Steven Cook (1971) and Richard Karp (1972), provides a method.",Safety,3
61,"The Foundations of Artiﬁcial Intelligence 9 problems are necessarily intractable, most theoreticians believe it.) These results contrast with the optimism with which the popular press greeted the ﬁrst computers—“Electronic Super-Brains” that were “Faster than Einstein!” Despite the increasing speed of computers, careful use of resources will characterize intelligent systems.",Constraint,1
62,Bayes’ rule underlies most modern approaches to uncertain reasoning in AI systems.,Constraint,1
63,1.2.3 Economics • How should we make decisions so as to maximize payoff?,Constraint,4
64,• How should we do this when others may not go along?,Constraint,4
65,• How should we do this when the payoff may be far in the future?,Safety,4
66,"Most people think of economics as being about money, but economists will say that they are really studying how people make choices that lead to preferred outcomes.",Constraint,4
67,"When McDonald’s offers a hamburger for a dollar, they are asserting that they would prefer the dollar and hoping that customers will prefer the hamburger.",Safety,4
68,"Decision theory, which combines probability theory with utility theory, provides a for- mal and complete framework for decisions (economic or otherwise) made under uncertainty— that is, in cases where probabilistic descriptions appropriately capture the decision maker’s environment.",Constraint,1
69,Introduction a rational agent should adopt policies that are (or least appear to be) randomized.,Constraint,4
70,"Since the 1990s, there has been a resurgence of interest in decision-theoretic techniques for agent systems (Wellman, 1995).",Safety,1
71,"Neuroscience is the study of the nervous system, particularly the brain.",Safety,1
72,"Although the exact way in which the brain enables thought is one of the great mysteries of science, the fact that it does enable thought has been appreciated for thousands of years because of the evidence that strong blows to the head can lead to mental incapacitation.",Safety,4
73,The signals control brain activity in the short term and also enable long-term changes in the connectivity of neurons.,Safety,4
74,"(It should be noted, however, that the brain does not seem to use all of its neurons simultaneously.) Futurists make much of these numbers, pointing to an approaching singularity at which computers reach a superhuman level of performance (Vinge, 1993; Kurzweil, 2005), but the raw comparisons are not especially informative.",Interface,4
75,The Foundations of Artiﬁcial Intelligence 13 COGNITIVE PSYCHOLOGY that introspection could not provide reliable evidence.,Interface,4
76,"Cognitive psychology, which views the brain as an information-processing device, can be traced back at least to the works of William James (1842–1910).",Interface,4
77,"Craik speciﬁed the three key steps of a knowledge-based agent: (1) the stimulus must be translated into an inter- nal representation, (2) the representation is manipulated by cognitive processes to derive new internal representations, and (3) these are in turn retranslated back into action.",Interface,4
78,"He clearly explained why this was a good design for an agent: If the organism carries a “small-scale model” of external reality and of its own possible actions within its head, it is able to try out various alternatives, conclude which is the best of them, react to future situations before they arise, utilize the knowledge of past events in dealing with the present and future, and in every way to react in a much fuller, safer, and more competent manner to the emergencies which face it.",Constraint,4
79,"(We shall see that this is just two months after the conference at which AI itself was “born.”) At the workshop, George Miller presented The Magic Number Seven, Noam Chomsky presented Three Models of Language, and Allen Newell and Herbert Simon presented The Logic Theory Machine.",Safety,4
80,"It is now a common (although far from universal) view among psychologists that “a cognitive theory should be like a computer program” (Anderson, 1980); that is, it should describe a detailed information- processing mechanism whereby some cognitive function might be implemented.",Functional,3
81,Current expectations are that future increases in power will come from massive parallelism—a curious convergence with the properties of the brain.,Constraint,4
82,"In the mid-19th century, Charles Babbage (1792–1871) designed two machines, neither of which he com- pleted.",Safety,4
83,"AI also owes a debt to the software side of computer science, which has supplied the operating systems, programming languages, and tools needed to write modern programs (and papers about them).",Safety,1
84,"Other examples of self-regulating feedback control systems include the steam engine governor, created by James Watt (1736–1819), and the thermostat, invented by Cornelis Drebbel (1572–1633), who also invented the submarine.",Safety,1
85,The mathematical theory of stable feedback systems was developed in the 19th century.,Safety,1
86,"Wiener was a brilliant mathematician who worked with Bertrand Rus- sell, among others, before developing an interest in biological and mechanical control systems and their connection to cognition.",Constraint,1
87,"Like Craik (who also used control systems as psychological models), Wiener and his colleagues Arturo Rosenblueth and Julian Bigelow challenged the behaviorist orthodoxy (Rosenblueth et al., 1943).",Safety,1
88,"Ashby, Alan Turing, Grey Walter, and others formed the Ratio Club for “those who had Wiener’s ideas before Wiener’s book appeared.” Ashby’s Design for a Brain (1948, 1952) elaborated on his idea that intelligence could be created by the use of homeostatic devices containing appro- priate feedback loops to achieve stable adaptive behavior.",Safety,4
89,"Modern control theory, especially the branch known as stochastic optimal control, has as its goal the design of systems that maximize an objective function over time.",Functional,1
90,This roughly matches our view of AI: designing systems that behave optimally.,Constraint,1
91,"Calculus and matrix algebra, the tools of control theory, lend themselves to systems that are describable by ﬁxed sets of continuous variables, whereas AI was founded in part as a way to escape from the these perceived limitations.",Safety,1
92,"Understanding language requires an understanding of the subject matter and context, not just an understanding of the structure of sentences.",Interface,4
93,"The state of a neuron was conceived of as “factually equivalent to a proposition which proposed its adequate stimulus.” They showed, for example, that any computable function could be computed by some network of connected neurons, and that all the logical connectives (and, or, not, etc.) could be implemented by simple net structures.",Functional,3
94,committee was skeptical about whether this kind of work should be considered HEBBIAN LEARNING Section 1.3.,Constraint,4
95,"The History of Artiﬁcial Intelligence 17 mathematics, but von Neumann reportedly said, “If it isn’t now, it will be someday.” Minsky was later to prove inﬂuential theorems showing the limitations of neural network research.",Safety,4
96,"An attempt will be made to ﬁnd how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.",Interface,4
97,"Although the others had ideas and in some cases programs for particular appli- cations such as checkers, Newell and Simon already had a reasoning program, the Logic Theorist (LT), about which Simon claimed, “We have invented a computer program capable of thinking non-numerically, and thereby solved the venerable mind–body problem.”12 Soon after the workshop, the program was able to prove most of the theorems in Chapter 2 of Rus- 10 This was the ﬁrst ofﬁcial usage of McCarthy’s term artiﬁcial intelligence.",Safety,4
98,"AI is the only one of these ﬁelds that is clearly a branch of computer science (although operations research does share an emphasis on computer simulations), and AI is the only ﬁeld to attempt to build machines that will function autonomously in complex, changing environments.",Functional,3
99,"Unlike Logic Theorist, this program was designed from the start to imitate human problem-solving protocols.",Safety,4
100,"The success of GPS and subsequent pro- grams as models of cognition led Newell and Simon (1976) to formulate the famous physical symbol system hypothesis, which states that “a physical symbol system has the necessary and sufﬁcient means for general intelligent action.” What they meant is that any system (human or machine) exhibiting intelligence must operate by manipulating data structures composed of symbols.",Safety,1
101,We will see later that this hypothesis has been challenged from many directions.,Constraint,4
102,"Along the way, he disproved the idea that comput- PHYSICAL SYMBOL SYSTEM Section 1.3.",Safety,1
103,"Also in 1958, McCarthy published a paper entitled Programs with Common Sense, in which he described the Advice Taker, a hypothetical program that can be seen as the ﬁrst complete AI system.",Constraint,1
104,"Like the Logic Theorist and Geometry Theorem Prover, McCarthy’s program was designed to use knowledge to search for solutions to problems.",Safety,4
105,"For example, he showed how some simple axioms would enable the program to generate a plan to drive to the airport.",Safety,4
106,"The program was also designed to accept new axioms in the normal course of operation, thereby allowing it to achieve competence in new areas without being reprogrammed.",Safety,4
107,"Applications of logic included Cordell Green’s question-answering and planning systems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute (SRI).",Safety,1
108,Minsky supervised a series of students who chose limited problems that appeared to require intelligence to solve.,Constraint,4
109,"The perceptron convergence theorem (Block et al., 1962) says that the learning algorithm can adjust the connection strengths of a perceptron to match any input data, provided such a match exists.",Safety,3
110,The History of Artiﬁcial Intelligence 21 their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.,Constraint,4
111,Simon’s overconﬁdence was due to the promising performance of early AI systems on simple examples.,Performance,1
112,"In almost all cases, however, these early systems turned out to fail miserably when tried out on wider selections of problems and on more difﬁcult problems.",Constraint,1
113,The fact is that accurate translation requires background knowledge in order to resolve ambiguity and establish the content of the sentence.,Safety,4
114,The famous retranslation of “the spirit is willing but the ﬂesh is weak” as “the vodka is good but the meat is rotten” illustrates the difﬁculties en- countered.,Safety,4
115,"For example, Minsky and Papert’s book Percep- trons (1969) proved that, although perceptrons (a simple form of neural network) could be shown to learn anything they were capable of representing, they could represent very little.",Constraint,4
116,1.3.5 Knowledge-based systems: The key to power?,Safety,1
117,"It was developed at Stanford, where Ed Feigenbaum (a former student of Herbert Simon), Bruce Buchanan (a philosopher turned computer scientist), and Joshua Lederberg (a Nobel laureate geneticist) teamed up to solve the problem of inferring molecular structure from the information provided by a mass spectrometer.",Safety,4
118,The History of Artiﬁcial Intelligence 23 EXPERT SYSTEMS CERTAINTY FACTOR (b) x1 − 28 is a high peak; (c) x2 − 28 is a high peak; (d) At least one of x1 and x2 is high.,Safety,1
119,Later systems also incorporated the main theme of McCarthy’s Advice Taker approach—the clean separation of the knowledge (in the form of rules) from the reasoning component.,Safety,1
120,"With this lesson in mind, Feigenbaum and others at Stanford began the Heuristic Pro- gramming Project (HPP) to investigate the extent to which the new methodology of expert systems could be applied to other areas of human expertise.",Constraint,1
121,"Although Winograd’s SHRDLU system for understanding natural language had engendered a good deal of excitement, its dependence on syntactic analysis caused some of the same problems as occurred in the early machine translation work.",Safety,1
122,"It was able to overcome ambiguity and understand pronoun references, but this was mainly because it was designed speciﬁcally for one area—the blocks world.",Safety,4
123,"Several researchers, including Eugene Charniak, a fellow graduate student of Winograd’s at MIT, suggested that robust language understanding would require general knowledge about the world and a general method for using that knowledge.",Safety,3
124,"The emphasis, however, was less on language per se and more on the problems of representing and reasoning with the knowledge required for language under- standing.",Constraint,4
125,"1.3.6 AI becomes an industry (1980–present) The ﬁrst successful commercial expert system, R1, began operation at the Digital Equipment Corporation (McDermott, 1982).",Interface,1
126,"The program helped conﬁgure orders for new computer systems; by 1986, it was saving the company an estimated $40 million a year.",Interface,1
127,"By 1988, DEC’s AI group had 40 expert systems deployed, with more on the way.",Constraint,1
128,corporation had its own AI group and was either using or investigating expert systems.,Constraint,1
129,"In response, the United States formed the Microelec- tronics and Computer Technology Corporation (MCC) as a research consortium designed to assure national competitiveness.",Safety,4
130,"In both cases, AI was part of a broad effort, including chip design and human-interface research.",Interface,2
131,"Overall, the AI industry boomed from a few million dollars in 1980 to billions of dollars in 1988, including hundreds of companies building expert systems, vision systems, robots, and software and hardware specialized for these purposes.",Safety,1
132,"These so-called connectionist models of intelligent systems were seen by some as di- rect competitors both to the symbolic models promoted by Newell and Simon and to the logicist approach of McCarthy and others (Smolensky, 1988).",Safety,1
133,"It might seem obvious that at some level humans manipulate symbols—in fact, Terrence Deacon’s book The Symbolic 13 To save embarrassment, a new ﬁeld called IKBS (Intelligent Knowledge-Based Systems) was invented because Artiﬁcial Intelligence had been ofﬁcially canceled.",Safety,1
134,"There is a recognition that machine learning should not be isolated from information theory, that uncertain reasoning should not be isolated from stochastic modeling, that search should not be isolated from classical optimization and control, and that automated reasoning should not be isolated from formal methods and static analysis.",Constraint,3
135,"To be ac- cepted, hypotheses must be subjected to rigorous empirical experiments, and the results must be analyzed statistically for their importance (Cohen, 1995).",Safety,4
136,"This ensures that the performance is robust, and in rigorous blind tests the HMMs have been improving their scores steadily.",Performance,4
137,"Speech technology and the related ﬁeld of handwritten character recognition are already making the transition to widespread industrial 14 Some have characterized this change as a victory of the neats—those who think that AI theories should be grounded in mathematical rigor—over the scrufﬁes—those who would rather try out lots of ideas, write some programs, and then assess what seems to be working.",Safety,4
138,Whether that stability will be disrupted by a new scruffy idea is another question.,Constraint,4
139,"Note that there is no scientiﬁc claim that humans use HMMs to recognize speech; rather, HMMs provide a mathematical framework for understanding the problem and support the engineering claim that they work well in practice.",Interface,1
140,"Judea Pearl’s (1988) Probabilistic Reasoning in Intelligent Systems led to a new accep- tance of probability and decision theory in AI, following a resurgence of interest epitomized by Peter Cheeseman’s (1985) article “In Defense of Probability.” The Bayesian network formalism was invented to allow efﬁcient representation of, and rigorous reasoning with, uncertain knowledge.",Constraint,1
141,This approach largely overcomes many problems of the probabilistic reasoning systems of the 1960s and 1970s; it now dominates AI research on uncertain reason- ing and expert systems.,Safety,1
142,"Work by Judea Pearl (1982a) and by Eric Horvitz and David Heckerman (Horvitz and Heckerman, 1986; Horvitz et al., 1986) promoted the idea of normative expert systems: ones that act rationally according to the laws of decision theory and do not try to imitate the thought steps of human experts.",Safety,1
143,The WindowsTM operating sys- tem includes several normative diagnostic expert systems for correcting problems.,Safety,1
144,AI systems have become so common in Web-based applications that the “-bot” sufﬁx has entered everyday language.,Safety,1
145,"The History of Artiﬁcial Intelligence 27 HUMAN LEVEL AI ARTIFICIAL GENERAL INTELLIGENCE FRIENDLY AI Internet tools, such as search engines, recommender systems, and Web site aggregators.",Safety,1
146,"In particular, it is now widely appreciated that sensory systems (vision, sonar, speech recognition, etc.) cannot deliver perfectly reliable information about the environment.",Safety,1
147,"Hence, reasoning and planning systems must be able to handle uncertainty.",Constraint,1
148,"They think that AI should put less emphasis on creating ever-improved versions of applications that are good at a spe- ciﬁc task, such as driving a car, playing chess, or recognizing speech.",Constraint,1
149,"Instead, they believe AI should return to its roots of striving for, in Simon’s words, “machines that think, that learn and that create.” They call the effort human-level AI or HLAI; their ﬁrst symposium was in 2004 (Minsky et al., 2004).",Safety,4
150,The effort will require very large knowledge bases; Hendler et al.,Safety,4
151,"Guaranteeing that what we create is really Friendly AI is also a concern (Yudkowsky, 2008; Omohundro, 2008), one we will return to in Chapter 26.",Constraint,4
152,"Yarowsky showed that the task can be done, with accuracy above 96%, with no labeled examples at all.",Constraint,4
153,Hays and Efros deﬁned an algorithm that searches through a collection of photos to ﬁnd something that will match.,Interface,3
154,"Work like this suggests that the “knowledge bottleneck” in AI—the problem of how to express all the knowledge that a system needs—may be solved in many applications by learn- ing methods rather than hand-coded knowledge engineering, provided the learning algorithms have enough data to go on (Halevy et al., 2009).",Safety,1
155,Speech recognition: A traveler calling United Airlines to book a ﬂight can have the en- tire conversation guided by an automated speech recognition and dialog management system.,Interface,1
156,These are just a few examples of artiﬁcial intelligence systems that exist today.,Constraint,1
157,"Not magic or science ﬁction—but rather science, engineering, and mathematics, to which this book provides an introduction.",Interface,4
158,"• Mathematicians provided the tools to manipulate statements of logical certainty as well as uncertain, probabilistic statements.",Safety,4
159,• Computer engineers provided the ever-more-powerful machines that make AI applica- tions possible.,Safety,4
160,"• Control theory deals with designing devices that act optimally on the basis of feedback Initially, the mathematical tools of control theory were quite from the environment.",Constraint,4
161,There have also been cycles of introducing new creative approaches and systematically reﬁning the best ones.,Safety,1
162,• Recent progress in understanding the theoretical basis for intelligence has gone hand in hand with improvements in the capabilities of real systems.,Constraint,1
163,These articles usually provide a good entry point into the research literature on each topic.,Interface,4
164,"The major journals for general AI are Artiﬁcial Intelligence, Computational Intelligence, the IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE In- telligent Systems, and the electronic Journal of Artiﬁcial Intelligence Research.",Safety,1
165,Introduction 1.7 To what extent are the following computer systems instances of artiﬁcial intelligence: • Supermarket bar code scanners.,Safety,1
166,"What sense does it make to say that the “vision system” is doing this kind of mathematics, whereas the actual person has no idea how to do it?",Safety,1
167,"For the currently infeasible tasks, try to ﬁnd out what the difﬁculties are and predict when, if ever, they will be overcome.",Safety,4
168,We will see that the concept of rationality can be applied to a wide variety of agents operating in any imaginable environ- ment.,Safety,4
169,Our plan in this book is to use this concept to develop a small set of design principles for building successful agents—systems that can reasonably be called intelligent.,Constraint,1
170,We give a crude categorization of environments and show how properties of an environment inﬂuence the design of suitable agents for that environment.,Interface,4
171,"We describe a number of basic “skeleton” agent designs, which we ﬂesh out in the rest of the book.",Safety,4
172,"Internally, the agent function for an artiﬁcial agent will be implemented by an agent program.",Interface,3
173,"The agent function is an abstract mathematical description; the agent program is a concrete implementation, running within some physical system.",Functional,1
174,A partial tabulation of this agent function is shown in Figure 2.3 and an agent program that implements it appears in Figure 2.8 on page 48.,Functional,3
175,"Partial tabulation of a simple agent function for the vacuum-cleaner world Before closing this section, we should emphasize that the notion of an agent is meant to be a tool for analyzing systems, not an absolute characterization that divides the world into agents and non-agents.",Interface,1
176,"In a sense, all areas of engineering can be seen as designing artifacts that interact with the world; AI operates at (what the authors consider to be) the most interesting end of the spectrum, where the artifacts have signiﬁcant computational resources and the task environment requires nontrivial decision making.",Constraint,4
177,"Obviously, there is not one ﬁxed performance measure for all tasks and agents; typically, a designer will devise one appropriate to the circumstances.",Performance,4
178,"As a general rule, it is better to design performance measures according to what one actually wants in the environment, rather than according to how one thinks the agent should behave.",Performance,4
179,"This leads to a deﬁnition of a rational agent: DEFINITION OF A RATIONAL AGENT For each possible percept sequence, a rational agent should select an action that is ex- pected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.",Performance,4
180,"For example, once all the dirt is cleaned up, the agent will oscillate needlessly back and forth; if the performance measure includes a penalty of one point for each movement left or right, the agent will fare poorly.",Performance,4
181,"If clean squares can become dirty again, the agent should occasionally check and re-clean them if needed.",Safety,4
182,"If the geography of the environment is un- known, the agent will need to explore it rather than stick to squares A and B.",Safety,4
183,Exercise 2.1 asks you to design agents for these cases.,Constraint,4
184,Retreating from a requirement of perfection is not just a question of being fair to agents.,Constraint,4
185,"The point is that if we expect an agent to do what turns out to be the best action after the fact, it will be impossible to design an agent to fulﬁll this speciﬁcation—unless we improve the performance of crystal balls or time machines.",Interface,4
186,"Good Behavior: The Concept of Rationality 39 INFORMATION GATHERING EXPLORATION LEARNING AUTONOMY Our deﬁnition of rationality does not require omniscience, then, because the rational choice depends only on the percept sequence to date.",Safety,4
187,We must also ensure that we haven’t inadvertently allowed the agent to engage in decidedly underintelligent activities.,Safety,4
188,"For exam- ple, if an agent does not look both ways before crossing a busy road, then its percept sequence will not tell it that there is a large truck approaching at high speed.",Interface,4
189,"Second, a rational agent should choose the “looking” action before stepping into the street, because looking helps maximize the expected performance.",Performance,4
190,A second example of information gathering is provided by the exploration that must be undertaken by a vacuum-cleaning agent in an initially unknown environment.,Interface,4
191,Our deﬁnition requires a rational agent not only to gather information but also to learn as much as possible from what it perceives.,Constraint,4
192,"The female sphex will dig a burrow, go out and sting a caterpillar and drag it to the burrow, enter the burrow again to check all is well, drag the caterpillar inside, and lay its eggs.",Safety,4
193,"So far so good, but if an entomologist moves the caterpillar a few inches away while the sphex is doing the check, it will revert to the “drag” step of its plan and will continue the plan without modiﬁcation, even after dozens of caterpillar-moving interventions.",Interface,4
194,"The sphex is unable to learn that its innate plan is failing, and thus will not change it.",Safety,4
195,"To the extent that an agent relies on the prior knowledge of its designer rather than on its own percepts, we say that the agent lacks autonomy.",Interface,4
196,A rational agent should be autonomous—it should learn what it can to compensate for partial or incorrect prior knowl- edge.,Constraint,4
197,"For example, a vacuum-cleaning agent that learns to foresee where and when additional dirt will appear will do better than one that does not.",Constraint,4
198,"As a practical matter, one seldom re- quires complete autonomy from the start: when the agent has had little or no experience, it would have to act randomly unless the designer gave some assistance.",Safety,4
199,"So, just as evolution provides animals with enough built-in reﬂexes to survive long enough to learn for themselves, it would be reasonable to provide an artiﬁcial intelligent agent with some initial knowledge as well as an ability to learn.",Interface,4
200,"Hence, the incorporation of learning allows one to design a single rational agent that will succeed in a vast variety of environments.",Safety,4
201,"First, however, we must think about task environments, which are essen- tially the “problems” to which rational agents are the “solutions.” We begin by showing how to specify a task environment, illustrating the process with a number of examples.",Constraint,4
202,The ﬂavor of the task environment directly affects the appropriate design for the agent program.,Safety,4
203,"In designing an agent, the ﬁrst step must always be to specify the task environment as fully as possible.",Interface,4
204,"We should point out, before the reader becomes alarmed, that a fully automated taxi is currently somewhat beyond the capabilities of existing technology.",Safety,4
205,"Obviously, some of these goals conﬂict, so tradeoffs will be required.",Constraint,4
206,"Next, what is the driving environment that the taxi will face?",Safety,4
207,"Any taxi driver must deal with a variety of roads, ranging from rural lanes and urban alleys to 12-lane freeways.",Constraint,4
208,The taxi must also interact with potential and actual passengers.,Constraint,4
209,"Obviously, the more restricted the environment, the easier the design problem.",Safety,4
210,"In addition, it will need output to a display screen or voice synthesizer to talk back to the passengers, and perhaps some way to communicate with other vehicles, politely or otherwise.",Constraint,4
211,The basic sensors for the taxi will include one or more controllable video cameras so that it can see the road; it might augment these with infrared or sonar sensors to detect dis- tances to other cars and obstacles.,Constraint,3
212,"To avoid speeding tickets, the taxi should have a speedome- ter, and to control the vehicle properly, especially on curves, it should have an accelerometer.",Interface,4
213,"To determine the mechanical state of the vehicle, it will need the usual array of engine, fuel, and electrical system sensors.",Safety,1
214,"Like many human drivers, it might want a global positioning system (GPS) so that it doesn’t get lost.",Safety,1
215,"Finally, it will need a keyboard or microphone for the passenger to request a destination.",Safety,4
216,"For example, a robot designed to inspect parts as they come by on a conveyor belt can make use of a number of simplifying assumptions: that the lighting is always just so, that the only thing on the conveyor belt will be parts of a kind that it knows about, and that only two actions (accept or reject) are possible.",Safety,4
217,"Imagine a softbot Web site operator designed to scan Internet news sources and show the interesting items to its users, while selling advertising space to generate revenue.",Safety,4
218,"To do well, that operator will need some natural language processing abilities, it will need to learn what each user and advertiser is interested in, and it will need to change its plans dynamically—for example, when the connection for one news source goes down or when a new one comes online.",Safety,4
219,"These dimensions determine, to a large extent, the appropriate agent design and the applicability of each of the principal families of techniques for agent implementation.",Safety,4
220,"Intelligent Agents Agent Type Performance Measure Environment Actuators Sensors Medical diagnosis system Healthy patient, reduced costs Patient, hospital, staff Display of questions, tests, diagnoses, treatments, referrals Keyboard entry of symptoms, ﬁndings, patient’s answers Satellite image analysis system Correct image categorization Downlink from orbiting satellite Display of scene categorization Color pixel arrays Part-picking robot Percentage of parts in correct bins Conveyor belt with parts; bins Jointed arm and hand Camera, joint angle sensors Reﬁnery controller Purity, yield, safety Reﬁnery, operators Valves, pumps, heaters, displays Interactive English tutor Student’s score on test Set of students, testing agency Display of exercises, suggestions, corrections Figure 2.5 Examples of agent types and their PEAS descriptions.",Safety,1
221,The deﬁnitions here are informal; later chapters provide more precise statements and examples of each kind of environment.,Interface,4
222,"First, we have described how an entity may be viewed as an agent, but we have not explained which entities must be viewed as agents.",Interface,4
223,"The agent-design problems in multiagent environments are often quite different from those in single-agent en- vironments; for example, communication often emerges as a rational behavior in multiagent environments; in some competitive environments, randomized behavior is rational because it avoids the pitfalls of predictability.",Safety,4
224,"Most real situations are so complex that it is impossible to keep track of all the unobserved aspects; for practical purposes, they must be treated as stochastic.",Safety,4
225,Nonde- terministic environment descriptions are usually associated with performance measures that require the agent to succeed for all possible outcomes of its actions.,Constraint,4
226,"unknown: Strictly speaking, this distinction refers not to the environment itself but to the agent’s (or designer’s) state of knowledge about the “laws of physics” of the environment.",Safety,4
227,"Obviously, if the environment is unknown, the agent will have to learn how it works in order to make good decisions.",Safety,4
228,"batch of defective parts, the robot should learn from several observations that the distribution of defects has changed, and should modify its behavior for subsequent parts.",Safety,4
229,"We have listed the medical-diagnosis task as single-agent because the disease process in a patient is not proﬁtably modeled as an agent; but a medical-diagnosis system might also have to deal with recalcitrant patients and skeptical staff, so the environment could have a multiagent aspect.",Constraint,1
230,"If we designed the agent for a single scenario, we might be able to take advantage of speciﬁc properties of the particular case but might not identify a good design for driving in general.",Safety,4
231,Now we must bite the bullet and talk about how the insides work.,Safety,4
232,The job of AI is to design an agent program that implements the agent function— the mapping from percepts to actions.,Functional,3
233,We assume this program will run on some sort of computing device with physical sensors and actuators—we call this the architecture: AGENT PROGRAM ARCHITECTURE agent = architecture + program .,Constraint,3
234,"Most of this book is about designing agent programs, although Chapters 24 and 25 deal directly with the sensors and actuators.",Constraint,3
235,"2.4.1 Agent programs The agent programs that we design in this book all have the same skeleton: they take the current percept as input from the sensors and return an action to the actuators.4 Notice the difference between the agent program, which takes the current percept as input, and the agent function, which takes the entire percept history.",Functional,3
236,"The agent program takes just the current percept as input because nothing more is available from the environment; if the agent’s actions need to depend on the entire percept sequence, the agent will have to remember the percepts.",Safety,4
237,"(The online code repository contains implementations in real programming languages.) For example, Figure 2.7 shows a rather trivial agent program that keeps track of the percept sequence and then uses it to index into a table of actions to decide what to do.",Safety,4
238,"The TABLE-DRIVEN-AGENT program is invoked for each new percept and this way, we as designers must construct a table that contains the appropriate action for every possible percept sequence.",Safety,4
239,Let P be the set of possible percepts and let T be the lifetime of the agent (the total number of percepts it will receive).,Safety,4
240,The lookup table will contain PT t = 1 |P|t entries.,Safety,4
241,"Despite all this, TABLE-DRIVEN-AGENT does do what we want: it implements the desired agent function.",Functional,3
242,"In the remainder of this section, we outline four basic kinds of agent programs that embody the principles underlying almost all intelligent systems: • Simple reﬂex agents; • Model-based reﬂex agents; • Goal-based agents; and • Utility-based agents.",Safety,1
243,This program implements the agent function tabulated in Figure 2.3.,Functional,3
244,This variety provides a major organizing principle for the ﬁeld and for the book itself.,Safety,4
245,"If the car in front brakes and its brake lights come on, then you should notice this and initiate braking.",Safety,4
246,"In the course of the book, we show several different ways in which such connections can be learned and implemented.",Safety,4
247,The Structure of Agents 49 Agent Condition-action rules Sensors What the world is like now What action I should do now Actuators E n v i r o n m e n t Figure 2.9 Schematic diagram of a simple reﬂex agent.,Safety,3
248,Note that the description in terms of “rules” and “matching” is purely conceptual; actual implementations can be as simple as a collection of logic gates implementing a Boolean circuit.,Safety,4
249,"The agent in Figure 2.10 will work only if the correct decision can be made on the basis of only the current percept—that is, only if the environment is fully observ- able.",Safety,4
250,It can Suck in response to [Dirty]; what should it do in response to [Clean]?,Constraint,4
251,It is easy to show that the agent will reach the other square in an average of two steps.,Interface,4
252,"Then, if that square is dirty, the agent will clean it and the task will be complete.",Safety,4
253,"That is, the agent should maintain some sort of internal state that depends on the percept history and thereby reﬂects at least some of the unobserved aspects of the current state.",Safety,4
254,"For other driving tasks such as changing lanes, the agent needs to keep track of where the other cars are if it can’t see them all at once.",Safety,4
255,"And for any driving to be possible at all, the agent needs to keep track of where its keys are.",Safety,4
256,Updating this internal state information as time goes by requires two kinds of knowl- edge to be encoded in the agent program.,Safety,4
257,"First, we need some information about how the world evolves independently of the agent—for example, that an overtaking car generally will be closer behind than it was a moment ago.",Interface,4
258,This knowledge about “how the world works”—whether implemented in simple Boolean circuits or in complete scientiﬁc theories—is called a model of the world.,Safety,4
259,The Structure of Agents 51 State How the world evolves What my actions do Sensors What the world is like now E n v i r o n m e n t Condition-action rules Agent What action I should do now Actuators Figure 2.11 A model-based reﬂex agent.,Safety,3
260,The details of how models and states are represented vary widely depending on the type of environment and the particular technology used in the agent design.,Safety,4
261,"Intelligent Agents State How the world evolves What my actions do Goals Agent Sensors What the world is like now What it will be like if I do action A What action I should do now Actuators E n v i r o n m e n t A model-based, goal-based agent.",Safety,3
262,"It keeps track of the world state as well as Figure 2.13 a set of goals it is trying to achieve, and chooses an action that will (eventually) lead to the achievement of its goals.",Interface,4
263,"Sometimes it will be more tricky—for example, when the agent has to consider long sequences of twists and turns in order to ﬁnd a way to achieve the goal.",Safety,4
264,"Notice that decision making of this kind is fundamentally different from the condition– action rules described earlier, in that it involves consideration of the future—both “What will happen if I do such-and-such?” and “Will that make me happy?” In the reﬂex agent designs, this information is not explicitly represented, because the built-in rules map directly from GOAL Section 2.4.",Safety,4
265,"A goal-based agent, in principle, could reason that if the car in front has its brake lights on, it will slow down.",Safety,4
266,"Given the way the world usually evolves, the only action that will achieve the goal of not hitting other cars is to brake.",Safety,4
267,"If it starts to rain, the agent can update its knowledge of how effectively its brakes will operate; this will automatically cause all of the relevant behaviors to be altered to suit the new conditions.",Safety,4
268,The reﬂex agent’s rules for when to turn and when to go straight will work only for a single destination; they must all be replaced to go somewhere new.,Safety,4
269,"For example, many action sequences will get the taxi to its destination (thereby achieving the goal) but some are quicker, safer, more reliable, or cheaper than others.",Safety,4
270,Goals just provide a crude binary distinction between “happy” and “unhappy” states.,Interface,4
271,A more general performance measure should allow a comparison of different world states according to exactly how happy they would make the agent.,Performance,4
272,"If the internal utility function and the external performance measure are in agreement, then an agent that chooses actions to maximize its utility will be rational according to the external performance measure.",Performance,3
273,"Second, when there are several goals that the agent can aim for, none of which can be achieved with certainty, utility provides a way in which the likelihood of success can be weighed against the importance of the goals.",Constraint,4
274,"Intelligent Agents State How the world evolves What my actions do Utility Agent Sensors What the world is like now What it will be like if I do action A How happy I will be in such a state What action I should do now Actuators E n v i r o n m e n t A model-based, utility-based agent.",Safety,3
275,"(Appendix A deﬁnes expectation more precisely.) In Chapter 16, we show that any rational agent must behave as if it possesses a utility function whose expected value it tries to maximize.",Functional,3
276,"In this way, the “global” deﬁnition of rationality—designating as rational those agent functions that have the highest performance—is turned into a “local” constraint on rational-agent designs that can be expressed in a simple program.",Performance,3
277,"Utility-based agent programs appear in Part IV, where we design decision-making agents that must handle the uncertainty inherent in stochastic or partially observable environments.",Safety,4
278,"In many areas of AI, this is now the preferred method for creating state-of-the-art systems.",Safety,1
279,The learning element uses feedback from the critic on how the agent is doing and determines how the performance element should be modiﬁed to do better in the future.,Performance,4
280,The design of the learning element depends very much on the design of the performance element.,Performance,4
281,"When trying to design an agent that learns a certain capability, the ﬁrst question is not “How am I going to get it to learn this?” but “What kind of performance element will my agent need to do this once it has learned how?” Given an agent design, learning mechanisms can be constructed to improve every part of the agent.",Interface,4
282,The critic is necessary because the percepts themselves provide no indication of the agent’s success.,Interface,4
283,"Conceptually, one should think of it as being outside the agent altogether because the agent must not modify it to ﬁt its own behavior.",Safety,4
284,It is responsible for suggesting actions that will lead to new and informative experiences.,Constraint,4
285,"But if the agent is willing to explore a little and do some perhaps suboptimal actions in the short run, it might discover much better actions for the long run.",Safety,4
286,"To make the overall design more concrete, let us return to the automated taxi example.",Safety,4
287,"Observation of pairs of successive states of the environ- ment can allow the agent to learn “How the world evolves,” and observation of the results of its actions can allow the agent to learn “What my actions do.” For example, if the taxi exerts a certain braking pressure when driving on a wet road, then it will soon ﬁnd out how much deceleration is actually achieved.",Safety,4
288,The external performance standard must inform the agent that the loss of tips is a negative contribution to its overall performance; then the agent might be able to learn that violent maneuvers do not contribute to its own utility.,Performance,4
289,"In a sense, the performance standard distinguishes part of the incoming percept as a reward (or penalty) that provides direct feedback on the quality of the agent’s behavior.",Performance,4
290,"2.4.7 How the components of agent programs work We have described agent programs (in very high-level terms) as consisting of various compo- nents, whose function it is to answer questions such as: “What is the world like now?” “What action should I do now?” “What do my actions do?” The next question for a student of AI is, “How on earth do these components work?” It takes about a thousand pages to begin to answer that question properly, but here we want to draw the reader’s attention to some basic distinctions among the various ways that the components can represent the environment that the agent inhabits.",Functional,3
291,"To illustrate these ideas, it helps to consider a particular agent component, such as the one that deals with “What my actions do.” This component describes the changes that might occur in the environment as the result of taking an action, and Figure 2.16 provides schematic depictions of how those transitions might be represented.",Constraint,4
292,"Often, the more expressive language is much more concise; for example, the rules of chess can be written in a page or two of a structured-representation language such as ﬁrst-order logic but require thousands of pages when written in a factored-representation language such as propositional logic.",Safety,4
293,"To gain the beneﬁts of expressive representations while avoiding their drawbacks, intelligent systems for the real world may need to operate at all points along the axis simultaneously.",Safety,1
294,"Summary 2.5 SUMMARY 59 This chapter has been something of a whirlwind tour of AI, which we have conceived of as the science of agent design.",Safety,4
295,"• A task environment speciﬁcation includes the performance measure, the external en- In designing an agent, the ﬁrst step must vironment, the actuators, and the sensors.",Performance,3
296,• The agent program implements the agent function.,Functional,3
297,There exists a variety of basic agent-program designs reﬂecting the kind of information made explicit and used in the decision process.,Safety,4
298,"The designs vary in efﬁciency, compactness, and ﬂexibility.",Safety,4
299,The appropriate design of the agent program depends on the nature of the environment.,Safety,4
300,"Perhaps sur- prisingly, AI has concentrated for most of its history on isolated components of agents— question-answering systems, theorem-provers, vision systems, and so on—rather than on whole agents.",Constraint,1
301,"A paper by Jon Doyle (1983) predicted that rational agent design would come to be seen as the core mission of AI, while other popular topics would spin off to form new disciplines.",Safety,4
302,"Careful attention to the properties of the environment and their consequences for ra- tional agent design is most apparent in the control theory tradition—for example, classical control systems (Dorf and Bishop, 2004; Kirk, 2004) handle fully observable, deterministic environments; stochastic optimal control (Kumar and Varaiya, 1986; Bertsekas and Shreve, 2007) handles partially observable, stochastic environments; and hybrid control (Henzinger and Sastry, 1998; Cassandras and Lygeros, 2006) deals with environments containing both discrete and continuous elements.",Constraint,1
303,"Most work in AI views the idea of pure reﬂex agents with state as too simple to provide much leverage, but work by Rosenschein (1985) and Brooks (1986) questioned this assumption (see Chapter 25).",Constraint,4
304,"It has also inﬁltrated the area of operating systems, where autonomic computing refers to computer systems and networks that monitor and con- trol themselves with a perceive–act loop and machine learning methods (Kephart and Chess, 2003).",Constraint,1
305,Noting that a collection of agent programs designed to work well together in a true multiagent environment necessarily exhibits modularity—the programs share no internal state and communicate with each other only through the environment—it is common within the ﬁeld of multiagent systems to design the agent program of a single agent as a collection of autonomous sub-agents.,Constraint,1
306,"In some cases, one can even prove that the resulting system gives the same optimal solutions as a monolithic design.",Safety,1
307,This theory has been inﬂuential both in AUTONOMIC COMPUTING MULTIAGENT SYSTEMS Exercises 61 natural language understanding and multiagent systems.,Constraint,1
308,"The general design for learning agents portrayed in Figure 2.15 is classic in the machine learning literature (Buchanan et al., 1978; Mitchell, 1997).",Safety,4
309,"Examples of the design, as em- bodied in programs, go back at least as far as Arthur Samuel’s (1959, 1967) learning program for playing checkers.",Safety,4
310,"Interest in agents and in agent design has risen rapidly in recent years, partly because of the growth of the Internet and the perceived need for automated and mobile softbot (Etzioni and Weld, 1994).",Safety,4
311,"Texts on multiagent systems usually provide a good introduction to many aspects of agent design (Weiss, 2000a; Wooldridge, 2002).",Interface,1
312,"Several conference series devoted to agents began in the 1990s, including the International Workshop on Agent Theories, Architectures, and Languages (ATAL), the International Conference on Autonomous Agents (AGENTS), and the International Confer- ence on Multi-Agent Systems (ICMAS).",Safety,1
313,"In 2002, these three merged to form the International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS).",Safety,1
314,The journal Autonomous Agents and Multi-Agent Systems was founded in 1998.,Safety,1
315,"Finally, Dung Beetle Ecology (Hanski and Cambefort, 1991) provides a wealth of interesting information on the behavior of dung beetles.",Safety,4
316,Does the corresponding agent program require internal state?,Safety,4
317,Discuss possible agent designs for the cases in which clean squares can become dirty and the geography of the environment is unknown.,Safety,4
318,"If so, what should it learn?",Constraint,4
319,Every agent function is implementable by some program/machine combination.,Functional,3
320,Can there be more than one agent program that implements a given agent function?,Functional,3
321,Are there agent functions that cannot be implemented by any agent program?,Constraint,3
322,"Given a ﬁxed machine architecture, does each agent program implement exactly one agent function?",Functional,3
323,Exercises 63 The following exercises all concern the implementation of environments and agents for the vacuum-cleaner world.,Safety,4
324,Implement a performance-measuring environment simulator for the vacuum-cleaner 2.9 world depicted in Figure 2.2 and speciﬁed on page 38.,Performance,4
325,"Your implementation should be modu- lar so that the sensors, actuators, and environment characteristics (size, shape, dirt placement, etc.) can be changed easily.",Safety,3
326,"(Note: for some choices of programming language and operating system there are already implementations in the online code repository.) 2.10 Consider a modiﬁed version of the vacuum environment in Exercise 2.9, in which the agent is penalized one point for each movement.",Safety,1
327,Design such an agent.,Interface,4
328,Design such an agent and measure its performance on several environments.,Performance,4
329,Can you design an environment in which your randomized agent will perform poorly?,Functional,4
330,Design such an agent and measure its performance on several environments.,Performance,4
331,Can you design a rational agent of this type?,Constraint,4
332,Suppose the bump sensor stops working; how should the agent behave?,Safety,3
333,Can you come up with a rational agent design for this case?,Constraint,4
334,3 SOLVING PROBLEMS BY SEARCHING PROBLEM SOLVING AGENT In which we see how an agent can ﬁnd a sequence of actions that achieves its goals when no single action will do.,Interface,4
335,We will see several uninformed search algorithms—algorithms that are given no information about the problem other than its deﬁnition.,Safety,3
336,Readers unfamiliar with these concepts should consult Appendix A.,Constraint,4
337,Goals help organize behavior by limiting the objectives that the agent is trying to achieve and hence the actions it needs to consider.,Safety,4
338,We will consider a goal to be a set of world states—exactly those states in which the goal is satisﬁed.,Safety,4
339,"Before it can do this, it needs to decide (or we need to decide on its behalf) what sorts of actions and states it should consider.",Constraint,4
340,"For now, let us assume that the agent will consider actions at the level of driving from one major town to another.",Safety,4
341,"None of these achieves the goal, so unless the agent is familiar with the geography of Romania, it will not know which road to follow.1 In other words, the agent will not know which of its possible actions is best, because it does not yet know enough about the state that results from taking each action.",Constraint,4
342,The point of a map is to provide the agent with information about the states it might get itself into and the actions it can take.,Constraint,4
343,"We will assume the environment is known, so the agent knows which states are reached by each action.",Safety,4
344,"Fortunately, if the agent knows the initial state and the environment is known and deterministic, it knows exactly where it will be after the ﬁrst action and what it will perceive.",Safety,4
345,"Thus, we have a simple “formulate, search, execute” design for the agent, as shown in Figure 3.1.",Safety,4
346,"Once the solution has been executed, the agent will formulate a new goal.",Safety,4
347,Notice that while the agent is executing the solution sequence it ignores its percepts when choosing an action because it knows in advance what they will be.,Interface,4
348,"An agent that carries out its plans with its eyes closed, so to speak, must be quite certain of what is going on.",Constraint,4
349,"Control theorists call this an open-loop system, because ignoring the percepts breaks the loop between agent and environment.",Interface,1
350,"In addition to abstracting the state description, we must abstract the actions themselves.",Safety,4
351,• Initial state: Any state can be designated as the initial state.,Safety,4
352,• Initial state: Any state can be designated as the initial state.,Safety,4
353,"Knuth conjectured that, starting with the number 4, a sequence of fac- torial, square root, and ﬂoor operations will reach any desired positive integer.",Constraint,4
354,"Some, such as Web sites and in-car systems that provide driving directions, are relatively straightforward extensions of the Romania example.",Interface,1
355,"Others, such as routing video streams in computer networks, military operations planning, and airline travel-planning systems, involve much more complex speciﬁcations.",Constraint,1
356,"Consider the airline travel problems that must be solved by a travel-planning Web site: • States: Each state obviously includes a location (e.g., an airport) and the current time.",Interface,4
357,"Furthermore, because the cost of an action (a ﬂight segment) may depend on previous segments, their fare bases, and their status as domestic or international, the state must record extra information about these “historical” aspects.",Interface,4
358,• Transition model: The state resulting from taking a ﬂight will have the ﬂight’s desti- nation as the current location and the ﬂight’s arrival time as the current time.,Safety,4
359,"Solving Problems by Searching TOURING PROBLEM TRAVELING SALESPERSON PROBLEM VLSI LAYOUT ROBOT NAVIGATION AUTOMATIC ASSEMBLY SEQUENCING Commercial travel advice systems use a problem formulation of this kind, with many addi- tional complications to handle the byzantine fare structures that airlines impose.",Constraint,1
360,A really good system should include contingency plans—such as backup reservations on alternate ﬂights— to the extent that these are justiﬁed by the cost and likelihood of failure of the original plan.,Safety,1
361,Each state must include not just the current location but also the set of cities the agent has visited.,Safety,4
362,The traveling salesperson problem (TSP) is a touring problem in which each city must be visited exactly once.,Safety,4
363,"A VLSI layout problem requires positioning millions of components and connections on a chip to minimize area, minimize circuit delays, minimize stray capacitances, and max- imize manufacturing yield.",Constraint,4
364,The layout problem comes after the logical design phase and is usually split into two parts: cell layout and channel routing.,Safety,4
365,Each cell has a ﬁxed footprint (size and shape) and requires a certain number of connections to each of the other cells.,Safety,4
366,"Later in this chapter, we present some algorithms capable of solving them.",Constraint,3
367,"When the robot has arms and legs or wheels that must also be controlled, the search space becomes many-dimensional.",Safety,4
368,Advanced techniques are required just to make the search space ﬁnite.,Safety,4
369,"In addition to the complexity of the problem, real robots must also deal with errors in their sensor readings and motor controls.",Constraint,3
370,"If the wrong order is chosen, there will be no way to add some part later in the sequence without Section 3.3.",Safety,4
371,Any practical algorithm must avoid exploring all but a tiny fraction of the state space.,Safety,3
372,"Another important assembly problem is protein design, in which the goal is to ﬁnd a sequence of amino acids that will fold into a three-dimensional protein with the right properties to cure some disease.",Constraint,4
373,"PROTEIN DESIGN 3.3 SEARCHING FOR SOLUTIONS SEARCH TREE NODE EXPANDING GENERATING PARENT NODE CHILD NODE LEAF NODE FRONTIER OPEN LIST SEARCH STRATEGY REPEATED STATE LOOPY PATH Having formulated some problems, we now need to solve them.",Constraint,2
374,Now we must choose which of these three possibilities to consider further.,Constraint,4
375,The eagle-eyed reader will notice one peculiar thing about the search tree shown in Fig- ure 3.6: it includes the path from Arad to Sibiu and back to Arad again!,Safety,4
376,The speciﬁc algorithms in this chapter draw on this general design.,Safety,3
377,"As every step moves a state from the frontier into the explored region while moving some states from the unexplored region into the frontier, we see that the algorithm is systematically examining the states in the state space, one by one, until it ﬁnds a solution.",Safety,1
378,3.3.1 Infrastructure for search algorithms Search algorithms require a data structure to keep track of the search tree that is being con- structed.,Safety,3
379,The frontier needs to be stored in such a way that the search algorithm can easily choose the next node to expand according to its preferred strategy.,Safety,2
380,The explored set can be implemented with a hash table to allow efﬁcient checking for repeated states.,Constraint,4
381,"With a good implementation, insertion and lookup can be done in roughly constant time no matter how many states are stored.",Constraint,4
382,One must take care to implement the hash table with the right notion of equality between states.,Constraint,4
383,"For example, in the traveling salesperson problem (page 74), the hash table needs to know that the set of visited cities {Bucharest,Urziceni,Vaslui} is the same as {Urziceni,Vaslui,Bucharest}.",Safety,4
384,"Sometimes this can be achieved most easily by insisting that the data structures for states be in some canonical form; that is, logically equivalent states should map to the same data structure.",Safety,4
385,"3.3.2 Measuring problem-solving performance Before we get into the design of speciﬁc search algorithms, we need to consider the criteria that might be used to choose among them.",Performance,3
386,"For these reasons, complexity is expressed in terms of three quantities: b, the branching factor or maximum number of successors of any node; d, the depth of the shallowest goal node (i.e., the number of steps along the path from the root); and m, the maximum length of any path in the state space.",Safety,2
387,This enables the agent to ﬁnd an optimal tradeoff point at which further computation to ﬁnd a shorter path becomes counterproductive.,Interface,4
388,The term means that the strategies have no additional information about states beyond that provided in the problem deﬁnition.,Safety,4
389,Breadth-ﬁrst search is an instance of the general graph-search algorithm (Figure 3.7) in which the shallowest unexpanded node is chosen for expansion.,Interface,2
390,"Thus, new nodes (which are always deeper than their parents) go to the back of the queue, and old nodes, which are shallower than the new nodes, get expanded ﬁrst.",Safety,2
391,"Note also that the algorithm, following the general template for graph search, discards any new path to a state already in the frontier or explored set; it is easy to see that any such path must be at least as deep as the one already found.",Safety,3
392,"Thus, breadth-ﬁrst search always has the shallowest path to every node on the frontier.",Safety,2
393,"We can easily see that it is complete—if the shallowest goal node is at some ﬁnite depth d, breadth-ﬁrst search will eventually ﬁnd it after generating all shallower nodes (provided the branching factor b is ﬁnite).",Safety,2
394,"Note that as soon as a goal node is generated, we know it is the shallowest goal node because all shallower nodes must have been generated already and failed the goal test.",Safety,2
395,"Now, the shallowest goal node is not necessarily the optimal one; 82 Chapter 3.",Safety,2
396,"Solving Problems by Searching function BREADTH-FIRST-SEARCH( problem) returns a solution, or failure node ← a node with STATE = problem.INITIAL-STATE, PATH-COST = 0 if problem.GOAL-TEST(node.STATE) then return SOLUTION(node) frontier ← a FIFO queue with node as the only element explored ← an empty set loop do /* chooses the shallowest node in frontier */ if EMPTY?( frontier ) then return failure node ← POP( frontier ) add node.STATE to explored for each action in problem.ACTIONS(node.STATE) do child ← CHILD-NODE( problem, node, action) if child .STATE is not in explored or frontier then if problem.GOAL-TEST(child .STATE) then return SOLUTION(child ) frontier ← INSERT(child , frontier ) Figure 3.11 Breadth-ﬁrst search on a graph.",Constraint,2
397,"There will be O(bd−1) nodes in the explored set and O(bd) nodes in the frontier, A A A A B C B C B C B C D E F G D E F G D E F G D E F G Figure 3.12 expanded next is indicated by a marker.",Safety,2
398,"It lists, for various values of the solution depth d, the time and memory required for a breadth- ﬁrst search with branching factor b = 10.",Constraint,4
399,The table assumes that 1 million nodes can be generated per second and that a node requires 1000 bytes of storage.,Safety,2
400,Time and memory requirements for breadth-ﬁrst search.,Constraint,4
401,"First, the memory requirements are a bigger problem for breadth-ﬁrst search than is the execution time.",Safety,4
402,"Fortunately, other strategies require less memory.",Constraint,4
403,"If your problem has a solution at depth 16, then (given our assumptions) it will take about 350 years for breadth-ﬁrst search (or indeed any uninformed search) to ﬁnd it.",Constraint,4
404,"3.4.2 Uniform-cost search When all step costs are equal, breadth-ﬁrst search is optimal because it always expands the shallowest unexpanded node.",Safety,2
405,"Instead of expanding the shallowest node, uniform-cost search expands the node n with the lowest path cost g(n).",Constraint,2
406,"The data structure for frontier needs to support efﬁcient membership testing, so it should combine the capabilities of a priority queue and a hash table.",Safety,4
407,"Hence, the ﬁrst goal node selected for expansion must be the optimal solution.",Safety,2
408,"Therefore, it will get stuck in an inﬁnite loop if there is a path with an inﬁnite sequence of zero-cost actions—for example, a sequence of NoOp actions.6 Completeness is guaranteed provided the cost of every step exceeds some small positive constant ǫ.",Interface,4
409,"This must be the deepest unexpanded node because it is one deeper than its parent—which, in turn, was the deepest unexpanded node when it was selected.",Safety,2
410,"As an alternative to the GRAPH-SEARCH-style implementation, it is common to im- plement depth-ﬁrst search with a recursive function that calls itself on each of its children in turn.",Functional,3
411,"The graph-search version, which avoids repeated states and re- dundant paths, is complete in ﬁnite state spaces because it will eventually expand every node.",Safety,2
412,"The tree-search version, on the other hand, is not complete—for example, in Figure 3.6 the algorithm will follow the Arad–Sibiu–Arad–Sibiu loop forever.",Safety,3
413,"For example, in Figure 3.16, depth- ﬁrst search will explore the entire left subtree even if node C is a goal node.",Safety,2
414,Note that m itself can be much larger than d (the depth of the shallowest solution) and is inﬁnite if the tree is unbounded.,Safety,4
415,"For a graph search, there is no advantage, but a depth-ﬁrst tree search needs to store only a single path from the root to a leaf node, along with the remaining unexpanded sibling nodes for each node on the path.",Constraint,2
416,"(See Figure 3.16.) For a state space with branching factor b and maximum depth m, depth-ﬁrst search requires storage of only O(bm) nodes.",Constraint,2
417,"Using the same assumptions as for Figure 3.13 and assuming that nodes at the same depth as the goal node have no successors, we ﬁnd that depth-ﬁrst search would require 156 kilobytes instead of 10 exabytes at depth d = 16, a factor of 7 trillion times less space.",Safety,2
418,This reduces the memory requirements to just one state description and O(m) actions.,Safety,4
419,"For this to work, we must be able to undo each modiﬁcation when we go back to generate the next suc- cessor.",Safety,4
420,"Unfortunately, it also introduces an additional source of incompleteness if we choose ℓ < d, that is, the shallowest goal is beyond the depth limit.",Interface,4
421,(This is likely when d is unknown.) Depth-limited search will also be nonoptimal if we choose ℓ > d.,Constraint,4
422,"Therefore, we know that if there is a solution, it must be of length 19 at the longest, so ℓ = 19 is a possible choice.",Safety,4
423,then return cutoﬀ else return failure DIAMETER ITERATIVE DEEPENING SEARCH Figure 3.17 A recursive implementation of depth-limited tree search.,Constraint,4
424,"For most problems, however, we will not know a good depth limit until we have solved the problem.",Safety,4
425,Depth-limited search can be implemented as a simple modiﬁcation to the general tree- or graph-search algorithm.,Safety,3
426,"Alternatively, it can be implemented as a simple recursive al- gorithm as shown in Figure 3.17.",Constraint,4
427,"This will occur when the depth limit reaches d, the depth of the shallowest goal node.",Safety,2
428,"Like depth-ﬁrst search, its memory requirements are modest: O(bd) to be precise.",Constraint,4
429,"It would seem worthwhile to develop an iterative analog to uniform-cost search, inheriting the latter algo- rithm’s optimality guarantees while avoiding its memory requirements.",Interface,4
430,"Bidirectional search is implemented by replacing the goal test with a check to see whether the frontiers of the two searches intersect; if they do, a solution has been found.",Constraint,4
431,"(It is important to realize that the ﬁrst such solution found may not be optimal, even if the two searches are both breadth-ﬁrst; some additional search is required to make sure there isn’t another short-cut across the gap.) The check can be done when each node is generated or selected for expansion and, with a hash table, will take constant time.",Constraint,2
432,"We can reduce this by roughly half if one of the two searches is done by iterative deepening, but at least one of the frontiers must be kept in memory so that the intersection check can be done.",Safety,4
433,This space requirement is the most signiﬁcant weakness of bidirectional search.,Safety,4
434,Bidirectional search requires a method for computing predecessors.,Constraint,3
435,Other cases may require substantial ingenuity.,Constraint,4
436,b is the branching factor; d is the depth Figure 3.21 of the shallowest solution; m is the maximum depth of the search tree; l is the depth limit.,Safety,4
437,"The implementation of best-ﬁrst graph search is identical to that for uniform-cost search (Fig- ure 3.14), except for the use of f instead of g to order the priority queue.",Safety,4
438,"Let us see how this works for route-ﬁnding problems in Romania; we use the straight- line distance heuristic, which we will call hSLD .",Safety,4
439,The ﬁrst node to be expanded from Arad will be Sibiu because it is closer to Bucharest than either Zerind or Timisoara.,Safety,2
440,The next node to be expanded will be Fagaras because it is closest.,Safety,2
441,"The algorithm will never ﬁnd this solution, however, because expanding Neamt puts Iasi back into the frontier, Iasi is closer to Fagaras than Vaslui is, and so Iasi will be expanded again, leading to an inﬁ- nite loop.",Interface,3
442,"It turns out that this strategy is more than just reasonable: provided that the heuristic function h(n) satisﬁes certain conditions, A∗ search is both complete and optimal.",Functional,3
443,Stages in a greedy best-ﬁrst tree search for Bucharest with the straight-line ADMISSIBLE HEURISTIC Conditions for optimality: Admissibility and consistency The ﬁrst condition we require for optimality is that h(n) be an admissible heuristic.,Constraint,4
444,"Another way to say this is that there might be a solution through Pitesti whose cost is as low as 417, so the algorithm will not settle for a solution that costs 450.",Safety,3
445,"A second, slightly stronger condition called consistency (or sometimes monotonicity) is required only for applications of A∗ to graph search.9 A heuristic h(n) is consistent if, for every node n and every successor n′ of n generated by any action a, the estimated cost of reaching the goal from n is no greater than the step cost of getting to n′ plus the estimated cost of reaching the goal from n′: h(n) ≤ c(n, a, n′) + h(n′) .",Safety,1
446,"Consistency is therefore a stricter requirement than admissibility, but one has to work quite hard to concoct heuristics that are admissible but not consistent.",Constraint,4
447,"Were this not the case, there would have to be another frontier node n′ on the optimal path from the start node to n, by the graph separation property of 9 With an admissible but inconsistent heuristic, A∗ requires some extra bookkeeping to ensure optimality.",Safety,2
448,"Hence, the ﬁrst goal node selected for expansion must be an optimal solution because f is the true cost for goal nodes (which have h = 0) and all later goal nodes will be at least as expensive.",Interface,2
449,"With uniform-cost search (A∗ search using h(n) = 0), the bands will be “circular” around the start state.",Constraint,4
450,"With more accurate heuristics, the bands will stretch toward the goal state and become more narrowly focused around the optimal path.",Constraint,4
451,"Completeness requires that there be only ﬁnitely many nodes with cost less than or equal to C ∗, a condition that is true if all step costs exceed some ﬁnite ǫ and if b is ﬁnite.",Constraint,2
452,"One can use variants of A∗ that ﬁnd suboptimal solutions quickly, or one can sometimes design heuristics that are more accurate but not strictly admissible.",Constraint,4
453,"In any case, the use of a good heuristic still provides enormous savings compared to the use of an uninformed search.",Interface,4
454,"In Section 3.6, we look at the question of designing good heuristics.",Safety,4
455,"ITERATIVE DEEPENING ∗ A RECURSIVE BEST FIRST SEARCH BACKED UP VALUE 3.5.3 Memory-bounded heuristic search The simplest way to reduce memory requirements for A∗ is to adapt the idea of iterative ∗ (IDA∗) al- deepening to the heuristic search context, resulting in the iterative-deepening A gorithm.",Safety,4
456,Each mind change corresponds to an iteration of IDA∗ and could require many reexpansions of forgotten nodes to recreate the best path and extend it one more node.,Interface,2
457,"Its space complexity is linear in the depth of the deepest optimal solution, but its time complexity is rather difﬁcult to characterize: it depends both on the accuracy of the heuristic function and on how often the best path changes as nodes are expanded.",Functional,2
458,"SMA∗ is—well—simpler, so are MA we will describe it.",Constraint,4
459,"Another way of saying this is that, if all the descendants of a node n are forgotten, then we will not know which way to go from n, but we will still have an idea of how worthwhile it is to go anywhere from n.",Interface,2
460,"These coincide when there is only one leaf, but in that case, the current search tree must be a single path from root to leaf that ﬁlls all of memory.",Safety,4
461,"SMA∗ is complete if there is any reachable solution—that is, if d, the depth of the shallowest goal node, is less than the memory size (expressed in nodes).",Safety,2
462,"Solving Problems by Searching On very hard problems, however, it will often be the case that SMA∗ is forced to switch back and forth continually among many candidate solution paths, only a small subset of which can ﬁt in memory.",Safety,4
463,"(This resembles the problem of thrashing in disk paging systems.) Then the extra time required for repeated regeneration of the same nodes means that problems that would be practically solvable by A∗, given unlimited memory, become intractable for SMA∗.",Safety,1
464,"Although no current theory explains the tradeoff between time and memory, it seems that this is an inescapable problem.",Interface,4
465,The only way out is to drop the optimality requirement.,Safety,4
466,"3.5.4 Learning to search better METALEVEL STATE SPACE OBJECT LEVEL STATE SPACE METALEVEL LEARNING We have presented several ﬁxed strategies—breadth-ﬁrst, greedy best-ﬁrst, and so on—that have been designed by computer scientists.",Constraint,4
467,"For harder problems, there will be many such missteps, and a metalevel learning algorithm can learn from these experiences to avoid exploring unpromis- ing subtrees.",Constraint,3
468,h1 is an admissible heuristic because it is clear that any tile that is out of place must be moved at least once.,Interface,4
469,"Because tiles cannot move along diagonals, the distance we will count is the sum of the horizontal and vertical distances.",Safety,4
470,MANHATTAN DISTANCE EFFECTIVE BRANCHING FACTOR 3.6.1 The effect of heuristic accuracy on performance One way to characterize the quality of a heuristic is the effective branching factor b∗.,Performance,4
471,"(The existence of an effective branching factor follows from the result, mentioned earlier, that the number of nodes expanded by A∗ grows exponentially with solution depth.) Therefore, experimental measurements of b∗ on a small set of problems can provide a good guide to the heuristic’s overall usefulness.",Interface,2
472,"A well- designed heuristic would have a value of b∗ close to 1, allowing fairly large problems to be solved at reasonable computational cost.",Constraint,4
473,Domination translates directly into efﬁciency: A∗ using h2 will never expand more nodes than A∗ using h1 (except possibly for some nodes with f (n) = C ∗).,Constraint,2
474,Recall the observation on page 97 that every node with f (n) < C ∗ will surely be expanded.,Constraint,2
475,This is the same as saying that every node with h(n) < C ∗ − g(n) will surely be expanded.,Constraint,2
476,"But because h2 is at least as big as h1 for all nodes, every node that is surely expanded by A∗ search with h2 will also surely be expanded with h1, and h1 might cause other nodes to be expanded as well.",Constraint,2
477,"Hence, it is generally better to use a heuristic function with higher values, provided it is consistent and that the computation time for the heuristic is not too long.",Functional,3
478,"Because the relaxed problem adds edges to the state space, any optimal solution in the original problem is, by deﬁnition, also a solution in the relaxed problem; but the relaxed problem may have better solutions if the added edges provide short cuts.",Interface,4
479,"Furthermore, because the derived heuristic is an exact cost for the relaxed problem, it must obey the triangle inequality and is therefore consistent (see page 95).",Interface,4
480,"If the relaxed problem is hard to solve, then the values of the corresponding heuristic will be expensive to obtain.12 A program called ABSOLVER can generate heuristics automatically from problem def- initions, using the “relaxed problem” method and various other techniques (Prieditis, 1993).",Safety,3
481,"hm is available for a problem and none of them dominates any of the others, which should we choose?",Safety,4
482,"12 Note that a perfect heuristic can be obtained simply by allowing h to run a full breadth-ﬁrst search “on the sly.” Thus, there is a tradeoff between accuracy and computation time for heuristic functions.",Safety,3
483,"The answer is no, because the solutions of the 1-2-3-4 subproblem and the 5-6-7-8 subproblem for a given state will almost certainly share some moves—it is 13 By working backward from the goal, the exact solution cost of every instance encountered is immediately available.",Safety,4
484,Each optimal solution to an 8-puzzle problem provides examples from which h(n) can be learned.,Interface,4
485,A second feature x2(n) might be “number of pairs of adjacent tiles that are not adjacent in the goal state.” How should x1(n) and x2(n) be combined to predict h(n)?,Safety,4
486,"• Before an agent can start searching for solutions, a goal must be identiﬁed and a well- deﬁned problem must be formulated.",Interface,4
487,"Complexity depends on b, the branching factor in the state space, and d, the depth of the shallowest solution.",Safety,4
488,"The basic algorithms are as follows: – Breadth-ﬁrst search expands the shallowest nodes ﬁrst; it is complete, optimal for unit step costs, but has exponential space complexity.",Safety,2
489,"– Bidirectional search can enormously reduce time complexity, but it is not always applicable and may require too much space.",Constraint,4
490,"A∗ is complete and optimal, provided that h(n) is admissible (for TREE-SEARCH) or consistent (for GRAPH-SEARCH).",Constraint,4
491,"It was widely believed to have been invented by the fa- mous American game designer Sam Loyd, based on his claims to that effect from 1891 on- ward (Loyd, 1959).",Safety,4
492,"The method of dynamic programming (Bellman, 1957; Bellman and Dreyfus, 1962), which systematically records solutions for all subproblems of increasing lengths, can be seen as a form of breadth-ﬁrst search on graphs.",Safety,1
493,A version of iterative deepening designed to make efﬁcient use of the chess clock was ﬁrst used by Slate and Atkin (1977) in the CHESS 4.5 game-playing program.,Safety,4
494,"Al- though they analyzed path length and “penetrance” (the ratio of path length to the total num- ber of nodes examined so far), they appear to have ignored the information provided by the path cost g(n).",Safety,2
495,"(2001) argue that the time cost is better modeled as O(bd−k), where k depends on the heuristic accuracy; this analysis has elicited some controversy, however.",Safety,4
496,"The same property is exhibited by the A∗ ǫ algorithm (Pearl, 1984), which can select any node from the frontier provided its f -cost is within a factor 1 + ǫ of the lowest-f -cost frontier node.",Safety,2
497,"The idea of keeping track of the best alternative path appeared earlier in Bratko’s (1986) elegant Prolog implementation of A∗ and in the DTA∗ algorithm (Russell and Wefald, 1991).",Safety,3
498,"SMA∗, or Simpliﬁed MA∗, emerged from an attempt to implement MA∗ as a comparison algorithm for IE (Russell, 1992).",Interface,3
499,"(See Exercise 3.33.) The automation of the relaxation process was implemented successfully by Priedi- tis (1993), building on earlier work with Mostow (Mostow and Prieditis, 1989).",Constraint,4
500,"This book provides especially good coverage of the wide variety of offshoots and variations of A∗, including rigorous proofs of their formal properties.",Safety,4
501,"The topic of parallel search algorithms was not covered in the chapter, partly because it requires a lengthy discussion of parallel computer architectures.",Safety,3
502,"Also of increasing importance are search algorithms for very large graphs that require disk storage (Korf, 2008).",Constraint,3
503,3.1 Explain why problem formulation must follow goal formulation.,Constraint,4
504,Choose a PARALLEL SEARCH EXERCISES Exercises 113 formulation that is precise enough to be implemented.,Constraint,4
505,"The amount of time needed to move from city i to neighbor j is equal to the road distance d(i, j) between the cities, but on each turn the friend that arrives ﬁrst must wait until the other one arrives (and calls the ﬁrst on his/her cell phone) before the next turn can begin.",Safety,4
506,(You will ﬁnd it helpful to deﬁne some formal notation here.) b.,Constraint,4
507,Are there maps in which all solutions require one friend to visit the same city twice?,Safety,4
508,Explain brieﬂy why the shortest path from one polygon vertex to any other in the scene must consist of straight-line segments joining some of the vertices of the polygons.,Safety,4
509,"Deﬁne the necessary functions to implement the search problem, including an ACTIONS function that takes a vertex as input and returns a set of vectors, each of which maps the current vertex to one of the vertices that can be reached in a straight line.",Functional,3
510,Does it help if we insist that step costs must be greater than or equal to some negative constant c?,Constraint,4
511,"Formulate the problem precisely, making only those distinctions necessary to ensure a valid solution.",Safety,4
512,Implement and solve the problem optimally using an appropriate search algorithm.,Interface,3
513,"Begin by pretending that every piece is unique.) 3.17 Implement two versions of the RESULT(s, a) function for the 8-puzzle: one that copies Exercises 117 and edits the data structure for the parent node s and one that modiﬁes the parent state di- rectly (undoing the modiﬁcations as needed).",Functional,2
514,How many iterations will iterative lengthening require?,Constraint,4
515,How many iterations are required in the worst case?,Safety,4
516,Implement the algorithm and apply it to instances of the 8-puzzle and traveling sales- person problems.,Safety,3
517,3.20 Write a program that will take as input two Web page URLs and ﬁnd a path of links from one to the other.,Safety,4
518,Could a search engine be used to implement a predecessor function?,Functional,3
519,Should the algorithm use tree search or graph search?,Safety,3
520,"That is, show the sequence of nodes that the algorithm will consider and the f , g, and h score for each node.",Safety,2
521,"3.32 Prove that if a heuristic is consistent, it must be admissible.",Constraint,4
522,Test these claims by implementing the heuristics and comparing the performance of the resulting algorithms.,Performance,3
523,"5.1 GAMES GAME ZERO SUM GAMES PERFECT INFORMATION Chapter 2 introduced multiagent environments, in which each agent needs to consider the actions of other agents and how they affect its own welfare.",Safety,4
524,"Mathematical game theory, a branch of economics, views any multiagent environment as a game, provided that the impact of each agent on the others is “signiﬁcant,” regardless of whether the agents are cooperative or competitive.1 In AI, the most common games are of a rather specialized kind—what game theorists call deterministic, turn-taking, two-player, zero-sum games of perfect information (such as chess).",Safety,4
525,"Games, like the real world, therefore require the ability to make some decision even when calculating the optimal decision is infeasible.",Safety,4
526,"Whereas an implementation of A∗ search that is half as efﬁcient will simply take twice as long to run to completion, a chess program that is half as efﬁcient in using its available time probably will be beaten into the ground, other things being equal.",Interface,4
527,"We ﬁrst consider games with two players, whom we call MAX and MIN for reasons that will soon become obvious.",Constraint,4
528,"MAX therefore must ﬁnd a contingent strategy, which speciﬁes MAX’s move in the initial state, then MAX’s moves in the states resulting from every possible response by 164 Chapter 5.",Safety,4
529,"Even a simple game like tic-tac-toe is too complex for us to draw the entire game tree on one page, so we will switch to the trivial game in Figure 5.2.",Safety,4
530,Then it is easy to show (Exercise 5.7) that MAX will do even better.,Constraint,4
531,"It uses a simple recursive computation of the minimax values of each successor state, directly implementing the deﬁning equations.",Safety,4
532,"(In two-player, zero-sum games, the two-element vector can be reduced to a single value because the values are always opposite.) The simplest way to implement this is to have the UTILITY function return a vector of utilities.",Functional,3
533,"Since 6 is bigger than 3, C should choose the ﬁrst move.",Safety,4
534,"This means that if state X is reached, subsequent play will lead to a terminal state with utilities hvA = 1, vB = 2, vC = 6i.",Constraint,4
535,"In other cases, a social stigma attaches to breaking an alliance, so players must balance the immediate advantage of breaking an alliance against the long-term disadvantage of being perceived as untrustworthy.",Interface,4
536,"Then the optimal strategy is for both players to do everything possible to reach this state—that is, the players will automatically cooperate to achieve a mutually desirable goal.",Safety,4
537,"If Player has a better choice m either at the parent node of n or at any choice point further up, then n will never be reached in actual play.",Safety,2
538,Alpha–Beta Pruning 169 Player Opponent m • • • Player Opponent n Figure 5.6 will never get to n in play.,Constraint,4
539,"If this can be done,2 then it turns out that alpha–beta needs to examine only O(bm/2) nodes to pick the best move, instead of O(bm) for minimax.",Safety,2
540,"If successors are examined in random order rather than best-ﬁrst, the total number of nodes examined will be roughly O(b3m/4) for moderate b.",Safety,2
541,"This depth is usually not practical, because moves must be made in a reasonable amount of time—typically a few minutes at most.",Constraint,4
542,"Claude Shannon’s paper Programming a Computer for Playing Chess (1950) proposed instead that programs should cut off the search earlier and apply a heuristic evaluation func- tion to states in the search, effectively turning nonterminal nodes into terminal leaves.",Safety,2
543,It should be clear that the performance of a game-playing program depends strongly on the quality of its evaluation function.,Functional,3
544,An inaccurate evaluation function will guide an agent toward positions that turn out to be lost.,Interface,3
545,How exactly do we design good evaluation functions?,Constraint,3
546,"First, the evaluation function should order the terminal states in the same way as the true utility function: states that are wins must evaluate better than draws, which in turn must be better than losses.",Functional,3
547,"Second, the computation must not take too long!",Safety,4
548,"(The whole point is to search faster.) Third, for nonterminal states, the evaluation function should be strongly correlated with the actual chances of winning.",Functional,3
549,"But if the search must be cut off at nonterminal states, then the algorithm will necessarily be uncertain about the ﬁnal outcomes of those states.",Safety,3
550,"Any given category, generally speaking, will contain some states that lead to wins, some that lead to draws, and some that lead to losses.",Constraint,4
551,"In practice, this kind of analysis requires too many categories and hence too much experience to estimate all the probabilities of winning.",Safety,4
552,"A secure advantage equivalent to a pawn gives a substantial likelihood of winning, and a secure advantage equivalent to three pawns should give almost certain victory, as illustrated in Figure 5.8(a).",Constraint,4
553,"Figure 5.8 In (a), Black has an advantage of a knight and two pawns, which should be enough to win the game.",Interface,4
554,"In (b), White will capture the queen, giving it an advantage that should be strong enough to win.",Interface,4
555,The astute reader will have noticed that the features and weights are not part of the rules of chess!,Safety,4
556,5.4.2 Cutting off search The next step is to modify ALPHA-BETA-SEARCH so that it will call the heuristic EVAL function when it is appropriate to cut off the search.,Functional,3
557,"We replace the two lines in Figure 5.7 that mention TERMINAL-TEST with the following line: if CUTOFF-TEST(state, depth) then return EVAL(state) We also must arrange for some bookkeeping so that the current depth is incremented on each recursive call.",Constraint,4
558,"(It must also return true for all terminal states, just as TERMINAL-TEST did.) The depth d is chosen so that a move is selected within the allocated time.",Safety,4
559,"The evaluation function should be applied only to positions that are quiescent—that is, unlikely to exhibit wild swings in value in the near future.",Functional,3
560,"This extra search is called a quiescence search; sometimes it is restricted to consider only certain types of moves, such as capture moves, that will quickly resolve the uncertainties in the position.",Safety,4
561,"Most moves by Black will lead to the eventual capture of the bishop, and thus will be marked as “bad” moves.",Safety,4
562,But Black will consider checking the white king with the pawn at e4.,Constraint,4
563,This will lead to the king capturing the pawn.,Safety,4
564,"Now Black will consider checking again, with the pawn at f5, leading to another pawn capture.",Constraint,4
565,"This makes the tree deeper, but because there will be few singular extensions, it does not add many total nodes to the tree.",Safety,2
566,"Unfortunately, this approach is rather dangerous because there is no guarantee that the best move will not be pruned away.",Safety,4
567,"The PROBCUT, or probabilistic cut, algorithm (Buro, 1995) is a forward-pruning ver- sion of alpha–beta search that uses statistics gained from prior experience to lessen the chance that the best move will be pruned.",Safety,3
568,"It computes this probability by doing a shallow search to compute the backed-up value v of a node and then using past experience to estimate how likely it is that a score of v at depth d in the tree would be outside (α, β).",Safety,2
569,"Let us assume we have implemented an evaluation function for chess, a reasonable cutoff test with a quiescence search, and a large transposition table.",Functional,3
570,"Adversarial Search 5.4.4 Search versus lookup Somehow it seems like overkill for a chess program to start a game by considering a tree of a billion game states, only to conclude that it will move its pawn to e4.",Constraint,4
571,"Usually after ten moves we end up in a rarely seen position, and the program must switch from table lookup to search.",Safety,4
572,How big will the KBNK lookup table be?,Safety,4
573,"Any move by White that, no matter what move Black responds with, ends up in a position marked as a win, must also be a win.",Constraint,4
574,Stiller discovered one case where a forced mate existed but required 262 moves; this caused some consternation because the rules of chess require a capture or pawn move to occur within 50 moves.,Safety,4
575,"Later work by Marc Bourzutschky and Yakov Konoval (Bourzutschky, 2006) solved all pawnless six-piece and some seven-piece endgames; there is a KQNKRBN endgame that with best play requires 517 moves until a capture, which then leads to a mate.",Constraint,4
576,"A piece can move to any position unless multiple opponent pieces are there; if there is one opponent, it is captured and must start over.",Constraint,4
577,"In the position shown, White has rolled 6–5 and must choose among four legal moves: (5–10,5–11), (5–11,19–24), (5–10,10–16), and (5–11,11–16), where the notation (5–11,11–16) means move one piece from position 5 to 11, and then move a piece from 11 to 16.",Safety,4
578,"CHANCE NODES Although White knows what his or her own legal moves are, White does not know what Black is going to roll and thus does not know what Black’s legal moves will be.",Constraint,2
579,A game tree in backgammon must include chance nodes in addition to MAX and MIN nodes.,Constraint,2
580,One might think that evaluation functions for games such as backgammon should be just like evaluation functions Section 5.5.,Constraint,3
581,"It turns out that to avoid this sensitivity, the evaluation function must be a positive linear transformation of the probability of winning from a position (or, more generally, of the expected utility of the position).",Functional,3
582,"Because expectiminimax is also considering all the possible dice-roll sequences, it will take O(bmnm), where n is the number of distinct rolls.",Safety,4
583,"This is a general problem whenever uncertainty enters the picture: the possibilities are multiplied enormously, and forming detailed plans of action becomes pointless because the world probably will not play along.",Safety,4
584,"(Recall that this is what alpha–beta needs in order to prune a node and its subtree.) At ﬁrst sight, it might seem impossible because the value of C is the average of its children’s values, and in order to compute the average of a set of numbers, we must look at all the numbers.",Safety,2
585,"We will examine the game of Kriegspiel, a partially observable variant of chess in which pieces can move but are completely invisible to the opponent.",Safety,4
586,Such checkmates are still required to work in every board state in the belief state; they are probabilistic with respect to randomization of the winning player’s moves.,Constraint,4
587,"Simply by moving randomly, the white king will eventually bump into the black king even if the latter tries to avoid this fate, since Black cannot keep guessing the right evasive moves indeﬁnitely.",Safety,4
588,"The KBNK endgame—king, bishop 3 Sometimes, the belief state will become too large to represent just as a list of board states, but we will ignore this issue for now; Chapters 7 and 8 suggest methods for compactly representing very large belief states.",Safety,3
589,"and knight against king—is won in this sense; White presents Black with an inﬁnite random sequence of choices, for one of which Black will guess incorrectly and reveal his position, leading to checkmate.",Constraint,4
590,"(Most checkmates in games between humans are of this accidental nature.) This idea leads naturally to the question of how likely it is that a given strategy will win, which leads in turn to the question of how likely it is that each board state in the current belief state is the true board state.",Safety,4
591,"By deﬁnition (assuming that Black plays optimally), Black must have played an optimal move, so all board states resulting from suboptimal moves ought to be assigned zero probability.",Interface,4
592,Playing any predictable “optimal” strategy provides the opponent with information.,Constraint,4
593,"Hence, optimal play in partially observable games requires a willingness to play somewhat randomly.",Constraint,4
594,"From these considerations, it seems that the probabilities associated with the board states in the current belief state can only be calculated given an optimal randomized strat- egy; in turn, computing that strategy seems to require knowing the probabilities of the var- ious states the board might be in.",Constraint,4
595,"At present, the design of effective algorithms for general Kriegspiel play is an open research topic.",Interface,3
596,"Most systems perform bounded-depth lookahead in their own belief- state space, ignoring the opponent’s belief state.",Functional,1
597,"5.6.2 Card games Card games provide many examples of stochastic partial observability, where the missing information is generated randomly.",Interface,4
598,"For games like whist and hearts, where there is no bidding or betting phase before play commences, each deal will be equally likely and so the values of P (s) are all equal.",Safety,4
599,The strategy described in Equations 5.1 and 5.2 is sometimes called averaging over clairvoyance because it assumes that the game will become observable to both players im- mediately after the ﬁrst move.,Safety,4
600,"Averaging over clairvoyance leads to the following reasoning: on Day 1, B is the right choice; on Day 2, B is the right choice; on Day 3, the situation is the same as either Day 1 or Day 2, so B must still be the right choice.",Safety,4
601,Now we can see how averaging over clairvoyance fails: it does not consider the belief state that the agent will be in after acting.,Safety,4
602,"Because it assumes that every future state will automatically be one of perfect knowledge, the approach never selects actions that gather in- formation (like the ﬁrst move in Figure 5.13); nor will it choose actions that hide information from the opponent or provide information to a partner because it assumes that they already know the information; and it will never bluff in poker,4 because it assumes the opponent can see its cards.",Interface,4
603,The system also used a large endgame database of solved positions con- taining all positions with ﬁve pieces and many with six pieces.,Constraint,1
604,"The most important of these is the null move heuristic, which generates a good lower bound on the value of a position, using a shallow search in which the opponent gets to move twice at the beginning.",Safety,4
605,"Also important is futility pruning, which helps decide in advance which moves will cause a beta cutoff in the successor nodes.",Safety,2
606,"It uses an off-the-shelf 8-core 3.2 GHz Intel Xeon processor, but little is known about the design of the program.",Interface,4
607,"While it does not play optimally, Bridge Baron is one of the few successful game-playing systems to use complex, hierarchical plans (see Chapter 11) involving high-level ideas, such as ﬁnessing and squeezing, that are familiar to bridge players.",Safety,1
608,GIB’s tactical accuracy makes up for its inability to reason about information.,Constraint,4
609,The problem is that Scrabble is both partially observable and stochastic: you don’t know what letters the other player has or what letters you will draw next.,Safety,4
610,"5.8 ALTERNATIVE APPROACHES Because calculating optimal decisions in games is intractable in most cases, all algorithms must make some assumptions and approximations.",Constraint,3
611,Some believe that this has caused game playing to become divorced from the main- stream of AI research: the standard approach no longer provides much room for new insight into general questions of decision making.,Safety,4
612,It selects an optimal move in a given search tree provided that the leaf node evaluations are exactly correct.,Interface,2
613,"If we get one node wrong, the chances are high that nearby nodes in the tree will also be wrong.",Safety,2
614,The aim of an algorithm designer is to specify a computation that runs quickly and yields a good move.,Interface,3
615,The alpha–beta algorithm is designed not just to select a good move but also to calculate bounds on the values of all the legal moves.,Safety,3
616,"Alpha–beta search still will generate and evaluate a large search tree, telling us that the only move is the best move and assigning it a value.",Safety,4
617,"A good search algorithm should select node expansions of high utility—that is, ones that are likely to lead to the discovery of a signiﬁcantly better move.",Safety,2
618,"If there are no node expansions whose utility is higher than their cost (in terms of time), then the algorithm should stop searching and make a move.",Safety,2
619,"Notice that this works not only for clear-favorite situations but also for the case of symmetrical moves, for which no amount of search will show that one move is better than another.",Safety,4
620,"In Chapter 16, we see how these ideas can be made precise and implementable.",Constraint,4
621,David Wilkins’ (1980) PARADISE is the only program to have used goal-directed reasoning successfully in chess: it was capable of solving some chess problems requiring an 18-move combination.,Interface,4
622,"As yet there is no good understanding of how to combine the two kinds of algorithms into a robust and efﬁcient system, although Bridge Baron might be a step in the right direction.",Safety,1
623,"A fully integrated system would be a signiﬁcant achievement not just for game-playing research but also for AI research in general, because it would be a good basis for a general intelligent agent.",Safety,1
624,"He did not understand the exponential complexity of search trees, claiming “the combinations involved in the Analytical Engine enormously surpassed any required, even by the game of chess.” Babbage also designed, but did not build, a special-purpose machine for playing tic-tac-toe.",Safety,4
625,"Zermelo says that should we eventually know, “Chess would of course lose the character of a game at all.” A solid foundation for game theory was developed in the seminal work Theory of Games and Economic Behavior (von Neumann and Morgenstern, 1944), which included an analysis showing that some games require strategies that are randomized (or otherwise unpredictable).",Interface,4
626,"The memory requirements and computational overhead of the queue make SSS∗ in its original form impractical, but a linear-space version has been developed from the RBFS algorithm (Korf and Chickering, 1996).",Safety,3
627,Bruce Ballard (1983) extended alpha–beta pruning to cover trees with chance nodes and Hauk (2004) reexamines this work and provides empirical results.,Constraint,2
628,Koller and Pfeffer (1997) describe a system for completely solving partially observ- able games.,Safety,1
629,"The system is quite general, handling games whose optimal strategy requires randomized moves and games that are more complex than those handled by any previous system.",Safety,1
630,"Incremental belief-state algorithms enabled much more complex midgame checkmates to be found (Russell and Wolfe, 2005; Wolfe and Russell, 2007), but efﬁcient state estimation remains the primary obstacle to effective general play (Parker et al., 2005).",Safety,3
631,"(Kasparov, 1997) Probably the most complete description of a modern chess program is provided by Ernst Heinz (2000), whose DARKTHOUGHT program was the highest-ranked noncommercial PC program at the 1999 world championships.",Safety,4
632,Building an endgame table for all of checkers would be impractical: it would require a billion gigabytes of storage.,Interface,4
633,"The General Game Competition (Love et al., 2006) tests programs that must learn to play an un- known game given only a logical description of the rules of the game.",Interface,4
634,Give an informal proof that someone will eventually win if both play perfectly.,Interface,4
635,"Exercises 197 5.4 Describe and implement state descriptions, move generators, terminal tests, utility func- tions, and evaluation functions for one or more of the following stochastic games: Monopoly, Scrabble, bridge play with a given contract, or Texas hold’em poker.",Constraint,3
636,"5.5 Describe and implement a real-time, multiplayer game-playing environment, where time is part of the environment state and players are given ﬁxed time allocations.",Safety,4
637,"5.7 Prove the following assertion: For every game tree, the utility obtained by MAX using minimax decisions against a suboptimal MIN will be never be lower than the utility obtained playing against an optimal MIN.",Interface,4
638,"The two players Figure 5.17 take turns moving, and each player must move his token to an open adjacent space in either direction.",Interface,4
639,"5.11 Develop a general game-playing program, capable of playing a variety of games.",Constraint,4
640,"Implement move generators and evaluation functions for one or more of the following games: Kalah, Othello, checkers, and chess.",Safety,3
641,"Implement a selective search algorithm, such as B* (Berliner, 1979), conspiracy number search (McAllester, 1988), or MGSS* (Russell and Wefald, 1989) and compare its performance to A*.",Performance,3
642,"Now reformulate the expression to show that in order to affect n1, nj must not exceed a certain bound derived from the li values.",Safety,4
643,Will that be enough for the 200 Chapter 5.,Safety,4
644,"Implement the expectiminimax algorithm and the *-alpha–beta algorithm, which is 5.17 described by Ballard (1983), for pruning game trees with chance nodes.",Constraint,2
645,Will this procedure work well?,Constraint,4
646,"In a fully observable, turn-taking, zero-sum game between two perfectly rational play- ers, it does not help the ﬁrst player to know what strategy the second player is using— that is, what move the second player will make, given the ﬁrst player’s move.",Safety,4
647,"In a partially observable, turn-taking, zero-sum game between two perfectly rational players, it does not help the ﬁrst player to know what move the second player will make, given the ﬁrst player’s move.",Safety,4
648,"Implement the algorithm and run it in your game-playing agent, with appropriate modiﬁcations to the game- playing environment.",Constraint,3
649,CSP search algorithms take advantage of the structure of states and use general-purpose rather than problem-speciﬁc heuristics to enable the solution of complex problems.,Safety,3
650,"Deﬁning Constraint Satisfaction Problems 203 the domain {A,B}, then the constraint saying the two variables must have different values can be written as h(X1, X2), [(A, B), (B, A)]i or as h(X1, X2), X1 6= X2i.",Safety,4
651,The constraints require neighboring regions to have distinct colors.,Constraint,4
652,"One reason is that the CSPs yield a natural rep- resentation for a wide variety of problems; if you already have a CSP-solving system, it is often easier to solve a problem using it than to design a custom solution using another search technique.",Safety,1
653,"Constraints can assert that one task must occur before another—for example, a wheel must be installed before the hubcap is put on—and that only so many tasks can go on at once.",Constraint,4
654,"Whenever a task T1 must occur before task T2, and task T1 takes duration d1 to complete, we add an arithmetic constraint of the form T1 + d1 ≤ T2 .",Interface,4
655,"Next we say that, for each wheel, we must afﬁx the wheel (which takes 1 minute), then tighten the nuts (2 minutes), and ﬁnally attach the hubcap (1 minute, but not represented yet): Wheel RF + 1 ≤ Nuts RF ; Nuts RF + 2 ≤ CapRF ; Wheel LF + 1 ≤ Nuts LF ; Nuts LF + 2 ≤ CapLF ; Wheel RB + 1 ≤ Nuts RB; Nuts RB + 2 ≤ CapRB; .",Safety,4
656,We need a disjunctive constraint to say that Axle F and AxleB must not overlap in time; either one comes ﬁrst or the other does: (Axle F + 10 ≤ Axle B) or (Axle B + 10 ≤ Axle F ) .,Safety,4
657,"Finally, suppose there is a requirement to get the whole assembly done in 30 minutes.",Safety,4
658,"Instead, a constraint language must be used that understands constraints such as T1 + d1 ≤ T2 directly, without enumerating the set of pairs of allowable values for (T1, T2).",Constraint,4
659,"For example, the scheduling of experiments on the Hubble Space Telescope requires very precise timing of observations; the start and ﬁnish of each observation and maneuver are continuous-valued variables that must obey a variety of astronomical, precedence, and power constraints.",Constraint,4
660,"The best-known category of continuous-domain CSPs is that of linear programming problems, where con- straints must be linear equalities or inequalities.",Safety,4
661,"One of the most common global constraints is Alldiﬀ , which says that all of the variables involved in the constraint must have different values.",Constraint,4
662,"In Sudoku problems (see Section 6.2.6), all variables in a row or column must satisfy an Alldiﬀ constraint.",Interface,4
663,An- other example is provided by cryptarithmetic puzzles.,Interface,4
664,"Another way to convert an n-ary CSP to a binary one is the dual graph transformation: create a new graph in which there will be one variable for each constraint in the original graph, and Section 6.1.",Interface,4
665,"Second, it is possible to design special-purpose inference algorithms for global constraints that are not available for a set of more primitive constraints.",Constraint,3
666,"Sometimes this preprocessing can solve the whole problem, so no search is required at all.",Safety,4
667,"We need to do that because the change in Di might enable further reductions in the domains of Dk, even if we have previously considered Xk.",Safety,4
668,"At that point, we are left with a CSP that is equivalent to the original CSP—they both have the same solutions—but the arc-consistent CSP will in most cases be faster to search because its variables have smaller domains.",Constraint,4
669,"We will make the set {WA, SA} path consistent with respect to NT .",Constraint,4
670,"Of course, there is no free lunch: any algorithm for establishing n-consistency must take time exponential in n in the worst case.",Safety,3
671,"Worse, n-consistency also requires space that is exponential in n.",Constraint,4
672,"For example, the Alldiﬀ constraint says that all the variables involved must have distinct values (as in the cryptarithmetic problem above and Sudoku puzzles be- low).",Safety,4
673,"Notice that the variables SA, NT , and Q are effectively connected by an Alldiﬀ constraint because each pair must have two different colors.",Interface,4
674,Now suppose we have the additional constraint that the two ﬂights together must carry 420 people: F1 + F2 = 420.,Safety,4
675,"Applying arc consistency in its column, we eliminate 5, 6, 2, 4 (since we now know E6 must be 4), 8, 9, and 3.",Constraint,4
676,"Now there are 8 known values in column 6, so arc consistency can infer that A6 must be 1.",Constraint,4
677,"To solve the hardest puzzles and to make efﬁcient progress, we will have to be more clever.",Safety,4
678,"From that we don’t know which square contains 1, 3, or 8, but we do know that the three numbers must be distributed among the three squares.",Safety,4
679,6.3 BACKTRACKING SEARCH FOR CSPS Sudoku problems are designed to be solved by inference over constraints.,Constraint,4
680,But many other CSPs cannot be solved by inference alone; there comes a time when we must search for a solution.,Constraint,4
681,"By varying the functions SELECT-UNASSIGNED-VARIABLE and ORDER-DOMAIN-VALUES, we can implement the general-purpose heuristics discussed in the text.",Safety,3
682,"Which variable should be assigned next (SELECT-UNASSIGNED-VARIABLE), and in what order should its values be tried (O RDER-DOMAIN-VALUES)?",Constraint,4
683,What inferences should be performed at each step in the search (INFERENCE)?,Safety,4
684,"If some variable X has no legal values left, the MRV heuristic will select X and failure will be detected immediately—avoiding pointless searches through other variables.",Safety,4
685,"Once a variable has been selected, the algorithm must decide on the order in which to examine its values.",Safety,3
686,"Why should variable selection be fail-ﬁrst, but value selection be fail-last?",Constraint,4
687,"Hence, forward checking has detected that the partial assignment {WA = red , Q = green, V = blue} is inconsistent with the constraints of the problem, and the algorithm will therefore backtrack immediately.",Constraint,3
688,For many problems the search will be more effective if we combine the MRV heuris- tic with forward checking.,Constraint,4
689,"Intuitively, it seems that that assignment constrains its neighbors, NT and SA, so we should handle those 218 Chapter 6.",Constraint,4
690,"variables next, and then all the other variables will fall into place.",Safety,4
691,We can view forward checking as an efﬁcient way to incrementally compute the information that the MRV heuristic needs to do its job.,Interface,4
692,"To do this, we will keep track of a set of assignments that are in conﬂict with some value for SA.",Constraint,4
693,This method is easily implemented by a modiﬁcation to BACKTRACK such that it accumulates the conﬂict set while checking for a legal value to assign.,Safety,3
694,"If no legal value is found, the algorithm should return the most recent element of the conﬂict set along with the failure indicator.",Constraint,3
695,"The sharp-eyed reader will have noticed that forward checking can supply the conﬂict set with no extra work: whenever forward checking based on an assignment X = x deletes a value from Y ’s domain, it should add X = x to Y ’s conﬂict set.",Constraint,4
696,The eagle-eyed reader will have noticed something odd: backjumping occurs when every value in a domain is in conﬂict with the current assignment; but forward checking detects this event and prevents the search from ever reaching such a node!,Constraint,2
697,"We know, however, that the four variables NT , Q, V , and SA, taken together, failed because of a set of preceding variables, which must be those variables that directly conﬂict with the four.",Constraint,4
698,"In this case, the set is WA and NSW , so the algorithm should backtrack to NSW and skip over Tasmania.",Safety,3
699,We must now explain how these new conﬂict sets are computed.,Constraint,4
700,"In this particular case, recording the no-good would not help, because once we prune this branch from the search tree, we will never encounter this combination again.",Safety,4
701,"At each step of the search, the algorithm chooses a variable/value pair to change that will result in the lowest total weight of all violated constraints.",Constraint,3
702,A backtracking search with the new set of constraints usually requires much more time and might ﬁnd a solution with many changes from the current schedule.,Constraint,4
703,"Then there are n/c subproblems, each of which takes at most dc work to solve, 3 A careful cartographer or patriotic Tasmanian might object that Tasmania should not be colored the same as its nearest mainland neighbor, to avoid the impression that it might be part of that state.",Safety,4
704,"Any tree with n nodes has n−1 arcs, so we can make this graph directed arc-consistent in O(n) steps, each of which must compare up to d possible domain values for two variables, for a total time of O(nd2).",Constraint,2
705,"Since each link from a parent to its child is arc consistent, we know that for any value we choose for the parent, there will be a valid value left to choose for the child.",Safety,4
706,"Constraint Satisfaction Problems function TREE-CSP-SOLVER( csp) returns a solution, or failure inputs: csp, a CSP with components X, D, C n ← number of variables in X assignment ← an empty assignment root ← any variable in X X ← TOPOLOGICALSORT(X , root) for j = n down to 2 do MAKE-ARC-CONSISTENT(PARENT(Xj), Xj) if it cannot be made consistent then return failure for i = 1 to n do assignment [Xi] ← any consistent value from Di if there is no consistent value then return failure return assignment Figure 6.11 CSP has a solution, we will ﬁnd it in linear time; if not, we will detect a contradiction.",Functional,3
707,"Now, any solution for the CSP after SA and its constraints are removed will be con- sistent with the value chosen for SA.",Constraint,4
708,"If the cycle cutset has size c, then the total run time is O(dc · (n − c)d2): we have to try each of the dc combinations of values for the variables in S, and for each combination we must solve a tree problem of size n − c.",Safety,4
709,"If the graph is “nearly a tree,” then c will be small and the savings over straight backtracking will be huge.",Safety,4
710,A tree decomposition must satisfy the following three requirements: • Every variable in the original problem appears in at least one of the subproblems.,Safety,4
711,"• If two variables are connected by a constraint in the original problem, they must appear together (along with the constraint) in at least one of the subproblems.",Constraint,4
712,"• If a variable appears in two subproblems in the tree, it must appear in every subproblem along the path connecting those subproblems.",Safety,4
713,The ﬁrst two conditions ensure that all the variables and constraints are represented in the decomposition.,Safety,4
714,"The third condition seems rather technical, but simply reﬂects the constraint that any given variable must have the same value in every subproblem in which it appears; the links joining subproblems in the tree enforce this constraint.",Safety,4
715,"For example, on the Australia map we know that WA, NT , and SA must all have different colors, but there are 3!",Safety,4
716,"For our example, we might impose an arbitrary ordering constraint, NT < SA < WA, that requires the three values to be in alphabetical order.",Interface,4
717,This constraint ensures that only one of the n!,Safety,4
718,Systematic methods for solving linear equations by variable elimination were studied by Gauss (1829); the solution of linear inequality constraints goes back to Fourier (1827).,Constraint,1
719,"The latter paper provides somewhat convincing evidence that, on harder CSPs, full arc-consistency checking pays off.",Safety,4
720,Marriott and Stuckey (1998) provide excellent coverage of research in this area.,Interface,4
721,A survey of global constraints is provided by van Hoeve and Katriel (2006).,Constraint,4
722,"Their technique of dependency-directed backtracking led to the develop- ment of truth maintenance systems (Doyle, 1979), which we discuss in Section 12.6.2.",Safety,1
723,"Assume that a list of words (i.e., a dictionary) is provided and that the task is to ﬁll in the blank squares by using any subset of the list.",Safety,4
724,Should the variables be words or letters?,Safety,4
725,Which formulation do you think will be better?,Constraint,4
726,"7 LOGICAL AGENTS REASONING REPRESENTATION KNOWLEDGE BASED AGENTS LOGIC In which we design agents that can form representations of a complex world, use a process of inference to derive new representations about the world, and use these new representations to deduce what to do.",Safety,4
727,We begin in Section 7.1 with the overall agent design.,Constraint,4
728,There must be a way to add new sentences to the knowledge base and a way to query what is known.,Safety,4
729,"Inference must obey the requirement that when one ASKs a question of the knowledge base, the answer should follow from what has been told (or TELLed) to the knowledge base previously.",Safety,4
730,"Later in this chapter, we will be more precise about the crucial word “follow.” For now, take it to mean that the inference process should not make things up as it goes along.",Safety,4
731,"Second, it ASKs the knowledge base what action it should In the process of answering this query, extensive reasoning may be done about perform.",Functional,4
732,The details of the representation language are hidden inside three functions that imple- ment the interface between the sensors and actuators on one side and the core representation and reasoning system on the other.,Interface,1
733,MAKE-ACTION-QUERY constructs a sentence that asks what action should be done at the current time.,Safety,4
734,Later sections will reveal these details.,Constraint,4
735,It is amenable to a description at 236 Chapter 7.,Constraint,4
736,"KNOWLEDGE LEVEL IMPLEMENTATION LEVEL DECLARATIVE the knowledge level, where we need specify only what the agent knows and what its goals are, in order to ﬁx its behavior.",Safety,4
737,Then we can expect it to cross the Golden Gate Bridge because it knows that that will achieve its goal.,Safety,4
738,Notice that this analysis is independent of how the taxi works at the implementation level.,Safety,4
739,"It doesn’t matter whether its geographical knowledge is implemented as linked lists or pixel maps, or whether it reasons by manipulating strings of symbols stored in registers or by propagating noisy signals in a network of neurons.",Constraint,4
740,A knowledge-based agent can be built simply by TELLing it what it needs to know.,Constraint,4
741,"Starting with an empty knowledge base, the agent designer can TELL sentences one by one until the agent knows how to operate in its environment.",Constraint,4
742,This is called the declarative ap- proach to system building.,Safety,1
743,"We now understand that a successful agent often combines both declarative and procedural elements in its design, and that declarative knowledge can often be compiled into more efﬁcient procedural code.",Constraint,4
744,We can also provide a knowledge-based agent with mechanisms that allow it to learn for itself.,Constraint,4
745,"The Wumpus World 237 bottomless pits that will trap anyone who wanders into these rooms (except for the wumpus, which is too big to fall in).",Safety,4
746,"• Sensors: The agent has ﬁve sensors, each of which gives a single bit of information: – In the square containing the wumpus and in the directly (not diagonally) adjacent squares, the agent will perceive a Stench.",Safety,3
747,"– In the squares directly adjacent to a pit, the agent will perceive a Breeze.",Safety,4
748,"– In the square where the gold is, the agent will perceive a Glitter.",Safety,4
749,"– When an agent walks into a wall, it will perceive a Bump.",Interface,4
750,"The percepts will be given to the agent program in the form of a list of ﬁve symbols; for example, if there is a stench and a breeze, but no glitter, bump, or scream, the agent program will get [Stench, Breeze, None, None, None].",Safety,4
751,"For an agent in the environment, the main challenge is its initial ignorance of the con- ﬁguration of the environment; overcoming this ignorance seems to require logical reasoning.",Interface,4
752,"Occasionally, the agent must choose between going home empty-handed and risking death to ﬁnd the gold.",Safety,4
753,A cautious agent will move only into a square that it knows to be OK.,Constraint,4
754,"The agent perceives a breeze (denoted by “B”) in [2,1], so there must be a pit in a neighboring square.",Safety,4
755,"The pit cannot be in [1,1], by the rules of the game, so there must be a pit in [2,2] or [3,1] or both.",Safety,4
756,"So the prudent agent will turn around, go back to [1,1], and then proceed to [1,2].",Safety,4
757,"The stench in [1,2] means that there must be a wumpus nearby.",Safety,4
758,"Yet the agent has already inferred that there must be a pit in either [2,2] or [3,1], so this means it must be in [3,1].",Safety,4
759,"In [2,3], the agent detects a glitter, so it should grab the gold and then return home.",Safety,4
760,A logic must also deﬁne the semantics or meaning of sentences.,Safety,4
761,"In standard logics, every sentence must be either true or false in each possible world—there is no “in between.”1 When we need to be precise, we use the term model in place of “possible world.” Whereas possible worlds might be thought of as (potentially) real environments that the agent might or might not be in, models are mathematical abstractions, each of which simply ﬁxes the truth or falsehood of every relevant sentence.",Safety,4
762,"For real haystacks, which are ﬁnite in extent, it seems obvious that a systematic examination can always decide whether the needle is in the haystack.",Safety,1
763,Logical reasoning should en- sure that the new conﬁgurations represent aspects of the world that actually follow from the aspects that the old conﬁgurations represent.,Safety,4
764,"P1,2 is just a symbol; it might mean “there is a pit in [1,2]” or “I’m in Paris today and tomorrow.” The semantics for propositional logic must specify how to compute the truth value of any sentence, given a model.",Safety,4
765,• The truth value of every other proposition symbol must be speciﬁed directly in the model.,Safety,4
766,"The truth table for ⇒ may not quite ﬁt one’s intuitive understanding of “P implies Q” or “if P then Q.” For one thing, propositional logic does not require any relation of causation or relevance between P and Q.",Safety,4
767,"The sentences we write will sufﬁce to derive ¬P1,2 (there is no pit in [1,2]), as was done informally in Section 7.3.",Safety,4
768,"Our ﬁrst algorithm for inference is a model-checking approach that is a direct implementation of the deﬁnition of entailment: enumerate the models, and check that α is true in every model in which KB is true.",Safety,3
769,"The algorithm is sound because it implements directly the deﬁnition of entailment, and complete because it works for any KB and α and always terminates—there are only ﬁnitely many models to examine.",Safety,3
770,"The symbols α, β, and γ stand for arbitrary 7.5 PROPOSITIONAL THEOREM PROVING THEOREM PROVING LOGICAL EQUIVALENCE VALIDITY TAUTOLOGY DEDUCTION THEOREM So far, we have shown how to determine entailment by model checking: enumerating models and showing that the sentence must hold in all models.",Safety,4
771,"Before we plunge into the details of theorem-proving algorithms, we will need some additional concepts related to entailment.",Safety,3
772,The second concept we will need is validity.,Safety,4
773,The ﬁnal concept we will need is satisﬁability.,Safety,4
774,"One ﬁnal property of logical systems is monotonicity, which says that the set of en- tailed sentences can only increase as information is added to the knowledge base.8 For any sentences α and β, if KB |= α then KB ∧ β |= α .",Safety,1
775,Monotonicity means that inference rules can be applied whenever suitable premises are found in the knowledge base—the conclusion of the rule must follow regardless of what else is in the knowledge base.,Safety,4
776,"Search algorithms such as iterative deepening search (page 89) are complete in the sense that they will ﬁnd any reachable goal, but if the available inference rules are inadequate, then the goal is not reachable—no proof exists that uses only those inference rules.",Safety,3
777,There is one more technical aspect of the resolution rule: the resulting clause should contain only one copy of each literal.9 The removal of multiple copies of literals is called factoring.,Safety,4
778,"If ℓi is true, then mj is false, and hence m1 ∨ · · · ∨ mj−1 ∨ mj+1 ∨ · · · ∨ mn must be true, because m1 ∨ · · · ∨ mn is given.",Constraint,4
779,"If ℓi is false, then ℓ1 ∨ · · · ∨ ℓi−1 ∨ ℓi+1 ∨ · · · ∨ ℓk must be true because ℓ1 ∨ · · · ∨ ℓk is given.",Constraint,4
780,"CNF requires ¬ to appear only in literals, so we “move ¬ inwards” by repeated appli- cation of the following equivalences from Figure 7.11: ¬(¬α) ≡ α (double-negation elimination) ¬(α ∧ β) ≡ (¬α ∨ ¬β) (De Morgan) ¬(α ∨ β) ≡ (¬α ∧ ¬β) (De Morgan) In the example, we require just one application of the last rule: (¬B1,1 ∨ P1,2 ∨ P2,1) ∧ ((¬P1,2 ∧ ¬P2,1) ∨ B1,1) .",Safety,1
781,"It is easy to see that RC (S) must be ﬁnite, because there are only ﬁnitely many distinct clauses that can be constructed out of the symbols P1, .",Safety,4
782,"For this to happen, it must be the case that all the other literals in C must already have been falsiﬁed by assignments to P1, .",Safety,4
783,"Thus, C must now look like either (false ∨ false ∨ · · · false ∨Pi) or like (false ∨false ∨· · · false ∨¬Pi).",Constraint,4
784,"If just one of these two is in RC(S), then the algorithm will assign the appropriate truth value to Pi to make C true, so C can only be falsiﬁed if both of these clauses are in RC(S).",Safety,3
785,"Now, since RC(S) is closed under resolution, it will contain the resolvent of these two clauses, and that resolvent will have all of its literals already falsiﬁed by the assignments to P1, .",Safety,4
786,"Some real-world knowledge bases satisfy certain restrictions on the form of sentences they contain, which enables them to use a more restricted and efﬁcient inference algorithm.",Safety,3
787,"In AND–OR graphs, multiple links joined by an arc indicate a conjunction—every link must be proved—while multiple links without an arc indicate a disjunction—any link can be proved.",Interface,4
788,Forward chaining is also complete: every entailed atomic sentence will be derived.,Constraint,4
789,∧ ak must be true in the model and b must be false in the model.,Safety,4
790,"Furthermore, any atomic sentence q that is entailed by the KB must be true in all its models and in this model in particular.",Safety,4
791,"Hence, every entailed atomic sentence q must be inferred by the algorithm.",Safety,3
792,"For example, if I am indoors and hear rain starting to fall, it might occur to me that the picnic will be canceled.",Safety,4
793,"Yet it will probably not occur to me that the seventeenth petal on the largest rose in my neighbor’s garden will get wet; humans keep forward chaining under careful control, lest they be swamped with irrelevant consequences.",Constraint,4
794,"As with forward chaining, an efﬁcient implementation runs in linear time.",Constraint,4
795,"It is useful for answering speciﬁc questions such as “What shall I do now?” and “Where are my keys?” Often, the cost of backward chaining is much less than linear in the size of the knowledge base, because the process touches only relevant facts.",Safety,4
796,"The algorithm is in fact the version described by Davis, Logemann, and Loveland (1962), so we will call it DPLL after the ini- tials of all four authors.",Safety,3
797,"It embodies three improvements over the simple scheme of TT-ENTAILS?: • Early termination: The algorithm detects whether the sentence must be true or false, even with a partially completed model.",Constraint,3
798,"Obviously, for this clause to be true, C must be set to false.",Constraint,4
799,any attempt to prove (by refutation) a literal that is already in the knowledge base will succeed immediately (Exercise 7.22).,Safety,4
800,What Figure 7.17 does not show are the tricks that enable SAT solvers to scale up to large problems.,Safety,4
801,"Restarting does not guarantee that a solution will be found faster, but it does reduce the variance on the time to solution.",Safety,4
802,"Clever indexing (as seen in many algorithms): The speedup methods used in DPLL itself, as well as the tricks used in modern solvers, require fast indexing of such things as “the set of clauses in which variable Xi appears as a positive literal.” This task is complicated by the fact that the algorithms are interested only in the clauses that have not yet been satisﬁed by previous assignments to variables, so the indexing structures must be updated dynamically as the computation proceeds.",Safety,3
803,"They have revolutionized areas such as hardware veriﬁcation and security protocol veriﬁcation, which previously required laborious, hand-guided proofs.",Constraint,4
804,"These algorithms can be applied di- rectly to satisﬁability problems, provided that we choose the right evaluation function.",Functional,3
805,"Be- cause the goal is to ﬁnd an assignment that satisﬁes every clause, an evaluation function that counts the number of unsatisﬁed clauses will do the job.",Interface,3
806,"The space usually contains many local minima, to escape from which various forms of random- ness are required.",Safety,4
807,"If we set max ﬂips = ∞ and p > 0, WALKSAT will eventually return a model (if one exists), because the random-walk steps will eventually hit Section 7.6.",Safety,4
808,"On the other hand, WALKSAT cannot always detect unsatisﬁability, which is required for deciding entailment.",Safety,4
809,"Easy problems can be solved by any old algo- rithm, but because we know that SAT is NP-complete, at least some problem instances must require exponential run time.",Constraint,4
810,"To go beyond these basic intuitions, we must deﬁne exactly how random sentences are generated.",Constraint,4
811,"The ﬁrst step is to enable the agent to deduce, to the extent possible, the state of the world given its percept history.",Safety,4
812,This requires writing down a complete logical model of the effects of actions.,Safety,4
813,"For each pair of locations, we add a sentence saying that at least one of them must be wumpus-free: ¬W1,1 ∨ ¬W1,2 ¬W1,1 ∨ ¬W1,3 · · · ¬W4,3 ∨ ¬W4,4 .",Constraint,4
814,"If there is currently a stench, one might suppose that a proposition Stench should be added to the knowledge base.",Safety,4
815,The need to do this gives rise to the frame problem.11 One possible solution to the frame problem would 10 Section 7.4.3 conveniently glossed over this requirement.,Safety,4
816,"In a world with m different actions and n ﬂuents, the set of frame axioms will be of size O(mn).",Constraint,4
817,Solving the repre- sentational frame problem requires deﬁning the transition model with a set of axioms of size O(mk) rather than size O(mn).,Constraint,4
818,"Thus, for each ﬂuent F , we will have an axiom that deﬁnes the truth value of F t+1 in terms of ﬂuents (including F itself) at time t and the actions that may have occurred at time t.",Interface,4
819,"Logical Agents Given a complete set of successor-state axioms and the other axioms listed at the begin- ning of this section, the agent will be able to ASK and answer any answerable question about the current state of the world.",Safety,4
820,"There is no complete solution within logic; system designers have to use good judgment in deciding how detailed they want to be in specifying their model, and what details they want to leave out.",Safety,1
821,We will see in Chapter 13 that probability theory allows us to summarize all the exceptions without explicitly naming them.,Safety,4
822,The function PLAN-SHOT (not shown) uses PLAN-ROUTE to plan a sequence of actions that will line up this shot.,Functional,3
823,This happens mainly because the required inferences have to go back further and further in time and involve more and more proposition symbols.,Safety,4
824,"That is, exact state estimation may require logical formulas whose size is exponential in the number of symbols.",Safety,4
825,"(Exercise 7.27 explores one possible solution to this problem.) On the other hand, because every literal in the 1-CNF belief state is proved from the previous belief state, and the initial belief state is a true assertion, we know that entire 1-CNF belief state must be true.",Safety,4
826,"It implements the basic idea just given, with one twist.",Constraint,4
827,"Because the agent does not know how many steps it will take to reach the goal, the algorithm tries each possible number of steps t, up to some maximum conceivable plan length Tmax.",Safety,3
828,"Because of the way SATPLAN searches for a solution, this approach cannot be used in a partially observable environment; SATP LAN would just set the unobservable variables to the values it needs to create a solution.",Safety,4
829,"There is, however, a signiﬁcant difference between the requirements for entailment (as tested by ASK) and those for satisﬁability.",Safety,4
830,"Now, SATPLAN will ﬁnd the plan [Forward 0].",Safety,4
831,"The axioms do predict correctly that nothing will happen when such an action is executed (see Exercise 10.14), but they do not say that the action cannot be executed!",Interface,4
832,"To avoid generating plans with illegal actions, we must add precondition axioms stating that an action occurrence requires the preconditions to be satisﬁed.13 For example, we need to say, for each time t, that Shoot t ⇒ HaveArrow t .",Constraint,4
833,"This ensures that if a plan selects the Shoot action at any time, it must be the case that the agent has an arrow at that time.",Interface,4
834,"By imposing action exclusion axioms only on pairs of actions that really do interfere with each other, we can allow for plans that include multiple simultaneous actions—and because SATPLAN ﬁnds the shortest legal plan, we can be sure that it will take advantage of this capability.",Constraint,4
835,"It can be shown that this collection of axioms is sufﬁcient, in the sense that there are no longer any spurious “solutions.” Any model satisfying the propositional sentence will be a valid plan for the original problem.",Safety,4
836,"Logical Agents “for each square [x, y].” For any practical agent, these phrases have to be implemented by code that generates instances of the general sentence schema automatically for insertion into the knowledge base.",Safety,4
837,"Each update step requires inference using the transition model of the environment, which is built from successor- state axioms that specify how each ﬂuent changes.",Safety,4
838,"It also raised the ﬂag of declarativism, pointing out that telling an agent what it needs to know is an elegant way to build software.",Interface,4
839,"The ﬁrst known systematic study of logic was carried out by Aristotle, whose work was assembled by his students after his death in 322 B.C.",Safety,1
840,"Although the syllogisms included elements of both propositional and ﬁrst-order logic, the system as a whole lacked the compositional properties required to handle sentences of arbitrary complexity.",Safety,1
841,and continuing for several centuries thereafter) began the systematic study of the basic logical connectives.,Safety,1
842,George Boole (1847) introduced the ﬁrst comprehensive and workable system of formal logic in his book The Mathematical Analysis of Logic.,Safety,1
843,"Although Boole’s system still fell short of full propositional logic, it was close enough that other mathematicians could quickly ﬁll in the gaps.",Safety,1
844,"William Stanley Jevons, one of those who improved upon and extended Boole’s work, constructed his “logical piano” in 1869 to per- form inferences in Boolean logic.",Constraint,4
845,"Mar- tin Davis (1957) had actually designed a program that came up with a proof in 1954, but the Logic Theorist’s results were published slightly earlier.",Constraint,4
846,(2005) exhibit families of 3-SAT instances for which all known DPLL-like algorithms require exponential running time.,Constraint,3
847,"Also noteworthy is MINISAT (Een and S¨orensson, 2003), an open-source implementation available at http://minisat.se that is designed to be easily modiﬁed and improved.",Interface,4
848,"(1992), who called it GSAT and showed that it was capable of solving a wide range of very hard problems very quickly.",Constraint,4
849,Cook and Mitchell (1997) provide an excellent summary of the early literature on the problem.,Interface,4
850,"Con- trary to popular supposition, the paper was concerned with the implementation of a Boolean circuit-based agent design in the brain.",Constraint,4
851,"(Rosenschein’s approach is described at some length in the second edition of this book.) The work of Rod Brooks (1986, 1989) demonstrates the effectiveness of circuit-based designs for controlling robots—a topic we take up in Chapter 25.",Safety,4
852,"Brooks (1991) argues that circuit-based designs are all that is needed for AI—that representation and reasoning are cumbersome, expensive, and unnecessary.",Constraint,4
853,"(2003) show how a hybrid agent design not too different from our wumpus agent has been used to control NASA spacecraft, planning sequences of actions and diagnosing and recovering from faults.",Constraint,4
854,"Logical state estimation, of course, requires a logical representation of the effects of actions—a key problem in AI since the late 1950s.",Safety,4
855,Thielscher (1999) identiﬁes the inferential frame problem as a separate idea and provides a solution.,Safety,4
856,"In retrospect, one can see that Rosenschein’s (1985) agents were using circuits that implemented successor-state axioms, but Rosenschein did not notice that the frame problem was thereby largely solved.",Safety,4
857,(You should ﬁnd 32 of them.) Mark the worlds in which the KB is true and those in which 280 Chapter 7.,Safety,4
858,The algorithm should run in time linear in the size of the sentence.,Safety,3
859,How much time will the algorithm take to terminate?,Safety,3
860,"If a propositional clause C can be resolved with a copy of itself, it must be logically equivalent to True.",Constraint,4
861,"Show a resolution refutation proof that if the agent is in ¬S at time t and does a, it will still be in ¬S at time t + 1.",Safety,4
862,7.26 Section 7.7.1 provides some of the successor-state axioms required for the wumpus world.,Safety,4
863,"We noted on that page that such an agent will not be able to acquire, maintain, and use more complex beliefs such as the disjunction P3,1 ∨ P2,2.",Interface,4
864,"Our discussion will be cursory, compressing cen- turies of thought, trial, and error into a few paragraphs.",Constraint,4
865,"(Such representations might be considered ad hoc; database systems were developed precisely to provide a more general, domain-independent way to store and 1 Also called ﬁrst-order predicate calculus, sometimes abbreviated as FOL or FOPC.",Interface,1
866,"A second drawback of data structures in programs (and of databases, for that matter) is the lack of any easy way to say, for example, “There is a pit in [2,2] or [3,1]” or “If the wumpus is in [1,1] then he is not in [2,2].” Programs can store a single value for each variable, and some systems allow the value to be “unknown,” but they lack the expressiveness required to handle partial information.",Safety,1
867,"Clearly, noncompositionality makes life much more difﬁcult for the reasoning system.",Safety,1
868,"If we could uncover the rules for natural language, we could use it in representation and reasoning systems and gain the beneﬁt of the billions of pages that have been written in natural language.",Safety,1
869,.” Wanner (1974) did a similar experiment and found that subjects made the right choice at chance level—about 50% of the time—but remembered the content of what they read with better than 90% accuracy.,Constraint,4
870,"First-Order Logic In a ﬁrst-order logic reasoning system that uses CNF, we can see that the linguistic form “¬(A ∨ B)” and “¬A ∧ ¬B” are the same because we can look inside the system and see that the two sentences are stored as the same canonical CNF form.",Safety,1
871,"Given two choices (e.g., “celery” or “airplane”), the system predicts correctly 77% of the time.",Safety,1
872,The system can even predict at above-chance levels for words it has never seen an fMRI image of before (by considering the images of related words) and for people it has never seen before (proving that fMRI reveals some level of common representation across people).,Interface,1
873,"From the viewpoint of formal logic, representing the same knowledge in two different ways makes absolutely no difference; the same facts will be derivable from either represen- tation.",Safety,4
874,"In practice, however, one representation might require fewer steps to derive a conclu- sion, meaning that a reasoner with limited resources could get to the conclusion using one representation but not the other.",Constraint,4
875,"This enables one to represent general laws or rules, such as the statement “Squares neighboring the wumpus are smelly.” The primary difference between propositional and ﬁrst-order logic lies in the ontologi- cal commitment made by each language—that is, what it assumes about the nature of reality.",Safety,4
876,"Systems using probability theory, on the other hand, 2 In contrast, facts in fuzzy logic have a degree of truth between 0 and 1.",Safety,1
877,"In the next section, we will launch into the details of ﬁrst-order logic.",Safety,4
878,"Just as a student of physics requires some familiarity with mathematics, a student of AI must develop a talent for working with logical notation.",Constraint,4
879,The do- DOMAIN DOMAIN ELEMENTS main is required to be nonempty—every possible world must contain at least one object.,Safety,4
880,"Indeed, some fuzzy systems allow uncertainty (degree of belief) about degrees of truth.",Constraint,1
881,"(8.1) Certain kinds of relationships are best considered as functions, in that a given object must be related to exactly one object in this way.",Constraint,3
882,"(8.2) TOTAL FUNCTIONS Strictly speaking, models in ﬁrst-order logic require total functions, that is, there must be a value for every input tuple.",Constraint,3
883,"Thus, the crown must have a left leg and so must each of the left legs.",Safety,4
884,We adopt the convention that these symbols will begin with uppercase letters.,Constraint,4
885,"As in propositional logic, every model must provide the information required to deter- mine if any given sentence is true or false.",Interface,4
886,"If there are two constant symbols and one object, then both symbols must refer to the same object; but this can still happen even with more objects.",Constraint,4
887,"When there are more objects than constant symbols, some of the objects will have no names.",Safety,4
888,"We can reason about left legs (e.g., stating the general rule that everyone has one and then deducing that John must have one) without ever providing a deﬁnition of LeftLeg.",Safety,4
889,"An atomic 5 λ-expressions provide a useful notation in which new function symbols are constructed “on the ﬂy.” For example, the function that squares its argument can be written as (λx x × x) and can be applied to arguments just like any other function symbol.",Functional,3
890,Nested quantiﬁers We will often want to express more complex sentences using multiple quantiﬁers.,Constraint,4
891,In other cases we will have mixtures.,Constraint,4
892,The rule is that the variable belongs to the innermost quantiﬁer that mentions it; then it will not be subject to any other quantiﬁcation.,Safety,4
893,"Because this can be a source of confusion, we will always use different variable names with nested quantiﬁers.",Constraint,4
894,"Because ∀ is really a conjunction over the universe of objects and ∃ is a disjunction, it should not be surprising that they obey De Morgan’s rules.",Safety,4
895,"Still, readability is more important than parsimony, so we will keep both of the quantiﬁers.",Safety,4
896,"As a consequence, humans may make mistakes in translating their knowledge into ﬁrst-order logic, resulting in unintuitive behaviors from logical reasoning systems that use the knowledge.",Safety,1
897,One proposal that is very popular in database systems works as follows.,Constraint,1
898,"Finally, we invoke domain closure, meaning that each model 8 Actually he had four, the others being William and Henry.",Safety,4
899,"Database semantics is also used in logic programming systems, as explained in Section 9.4.5.",Constraint,1
900,"On the other hand, the database semantics requires deﬁnite knowledge of what the world contains.",Safety,4
901,"We have seen some simple sentences illustrating the various aspects of logical syntax; in this section, we provide more systematic representations of some simple domains.",Interface,1
902,"Generally speaking, any query that is logically entailed by the knowledge base should be answered afﬁrmatively.",Safety,4
903,"For example, given the two preceding assertions, the query ASK(KB , Person(John)) should also return true.",Safety,4
904,"It is rather like answering “Can you tell me the time?” with “Yes.” If we want to know what value of x makes the sentence true, we will need a different function, ASKVARS, which we call with ASKVARS(KB , Person(x)) SUBSTITUTION BINDING LIST and which yields a stream of answers.",Constraint,3
905,In this case there will be two answers: {x/John} and {x/Richard }.,Constraint,4
906,"ASKVARS is usually reserved for knowledge bases consisting solely of Horn clauses, because in such knowledge bases every way of making the query true will bind the variables to speciﬁc values.",Safety,4
907,"This domain includes facts such as “Elizabeth is the mother of Charles” and “Charles is the father of William” and rules such as “One’s grandmother is the mother of one’s parent.” Clearly, the objects in our domain are people.",Safety,4
908,"We use functions for Mother and Father , because every person has exactly one of each of these (at least according to nature’s design).",Constraint,3
909,Axioms are commonly associated with purely mathematical domains—we will see some axioms for numbers shortly—but they are needed in all domains.,Constraint,4
910,They provide the basic factual information from which useful conclusions can be derived.,Interface,4
911,"If we ASK the knowledge base this sentence, it should return true.",Safety,4
912,"Without them, a reasoning system has to start from ﬁrst principles every time, rather like a physicist having to rederive the rules of calculus for every new problem.",Safety,1
913,Some provide more general information about certain predicates without constituting a deﬁnition.,Interface,4
914,The answers to these questions will then be theorems that follow from the axioms.,Safety,4
915,"We need a predicate NatNum that will be true of natural numbers; we need one constant symbol, 0; and we need one function symbol, S (successor).",Functional,3
916,"(After reading Section 8.2.8, you will notice that these axioms allow for other natural numbers besides the usual ones; see Exercise 8.13.) We also need axioms to constrain the successor function: ∀ n 0 6= S(n) .",Functional,3
917,We will want to know whether an element is a member of a set and we will want to distinguish sets from objects that are not sets.,Interface,4
918,We will use the normal vocabulary of set theory as syntactic sugar.,Safety,4
919,"The corre- sponding ﬁrst-order sentence stored in the knowledge base must include both the percept and the time at which it occurred; otherwise, the agent will get confused about when it saw what.",Safety,4
920,Simple “reﬂex” behavior can also be implemented by quantiﬁed implication sentences.,Constraint,4
921,"An ornithologist wishing to study migration patterns, survival rates, and so on does name each bird, by means of a ring on its leg, because individual birds must be tracked.",Interface,4
922,"We illustrate the knowledge engi- neering process in an electronic circuit domain that should already be fairly familiar, so that we can concentrate on the representational issues involved.",Interface,4
923,The knowledge engineer must delineate the range of questions that the knowledge base will support and the kinds of facts that will be available for each speciﬁc problem instance.,Safety,4
924,"For example, does the wumpus knowledge base need to be able to choose actions or is it required to answer questions only about the contents of the environment?",Safety,4
925,Will the sensor facts include the current location?,Safety,3
926,The task will determine what knowledge must be represented in order to connect problem instances to answers.,Safety,4
927,This step is analogous to the PEAS process for designing agents in Chapter 2.,Safety,4
928,"(Notice, however, that the deﬁnition of adjacency was not supplied explicitly in the wumpus-world rules.) For real domains, the issue of relevance can be quite difﬁcult—for example, a system for simulating VLSI designs might or might not need to take into account stray capacitances and skin effects.",Safety,1
929,"For example, should pits be represented by objects or by a unary predicate on squares?",Constraint,4
930,Should the agent’s orientation be a function or a predicate?,Functional,3
931,Should the wumpus’s location depend on time?,Safety,4
932,"Often, this step reveals misconceptions or gaps in the vocabulary that must be ﬁxed by returning to step 3 and iterating through the process.",Safety,4
933,"If the ontology is well thought out, this step will be easy.",Safety,4
934,It will involve writing simple atomic sentences about in- stances of concepts that are already part of the ontology.,Safety,4
935,"Alas, the answers to queries will seldom be correct on the ﬁrst try.",Safety,4
936,"More precisely, the answers will be correct for the knowledge base as written, assuming that the inference procedure is sound, but they will not be the ones that the user is expecting.",Safety,4
937,"For example, if an axiom is missing, some queries will not be answerable from the knowledge base.",Interface,4
938,"For example, if the knowledge base includes a diagnostic rule (see Exercise 8.14) for ﬁnding the wumpus, ∀ s Smelly(s) ⇒ Adjacent (Home(Wumpus), s) , instead of the biconditional, then the agent will never be able to prove the absence of wumpuses.",Safety,4
939,"It is impossible to tell whether this statement is correct without looking at the rest of the program to see whether, for example, offset is used to refer to the current position, or to one beyond the current position, or whether the value of position is changed by another statement and so offset should also be changed again.",Safety,4
940,8.4.2 The electronic circuits domain We will develop an ontology and knowledge base that allow us to reason about digital circuits of the kind shown in Figure 8.6.,Interface,4
941,These will be our tasks in this section.,Constraint,4
942,Each of these levels would require additional knowledge.,Constraint,4
943,"To determine what these signals will be, we need to know how the gates transform their input signals.",Safety,4
944,"If our purpose were something other than verifying designs at the gate level, the ontol- ogy would be different.",Safety,4
945,"If we were interested in designing a product that would be proﬁtable, then the cost of the circuit and its speed relative to other products on the market would be important.",Safety,4
946,"Encode general knowledge of the domain One sign that we have a good ontology is that we require only a few general rules, which can be stated clearly and concisely.",Safety,4
947,These are all the axioms we will need: 1.,Safety,4
948,"ASKVARS will give us three such substitutions: {i1/1, i2/1, i3/0} {i1/1, i2/0, i3/1} What are the possible sets of values of all the terminals for the adder circuit?",Safety,4
949,"CIRCUIT VERIFICATION This ﬁnal query will return a complete input–output table for the device, which can be used to check that it does in fact add its inputs correctly.",Safety,4
950,"We can also use the deﬁnition of the circuit to build larger digital systems, for which the same kind of veriﬁcation procedure can be carried out.",Safety,1
951,"(See Exercise 8.28.) Many domains are amenable to the same kind of structured knowledge-base development, in which more complex concepts are deﬁned on top of simpler concepts.",Safety,4
952,"Suddenly, the system will be unable to prove any outputs for the circuit, except for the input cases 000 and 110.",Safety,1
953,"Now the problem is apparent: the system is unable to infer that Signal (Out(1, X1)) = 1, so we need to tell it that 1 6= 0.",Safety,1
954,"The important points are as follows: • Knowledge representation languages should be declarative, compositional, expressive, context independent, and unambiguous.",Safety,4
955,"• Developing a knowledge base in ﬁrst-order logic requires a careful process of analyzing the domain, choosing a vocabulary, and encoding the axioms required to support the desired inferences.",Safety,4
956,"The ﬁrst systematic treatment of relations was given by Augustus De Morgan (1864), who cited the following example to show the sorts of inferences that Aristotle’s logic could not handle: “All horses are animals; therefore, the head of a horse is the head of an animal.” This inference is inaccessible to Aristotle because any valid rule that can support this inference must ﬁrst analyze the sentence using the two-place predicate “x is the head of y.” The logic of relations was studied in depth by Charles Sanders Peirce (1870, 2004).",Interface,1
957,"First-Order Logic Leopold L¨owenheim (1915) gave a systematic treatment of model theory for ﬁrst-order logic, including the ﬁrst proper treatment of the equality symbol.",Safety,1
958,McCarthy (1958) was primarily responsible for the introduction of ﬁrst-order logic as a tool for building AI systems.,Safety,1
959,"Cordell Green (1969a, 1969b) developed a ﬁrst-order reasoning system, QA3, leading to the ﬁrst attempts to build a logical robot at SRI (Fikes and Nilsson, 1971).",Safety,1
960,"In Europe, logic programming (a restricted form of ﬁrst-order reasoning) was developed for linguistic analysis (Colmerauer et al., 1973) and for general declarative systems (Kowalski, 1974).",Constraint,1
961,"Practical applications built with ﬁrst-order logic include a system for evaluating the manufacturing requirements for electronic products (Mannion, 2002), a system for reasoning about policies for ﬁle access and digital rights management (Halpern and Weissman, 2008), and a system for the automated composition of Web services (McIlraith and Zeng, 2001).",Constraint,1
962,"Just as the predictions of a machine learning algorithm depend strongly on the vocabulary supplied to it, so will the child’s formulation of theories depend on the linguistic environment in which learning occurs.",Safety,3
963,"A highly formal treatment of ﬁrst-order logic, along with many more advanced topics in logic, is provided by Bell and Machover (1977).",Constraint,4
964,"Gallier (1986) provides an extremely rigorous mathematical exposition of ﬁrst-order logic, along with a great deal of material on its use in automated reasoning.",Constraint,4
965,"Logical Foundations of Artiﬁcial Intelligence (Genesereth and Nilsson, 1987) is both a solid introduction to logic and the ﬁrst systematic treatment of logical agents with percepts and actions, and there are two good handbooks: van Bentham and ter Meulen (1997) and Robinson and Voronkov (2001).",Constraint,1
966,"For ﬁnd- ing pits, the obvious diagnostic rules say that if a square is breezy, some adjacent square must contain a pit; and if a square is not breezy, then no adjacent square contains a pit.",Safety,4
967,First-Order Logic George Mum Spencer Kydd Elizabeth Philip Margaret Diana Charles Anne Mark Andrew Sarah Edward Sophie William Harry Peter Zara Beatrice Eugenie Louise James A typical family tree.,Constraint,4
968,"Using a suitable logical reasoning system, TELL it all the sentences you have written down, and ASK it who are Elizabeth’s grandchildren, Diana’s brothers-in-law, Zara’s great-grandparents, and Eugenie’s ancestors.",Safety,1
969,"8.19 Write out the axioms required for reasoning about the wumpus’s location, using a constant symbol Wumpus and a binary predicate At(Wumpus, Location ).",Safety,4
970,"Repre- senting this in ﬁrst-order logic, we must write more verbosely ColorOf (WA) = red .",Constraint,4
971,"8.23 Write in ﬁrst-order logic the assertion that every key and at least one of every pair of socks will eventually be lost forever, using only the following vocabulary: Key(x), x is a key; Sock (x), x is a sock; Pair (x, y), x and y are a pair; Now , the current time; Before(t1, t2), time t1 comes before time t2; Lost (x, t), object x is lost at time t.",Safety,4
972,8.24 Translate into ﬁrst-order logic the sentence “Everyone’s DNA is unique and is derived from their parents’ DNA.” You must specify the precise intended meaning of your vocabulary terms.,Safety,4
973,"If an apartment is more expensive than all apartments in London, it must be in Moscow.",Interface,4
974,"8.26 Represent the following sentences in ﬁrst-order logic, using a consistent vocabulary (which you must deﬁne): a.",Safety,4
975,"Exercises 321 Using this representation, deﬁne the one-bit adder in Figure 8.6 and the four-bit adder in Figure 8.8, and explain what queries you would use to verify the designs.",Safety,4
976,Forward chaining and its applica- tions to deductive databases and production systems are covered in Section 9.3; backward chaining and logic programming systems are developed in Section 9.4.,Constraint,1
977,"General ﬁrst-order sentences require resolution-based theorem proving, which is described in Section 9.5.",Constraint,4
978,FIRST-ORDER INFERENCE This section and the next introduce the ideas underlying modern logical inference systems.,Safety,1
979,"Of course, that name must not already belong to another object.",Constraint,4
980,Mathematics provides a nice example: suppose we discover that there is a number that is a little bigger than 2.71828 and that satisﬁes the equation d(xy)/dy = xy for x.,Safety,4
981,Our propositional algorithms will have difﬁculty with an inﬁnitely large set of sentences.,Constraint,3
982,"Our proof procedure can go on and on, generating more and more deeply nested terms, but we will not know whether it is stuck in a hopeless loop or whether the proof is just about to pop out.",Safety,4
983,The sharp-eyed reader (and certainly the computational logicians of the early 1960s) will have noticed that the propositionalization approach is rather inefﬁcient.,Safety,4
984,"In this case, applying the substitution {x/John, y/John} to the implication premises King(x) and Greedy (x) and the knowledge-base sentences King(John) and Greedy (y) will make them identical.",Safety,4
985,"We will see in the rest of this chapter that we can develop lifted versions of the forward chaining, backward chaining, and resolution algorithms introduced in Chapter 7.",Safety,3
986,The key advantage of lifted inference rules over propositionalization is that they make only those substitutions that are required to allow particular inferences to proceed.,Safety,4
987,LIFTING 9.2.2 Uniﬁcation UNIFICATION UNIFIER Lifted inference rules require ﬁnding substitutions that make different logical expressions look identical.,Constraint,4
988,Let us look at some examples of how UNIFY should behave.,Constraint,4
989,"Now, remember that Knows(x, Elizabeth) means “Everyone knows Elizabeth,” so we should be able to infer that John knows Elizabeth.",Constraint,4
990,"Now the uniﬁcation will work: UNIFY(Knows(John, x), Knows(x17, Elizabeth)) = {x/Elizabeth, x17/John} .",Safety,4
991,There is one more complication: we said that UNIFY should return a substitution that makes the two arguments look the same.,Safety,4
992,"There is one expensive step: when matching a variable against a complex term, one must check whether the variable itself occurs inside the term; if it does, the match fails because no consistent uniﬁer can be constructed.",Safety,4
993,"Some systems, including all logic programming systems, simply omit the occur check and sometimes make unsound inferences as a result; other systems use more complex algorithms with linear-time complexity.",Constraint,1
994,The simplest way to implement STORE and FETCH is to keep all the facts in one long list and unify each query against every element of the list.,Safety,4
995,"Answering a query such as Employs(x, Richard ) with predicate indexing would require scanning the entire bucket.",Constraint,4
996,The portion of the lattice above any ground fact can be constructed systematically (Exercise 9.5).,Safety,1
997,"For most AI systems, the number of facts to be stored is small enough that efﬁcient indexing is considered a solved problem.",Safety,1
998,Deﬁnite clauses such as Situation ⇒ Response are especially useful for systems that make inferences in response to newly arrived information.,Constraint,1
999,"Many systems can be deﬁned this way, and forward chaining can be implemented very efﬁciently.",Constraint,1
1000,We will prove that West is a criminal.,Constraint,4
1001,"First, we will represent these facts as ﬁrst-order deﬁnite clauses.",Constraint,4
1002,We will also need to know that missiles are weapons: Missile(x) ⇒ Weapon(x) (9.4) (9.5) (9.6) (9.7) Section 9.3.,Constraint,4
1003,"Forward Chaining and we must know that an enemy of America counts as “hostile”: Enemy(x, America) ⇒ Hostile(x) .",Interface,4
1004,We will see that the absence of function symbols makes inference much easier.,Functional,3
1005,The process repeats until the query is answered (assuming that just one answer is required) or no new facts are added.,Safety,4
1006,"Two iterations are required: • On the ﬁrst iteration, rule (9.3) has unsatisﬁed premises.",Safety,4
1007,"Clearly, there can be no more than pnk distinct ground facts, so after this many iterations the algorithm must have reached a ﬁxed point.",Safety,3
1008,"For the case in which an answer to the query sentence q is entailed, we must appeal to Herbrand’s theorem to establish that the algorithm will ﬁnd a proof.",Interface,3
1009,9.3.3 Efﬁcient forward chaining The forward-chaining algorithm in Figure 9.3 is designed for ease of understanding rather than for efﬁciency of operation.,Safety,3
1010,"Essentially every Datalog clause can be viewed as deﬁning a CSP, so matching will be tractable just when the corresponding CSP is tractable.",Safety,4
1011,Such redundant rule matching can be avoided if we make the following observation: Every new fact inferred on iteration t must be derived from at least one new fact inferred on iteration t − 1.,Safety,4
1012,This is true because any inference that does not require a new fact from iteration t − 1 could have been done at iteration t − 1 already.,Constraint,4
1013,"With suitable indexing, it is easy to identify all the rules that can be triggered by any given fact, and indeed many real systems operate in an “update” mode wherein forward chain- ing occurs in response to each new fact that is TELLed to the system.",Constraint,1
1014,"Rete networks, and various improvements thereon, have been a key component of so- called production systems, which were among the earliest forward-chaining systems in widespread use.4 The XCON system (originally called R1; McDermott, 1982) was built with a production-system architecture.",Constraint,1
1015,X CON contained several thousand rules for designing conﬁgurations of computer components for customers of the Digital Equipment Corporation.,Safety,4
1016,It was one of the ﬁrst clear commercial successes in the emerging ﬁeld of expert systems.,Safety,1
1017,"Many other similar systems have been built with the same underlying technology, which has been implemented in the general-purpose language OPS-5.",Constraint,1
1018,"Production systems are also popular in cognitive architectures—that is, models of hu- man reasoning—such as ACT (Anderson, 1983) and SOAR (Laird et al., 1987).",Constraint,1
1019,"In such sys- tems, the “working memory” of the system models human short-term memory, and the pro- ductions are part of long-term memory.",Safety,1
1020,"In contrast to the typical situation in databases, production systems often have many rules and relatively few facts.",Safety,1
1021,"With suitably optimized matching technology, some modern systems can operate in real time with tens of millions of rules.",Constraint,1
1022,"In our crime example, there were no rules capable of drawing irrelevant conclusions, so the lack of directedness was not a problem.",Safety,4
1023,"In other cases (e.g., if many rules describe the eating habits of Americans and the prices of missiles), FOL-FC-ASK will generate many irrelevant conclusions.",Safety,4
1024,"For example, if the goal is Criminal (West), the rule that concludes Criminal (x) will be rewritten to include an extra conjunct that constrains the value of x: Magic(x) ∧ American(x) ∧ Weapon(y) ∧ Sells(x, y, z) ∧ Hostile(z) ⇒ Criminal (x) .",Interface,4
1025,4 The word production in production systems denotes a condition–action rule.,Safety,1
1026,PRODUCTION SYSTEM COGNITIVE ARCHITECTURES DEDUCTIVE DATABASES MAGIC SET Section 9.4.,Safety,1
1027,"In this way, even if the knowledge base contains facts about millions of Americans, only Colonel West will be considered during the forward inference process.",Safety,4
1028,"FOL-BC-ASK(KB, goal ) will be proved if the knowledge base contains a clause of the form lhs ⇒ goal , where lhs (left-hand side) is a list of conjuncts.",Safety,4
1029,"So we implement FOL-BC-ASK as a generator— a function that returns multiple times, each time giving one possible result.",Functional,3
1030,"Backward chaining is a kind of AND/OR search—the OR part because the goal query can be proved by any rule in the knowledge base, and the AND part because all the conjuncts in the lhs of a clause must be proved.",Safety,4
1031,"This means that its space requirements are linear in the size of the proof (neglecting, for now, the space required to accumulate the solutions).",Safety,4
1032,"We will discuss these problems and some potential solutions, but ﬁrst we show how backward chaining is used in logic programming systems.",Constraint,1
1033,"Figure 9.7 The tree should be read depth ﬁrst, left to right.",Safety,4
1034,"Some of these are in the knowledge base, and others require further backward chaining.",Safety,4
1035,Backward Chaining 339 PROLOG 9.4.2 Logic programming Logic programming is a technology that comes fairly close to embodying the declarative ideal described in Chapter 7: that systems should be constructed by expressing knowledge in a formal language and that problems should be solved by running inference processes on that knowledge.,Safety,1
1036,"Many expert systems have been written in Prolog for legal, medical, ﬁnancial, and other domains.",Constraint,1
1037,"In English, we can read these clauses as (1) appending an empty list with a list Y produces the same list Y and (2) [A|Z] is the result of appending [A|X] onto Y, provided that Z is the result of appending X onto Y.",Constraint,4
1038,Prolog’s design represents a compromise between declarativeness and execution efﬁciency— inasmuch as efﬁciency was understood at the time Prolog was designed.,Safety,4
1039,9.4.3 Efﬁcient implementation of logic programs The execution of a Prolog program can happen in two modes: interpreted and compiled.,Safety,4
1040,We say “essentially” because Prolog interpreters contain a variety of improvements designed to maximize speed.,Constraint,4
1041,"First, our implementation had to explicitly manage the iteration over possible results generated by each of the subfunctions.",Safety,3
1042,"Second, our simple implementation of FOL-BC-ASK spends a good deal of time gener- ating substitutions.",Constraint,4
1043,"When a path in the search fails, Prolog will back up to a previous choice point, and then it might have to unbind some variables.",Safety,4
1044,"Even the most efﬁcient Prolog interpreters require several thousand machine instruc- tions per inference step because of the cost of index lookup, uniﬁcation, and building the recursive call stack.",Safety,4
1045,"A compiled Prolog 5 Note that if the Peano axioms are provided, such goals can be solved by inference within a Prolog program.",Safety,4
1046,"Warren, one of the implementers of the ﬁrst Prolog com- piler.",Safety,4
1047,This will undo any bindings generated by the ﬁrst call to UNIFY.,Safety,4
1048,• The trickiest part is the use of continuations to implement choice points.,Safety,4
1049,You can think of a continuation as packaging up a procedure and a list of arguments that together deﬁne what should be done next whenever the current goal succeeds.,Safety,4
1050,"We then CALL the continuation, with the appropriate bindings on the trail, to do whatever should be done next.",Constraint,4
1051,Parallelization can also provide substantial speedup.,Interface,4
1052,"AND-parallelism is more difﬁcult to achieve, because solutions for the whole conjunction require consistent bindings for all the variables.",Safety,4
1053,Each conjunctive branch must communicate with the other branches to ensure a global solution.,Safety,4
1054,Finding a path from A1 to J4 requires 877 inferences.,Constraint,4
1055,"We can obtain a similar effect in a backward chaining system using memoization—that is, caching solutions to subgoals as they are found and then reusing those solutions when the subgoal recurs, rather than repeat- ing the previous computation.",Safety,1
1056,"The completion is useful for understanding database semantics, but for practical purposes, if your problem can be described with database semantics, it is more efﬁcient to reason with Prolog or some other database semantics system, rather than translating into FOL and rea- soning with a full FOL theorem prover.",Constraint,1
1057,"In Prolog terms, there must be a ﬁnite number of solutions for any goal with unbound variables.",Constraint,4
1058,"(For example, the goal diff(Q,SA), which says that Queensland and South Australia must be different colors, has six solutions if three colors are allowed.) Inﬁnite-domain CSPs—for example, with integer or real-valued variables—require quite dif- ferent algorithms, such as bounds propagation or linear programming.",Constraint,3
1059,"On the other hand, if we ask triangle(3,4,Z), no solution will be found, because the subgoal Z>=0 cannot be handled by Prolog; we can’t compare an unbound value to 0.",Interface,4
1060,"Standard logic programs are just a special case of CLP in which the solution constraints must be equality constraints—that is, bindings.",Constraint,4
1061,CLP systems incorporate various constraint-solving algorithms for the constraints al- lowed in the language.,Constraint,1
1062,"For example, a system that allows linear inequalities on real-valued variables might include a linear programming algorithm for solving those constraints.",Constraint,1
1063,CLP systems also adopt a much more ﬂexible approach to solving standard logic programming queries.,Constraint,1
1064,"CLP systems therefore combine elements of constraint satisfaction algorithms, logic programming, and deductive databases.",Constraint,1
1065,Several systems that allow the programmer more control over the search order for in- ference have been deﬁned.,Safety,1
1066,The user could write a rule saying that the goal with the fewest variables should be tried ﬁrst or could write domain-speciﬁc rules for particular predicates.,Constraint,4
1067,9.5 RESOLUTION The last of our three families of logical systems is based on resolution.,Safety,1
1068,"9.5.1 Conjunctive normal form for ﬁrst-order logic As in the propositional case, ﬁrst-order resolution requires that sentences be in conjunctive normal form (CNF)—that is, a conjunction of clauses, where each clause is a disjunction of literals.6 Literals can contain variables, which are assumed to be universally quantiﬁed.",Safety,4
1069,"In particular, the CNF sentence will be unsatisﬁable just when the original sentence is unsatisﬁable, so we have a basis for doing proofs by contradiction on the CNF sentences.",Safety,4
1070,"Resolution 347 • Drop universal quantiﬁers: At this point, all remaining variables must be universally quantiﬁed.",Constraint,4
1071,This step may also require ﬂattening out nested conjunctions and disjunctions.,Constraint,4
1072,The uniﬁer must be applied to the entire clause.,Safety,4
1073,We know that either Jack or Curiosity did; thus Jack must have.,Constraint,4
1074,It can be safely skipped by those who are willing to take it on faith.,Constraint,4
1075,"We show that resolution is refutation-complete, which means that if a set of sentences is unsatisﬁable, then resolution will always be able to derive a contradiction.",Constraint,4
1076,"Our goal therefore is to prove the following: if S is an unsatisﬁable set of clauses, then the application of a ﬁnite number of resolution steps to S will yield a contradiction.",Interface,1
1077,"That is, running propositional resolution to completion on S′ will derive a contradiction.",Constraint,4
1078,"This establishes, among other things, that we can never prove all the theorems of mathematics within any given system of axioms.",Safety,1
1079,"In general, for C ′ 2 to have any resolvents, they must be constructed by ﬁrst applying to C1 and C2 the most general uniﬁer of a pair of complementary literals in C1 and C2.",Safety,4
1080,"From this fact, it follows that if the empty clause appears in the resolution closure of S′, it must also appear in the resolution closure of S.",Safety,4
1081,The lifting of theorem proving from ground clauses to ﬁrst-order clauses provides a vast increase in power.,Safety,4
1082,"This increase comes from the fact that the ﬁrst-order proof need instantiate variables only as far as necessary for the proof, whereas the ground-clause methods were required to examine a huge number of arbitrary instantiations.",Safety,3
1083,"However, these axioms will generate a lot of conclusions, most of them not helpful to a proof.",Constraint,4
1084,"Equational uniﬁcation of this kind can be done with efﬁcient algorithms designed for the particular axioms used (commutativity, associativity, and so on) rather than through explicit inference with those axioms.",Constraint,3
1085,Theorem provers using this technique are closely related to the CLP systems described in Section 9.4.,Safety,1
1086,9.5.6 Resolution strategies We know that repeated applications of the resolution inference rule will eventually ﬁnd a proof if one exists.,Safety,1
1087,Unit resolution is a restricted form of resolution in which every resolution step must involve a unit clause.,Constraint,4
1088,"The exact choice of heuristic is up to the user, but generally, the weight of a clause should be correlated with its size or difﬁculty.",Constraint,4
1089,"If the set of support is small relative to the whole knowledge base, the search space will be reduced dramatically.",Safety,4
1090,We have to be careful with this approach because a bad choice for the set of support will make the algorithm incomplete.,Constraint,3
1091,"Thus, theorem-proving research is carried out in the ﬁelds of hardware design, programming languages, and software engineering—not just in AI.",Safety,4
1092,"(See Section 8.4.2 on page 309 for an example.) Logical reasoners designed specially for veriﬁcation have been able to verify entire CPUs, including their timing prop- erties (Srivas and Bickford, 1990).",Interface,4
1093,"The AURA theorem prover has been applied to design circuits that are more compact than any previous design (Wojciechowski and Wojcik, 1983).",Safety,4
1094,"The idea is to constructively prove a theorem to the effect that “there exists a program p satisfying a certain speciﬁcation.” Although fully automated deductive synthesis, as it is called, has not yet become feasible for general-purpose programming, hand-guided deductive synthesis has been successful in designing several novel and sophisticated algorithms.",Safety,3
1095,"Similar techniques are now being applied to software veriﬁcation by systems such as the SPIN model checker (Holzmann, 1997).",Safety,1
1096,"• A lifted version of Modus Ponens uses uniﬁcation to provide a natural and powerful inference rule, generalized Modus Ponens.",Interface,4
1097,"It is also used in production systems, which perform efﬁcient updates with very large rule sets.",Functional,1
1098,"• Backward chaining is used in logic programming systems, which employ sophisti- cated compiler technology to provide very fast inference.",Interface,1
1099,"• The generalized resolution inference rule provides a complete proof system for ﬁrst- order logic, using knowledge bases in conjunctive normal form.",Safety,1
1100,• Several strategies exist for reducing the search space of a resolution system without compromising completeness.,Safety,1
1101,"BIBLIOGRAPHICAL AND HISTORICAL NOTES Gottlob Frege, who developed full ﬁrst-order logic in 1879, based his system of inference on a collection of valid schemas plus a single inference rule, Modus Ponens.",Safety,1
1102,"In AI, resolution was adopted for question-answering systems by Cordell Green and Bertram Raphael (1968).",Constraint,1
1103,"Early AI implementations put a good deal of effort into data struc- tures that would allow efﬁcient retrieval of facts; this work is covered in AI programming texts (Charniak et al., 1987; Norvig, 1992; Forbus and de Kleer, 1993).",Constraint,4
1104,The tech- nology for production systems was developed to support such applications.,Safety,1
1105,"The production system language OPS-5 (Forgy, 1981; Brownston et al., 1985), incorporating the efﬁcient rete match process (Forgy, 1982), was used for applications such as the R1 expert system for minicomputer conﬁguration (McDermott, 1982).",Safety,1
1106,"The SOAR cognitive architecture (Laird et al., 1987; Laird, 2008) was designed to han- dle very large rule sets—up to a million rules (Doorenbos, 1994).",Safety,4
1107,"The ﬁeld of deductive databases began with a workshop in Toulouse in 1977 that brought together experts in logical inference and database systems (Gallaire and Minker, 1978).",Constraint,1
1108,"Meanwhile, in 1972, Alain Colmerauer had developed and implemented Pro- log for the purpose of parsing natural language—Prolog’s clauses were intended initially as context-free grammar rules (Roussel, 1975; Colmerauer et al., 1973).",Safety,4
1109,"Swift and Warren (1994) show how to extend the WAM to handle tabling, enabling Datalog programs to execute an order of magnitude faster than forward-chaining deductive database systems.",Interface,1
1110,(1992) developed the CLP(R) system for handling real-valued constraints.,Constraint,1
1111,Research into mathematical theorem proving began even before the ﬁrst complete ﬁrst-order systems were developed.,Safety,1
1112,"These rules were also developed independently in the context of term-rewriting systems (Knuth and Bendix, 1970).",Safety,1
1113,(1965) to provide a degree of goal-directedness in resolution.,Interface,4
1114,"Genesereth and Nilsson (1987, Chapter 5) provide a short but thorough analysis of a wide variety of control strategies.",Interface,4
1115,"LEANTAP (Beckert and Posegga, 1995) is an efﬁcient theorem prover implemented in only 25 lines of Prolog.",Interface,4
1116,"The COQ system (Bertot et al., 2004) and the E 360 Chapter 9.",Safety,1
1117,"The design of the FM9001 32-bit microprocessor was proved correct by the NQTHM system (Hunt and Brock, 1992).",Safety,1
1118,"From 2002 through 2008, the most successful system has been VAMPIRE (Riazanov and Voronkov, 2002).",Safety,1
1119,"TPTP (Thousands of Problems for Theorem Provers) is a library of theorem-proving problems, useful for comparing the performance of systems (Sutcliffe and Suttner, 1998; Sutcliffe et al., 2006).",Performance,1
1120,"The NUPRL system proved Girard’s paradox (Howe, 1987) and Higman’s Lemma (Murthy and Russell, 1990).",Safety,1
1121,State carefully the conditions that must be satisﬁed by the variables and terms involved.,Safety,4
1122,"Explain how FETCH should work when some of these sentences contain variables; use as examples the sentences in (a) and (b) and the query Employs(x, Father (x)).",Safety,4
1123,"(As mentioned, most standard implementations of Prolog actually do allow this.) Show that such an inference engine will allow the conclusion ∃ y P (q, q) to be inferred from the premise ∀ x ∃ y P (x, y).",Interface,4
1124,"Show that an inference engine that uses such a procedure will likewise allow ∃ q P (q, q) to be inferred from the premise ∀ x ∃ y P (x, y).",Interface,4
1125,"For instance, they will say that the formulas P (Sk1) and P (A) can be uniﬁed under the substitution {Sk1/A}.",Safety,4
1126,FC will infer the literal Q(A).,Safety,4
1127,FC will infer the literal P (B).,Safety,4
1128,BC will return true given the query P (B).,Safety,4
1129,Which of the following indexing schemes S1–S5 enable an efﬁcient solution for which of the queries Q1–Q4 (assuming normal backward chaining)?,Interface,4
1130,Rewrite rules are a key component of equational reason- ing systems.,Constraint,1
1131,9.19 This exercise considers the implementation of search algorithms in Prolog.,Safety,3
1132,You will ﬁnd that depth-ﬁrst search is the easiest way to do this.,Safety,4
1133,"The representation of states is carefully designed so that a state can be treated either as a conjunction of ﬂuents, which can be manipulated by logical inference, or as a set of ﬂuents, which can be manipulated with set operations.",Constraint,4
1134,We saw in Chapter 7 that any system for action description needs to solve the frame problem—to say what changes and what stays the same as the result of the action.,Safety,1
1135,A concise description of the action should mention only ∆; it shouldn’t have to mention all the objects that stay in place.,Safety,4
1136,It is a requirement of action schemas that any variable in the effect must also appear in the precondition.,Safety,4
1137,"That way, when the precondition is matched against the state s, all the variables will be bound, and RESULT(s, a) will therefore have only ground atoms.",Safety,4
1138,Note that some care must be taken to make sure the At predicates are maintained properly.,Safety,4
1139,"Classical Planning Finally, there is the problem of spurious actions such as Fly(P1, JFK , JFK ), which should be a no-op, but which has contradictory effects (according to the deﬁnition, the effect would include At(P1, JFK ) ∧ ¬At(P1, JFK )).",Safety,4
1140,The correct approach is to add inequality preconditions saying that the from and to airports must be different; see another example of this in Figure 10.3.,Safety,4
1141,"The goal will always be to build one or more stacks of blocks, speciﬁed in terms of what blocks are on top 2 The blocks world used in planning research is much simpler than SHRDLU’s version, shown on page 20.",Safety,4
1142,"The action for moving block b from the top of x to the top of y will be Move(b, x, y).",Safety,4
1143,"When x is the Table, this action has the effect Clear (Table), but the table should not become clear; and when y = Table, it has the precondition Clear (Table), but the table does not have to be clear 372 Chapter 10.",Safety,4
1144,"Second, we take the interpretation of Clear (x) to be “there is a clear space on x to hold a block.” Under this interpretation, Clear (Table) will always be true.",Safety,4
1145,"We could live with this problem—it will lead to a larger-than-necessary search space, but will not lead to incorrect answers—or we could introduce the predicate Block and add Block (b) ∧ Block (y) to the precondition of Move.",Constraint,4
1146,"But if we add function symbols to the language, then the number of states becomes inﬁnite, and PlanSAT becomes only semidecidable: an algorithm exists that will terminate with the correct answer for any solvable problem, but may not terminate on unsolvable problems.",Functional,3
1147,"To do well on easier-than-worst-case problems, we will need good search heuristics.",Constraint,4
1148,"That’s the true advantage of the classical planning formalism: it has facilitated the develop- ment of very accurate domain-independent heuristics, whereas systems based on successor- state axioms in ﬁrst-order logic have had less success in coming up with good heuristics.",Constraint,1
1149,"10.2.1 Forward (progression) state-space search Now that we have shown how a planning problem maps into a search problem, we can solve planning problems with any of the heuristic search algorithms from Chapter 3 or a local search algorithm from Chapter 4 (provided we keep track of the actions used to reach the goal).",Constraint,3
1150,"Happily, the PDDL representation was designed to make it easy to regress actions—if a domain can be expressed in PDDL, then we can do regression search on it.",Safety,4
1151,"That is, the effects that were added by the action need not have been true before, and also the preconditions must have held before, or else the action could not have been executed.",Safety,4
1152,(Note that we have standardized variable names (changing p to p′ in this case) so that there will be no confusion between variable names if we happen to use the same action schema twice in a plan.,Safety,4
1153,"The same approach was used in Chapter 9 for ﬁrst-order logical inference.) This represents unloading the package from an unspeciﬁed plane at SFO; any plane will do, but we need not say which one now.",Interface,4
1154,For an action to be relevant to a goal it obviously must contribute to the goal: at least one of the action’s effects (either positive or negative) must unify with an element of the goal.,Interface,4
1155,What is less obvious is that the action must not have any effect (positive or negative) that negates an element of the goal.,Interface,4
1156,That is the main reason why the majority of current systems favor forward search.,Safety,1
1157,"By deﬁnition, there is no way to analyze an atomic state, and thus it it requires some ingenuity by a human analyst to deﬁne good domain-speciﬁc heuristics for search problems with atomic states.",Constraint,4
1158,"This almost implies that the number of steps required to solve the relaxed problem is the number of unsatisﬁed goals—almost but not quite, because (1) some action may achieve multiple goals and (2) some actions may undo the effects of others.",Safety,4
1159,"Then, we count the minimum number of actions required such that the union of those actions’ effects satisﬁes the goal.",Safety,4
1160,"Assume for a moment that all goals and preconditions contain only positive literals3 We want to create a relaxed version of the original problem that will be easier to solve, and where the length of the solution will serve as a good heuristic.",Safety,4
1161,That makes it possible to make monotonic progress towards the goal—no action will ever undo progress made by another action.,Safety,4
1162,"There are no dead ends, so no need for backtracking; a simple hillclimbing search will easily ﬁnd a solution to these problems (although it may not be an optimal solution).",Interface,4
1163,"A solution in this abstract state space will be shorter than a solution in the original space (and thus will be an admissible heuristic), and the abstract solution is easy to extend to a solution to the original problem (by adding additional Load and Unload actions).",Interface,4
1164,We show in Section 10.3.1 that planning graphs can help provide better heuristic estimates.,Interface,4
1165,"An example of a system that makes use of effective heuristics is FF, or FASTFORWARD (Hoffmann, 2005), a forward state-space searcher that uses the ignore-delete-lists heuristic, estimating the heuristic with the help of a planning graph (see Section 10.3).",Constraint,1
1166,"If it is possible that either P or ¬P could hold, then both will be represented in Si.",Constraint,4
1167,"(A literal will never show up too late.) Despite the possible error, the level j at which a literal ﬁrst appears is a good estimate of how difﬁcult it is to achieve the literal from the initial state.",Safety,4
1168,"In general, if two literals are mutex at Si, then the persistence actions for those literals will be mutex at Ai and we need not draw that mutex link.",Safety,4
1169,We shall see shortly how mutex links are computed.,Safety,4
1170,"It is important to note that the process of constructing the planning graph does not require choosing among actions, which would entail combinatorial search.",Safety,4
1171,"For our problem, the level-sum heuristic estimate for the conjunctive goal Have(Cake) ∧ Eaten(Cake) will be 0 + 1 = 1, whereas the correct answer is 2, achieved by the plan [Eat(Cake), Bake(Cake)].",Safety,4
1172,"10.3.2 The GRAPHPLAN algorithm This subsection shows how to extract a plan directly from the planning graph, rather than just using the graph to provide a heuristic.",Interface,3
1173,"That means that a solution might exist, and EXTRACT-SOLUTION will try to ﬁnd it.",Constraint,4
1174,"We know that planning is PSPACE-complete and that constructing the planning graph takes polynomial time, so it must be the case that solution extraction is intractable in the worst case.",Safety,4
1175,Here we show that GRAPHPLAN will in fact terminate and return failure when there is no solution.,Constraint,4
1176,"The graph will level off at level 4, reﬂecting the fact that for any single piece of cargo, we can load it, ﬂy it, and unload it at the destination in three steps.",Safety,4
1177,"But that does not mean that a solution can be extracted from the graph at level 4; in fact a solution will require 4n − 1 steps: for each piece of cargo we load, ﬂy, and unload, and for all but the last piece we need to ﬂy back to airport A to get the next piece.",Safety,4
1178,"If the function EXTRACT-SOLUTION fails to ﬁnd a solution, then there must have been at least one set of goals that were not achievable and were marked as a no-good.",Functional,3
1179,"So if it is possible that there might be fewer no-goods in the next level, then we should continue.",Safety,4
1180,Classical Planning Now all we have to do is prove that the graph and the no-goods will always level off.,Safety,4
1181,"The properties are as follows: • Literals increase monotonically: Once a literal appears at a given level, it will appear at all subsequent levels.",Safety,4
1182,"• Actions increase monotonically: Once an action appears at a given level, it will appear at all subsequent levels.",Interface,4
1183,"This is a consequence of the monotonic increase of literals; if the preconditions of an action appear at one level, they will appear at subsequent levels, and thus so will the action.",Interface,4
1184,"• Mutexes decrease monotonically: If two actions are mutex at a given level Ai, then they will also be mutex for all previous levels at which they both appear.",Constraint,4
1185,"The proof can be handled by cases: if actions A and B are mutex at level Ai, it must be because of one of the three types of mutex.",Safety,4
1186,"The ﬁrst two, inconsistent effects and interference, are properties of the actions themselves, so if the actions are mutex at Ai, they will be mutex at every level.",Safety,4
1187,"The third case, competing needs, depends on conditions at level Si: that level must contain a precondition of A that is mutex with a precondition of B.",Constraint,4
1188,"But we already know that the available actions are increasing monotonically, so, by induction, the mutexes must be decreasing.",Safety,4
1189,"Because the actions and literals increase monotonically and because there are only a ﬁnite number of actions and literals, there must come a level that has the same number of actions and literals as the previous level.",Safety,4
1190,"Because mutexes and no-goods decrease, and because there can never be fewer than zero mutexes or no-goods, there must come a level that has the same number of mutexes and no-goods as the previous level.",Safety,4
1191,"Other Classical Planning Approaches 387 Year Track Winning Systems (approaches) Satisﬁcing Satisﬁcing 2008 Optimal 2008 2006 Optimal 2006 2004 Optimal Satisﬁcing 2004 2002 Automated 2002 Hand-coded 2000 Automated 2000 Hand-coded 1998 Automated GAMER (model checking, bidirectional search) LAMA (fast downward search with FF heuristic) SATPLAN, MAXPLAN (Boolean satisﬁability) SGPLAN (forward search; partitions into independent subproblems) SATPLAN (Boolean satisﬁability) FAST DIAGONALLY DOWNWARD (forward search with causal graph) LPG (local search, planning graphs converted to CSPs) TLPLAN (temporal action logic with control rules for forward search) FF (forward search) TALPLANNER (temporal action logic with control rules for forward search) IPP (planning graphs); HSP (forward search) Some of the top-performing systems in the International Planning Compe- Figure 10.11 tition.",Constraint,1
1192,"Each year there are various tracks: “Optimal” means the planners must produce the shortest possible plan, while “Satisﬁcing” means nonoptimal solutions are accepted.",Safety,4
1193,"Figure 10.11 shows some of the top systems in the International Planning Competitions, which have been held every even year since 1998.",Safety,1
1194,"These ground actions are not part of the translation, but will be used in subsequent steps.",Safety,4
1195,"• Add precondition axioms: For each ground action A, add the axiom At ⇒ PRE(A)t, that is, if an action is taken at time t, then the preconditions must have been true.",Interface,4
1196,"A solution to an air cargo problem consists of a totally ordered sequence of actions, yet if 30 packages are being loaded onto one plane in one airport and 50 packages are being loaded onto another at another airport, it seems pointless to come up with a strict linear ordering of 80 load actions; the two subsets of actions should be thought of independently.",Constraint,4
1197,Boxes represent actions and arrows indicate that one action must occur before another.,Constraint,4
1198,"By 2000, forward-search planners had developed excellent heuristics that allowed them to efﬁciently discover the independent subproblems that partial- order planning was designed for.",Safety,4
1199,"Many of these systems use libraries of high-level plans, as described in Section 11.2.",Constraint,1
1200,"Quite possibly, new techniques will emerge that dominate existing methods.",Constraint,3
1201,"For example, in the blocks world, if the goal is to build a tower (e.g., A on B, which in turn is on C, which in turn is on the Table, as in Figure 10.4 on page 371), then the subgoals are serializable bottom to top: if we ﬁrst achieve C on Table, we will never have to undo it while we are achieving the other subgoals.",Safety,4
1202,"This is perhaps not too surprising, because a spacecraft is designed by its engineers to be as easy as possible to control (subject to other constraints).",Constraint,4
1203,"Planners such as GRAPHPLAN, SATPLAN, and FF have moved the ﬁeld of planning forward, by raising the level of performance of planning systems, by clarifying the repre- sentational and combinatorial issues involved, and by the development of useful heuristics.",Performance,1
1204,"However, there is a question of how far these techniques will scale.",Constraint,4
1205,"It seems likely that further progress on larger problems cannot rely only on factored and propositional representations, and will require some kind of synthesis of ﬁrst-order and hierarchical representations with the efﬁcient heuristics currently in use.",Constraint,4
1206,The points to remember: • Planning systems are problem-solving algorithms that operate on explicit propositional or relational representations of states and actions.,Safety,1
1207,Competition and cross-fertilization among the approaches have resulted in signiﬁcant gains in efﬁciency for planning systems.,Safety,1
1208,"STRIPS (Fikes and Nilsson, 1971), the ﬁrst major planning system, illustrates the interaction of these inﬂu- ences.",Safety,1
1209,STRIPS was designed as the planning component of the software for the Shakey robot project at SRI.,Safety,4
1210,"Its overall control structure was modeled on that of GPS, the General Problem Solver (Newell and Simon, 1961), a state-space search system that used means–ends anal- ysis.",Safety,1
1211,"It cannot solve some very simple problems, such as the Sussman anomaly (see Exercise 10.7), found by Allen Brown during experimen- tation with the HACKER system (Sussman, 1975).",Constraint,1
1212,A complete planner must allow for inter- leaving of actions from different subplans within a single sequence.,Constraint,4
1213,"The construction of partially ordered plans (then called task networks) was pioneered by the NOAH planner (Sacerdoti, 1975, 1977) and by Tate’s (1975b, 1977) NONLIN system.",Safety,1
1214,"Chapman’s work led to a straightforward description of a complete partial- order planner (McAllester and Rosenblitt, 1991), then to the widely distributed implementa- tions SNLP (Soderland and Weld, 1991) and UCPOP (Penberthy and Weld, 1992).",Safety,4
1215,"Avrim Blum and Merrick Furst (1995, 1997) revitalized the ﬁeld of planning with their GRAPHPLAN system, which was orders of magnitude faster than the partial-order planners of the time.",Constraint,1
1216,"Other graph-planning systems, such as IPP (Koehler et al., 1997), STAN (Fox and Long, 1998), and SGP (Weld et al., 1998), soon followed.",Constraint,1
1217,A systematic analysis was carried out by Ernst et al.,Constraint,1
1218,GRAPHPLAN and SATPLAN have trouble in domains with many objects because that means they must create many actions.,Constraint,4
1219,"Weld (1994, 1999) provides two excellent surveys of planning algorithms of It is interesting to see the change in the ﬁve years between the two surveys: the 1990s.",Safety,3
1220,"There are also specialized conferences such as the International Conference on AI Planning Systems, the International Workshop on Planning and Scheduling for Space, and the European Conference on Planning.",Safety,1
1221,A box is available that will enable the monkey to reach the bananas if he climbs on it.,Safety,4
1222,"The monkey and box have height Low , but if the monkey climbs onto the box he will have height High, the same as the bananas.",Safety,4
1223,Can this goal be solved by a classical planning system?,Safety,1
1224,"Your schema for pushing is probably incorrect, because if the object is too heavy, its position will remain the same when the Push schema is applied.",Safety,4
1225,10.5 The original STRIPS planner was designed to control Shakey the robot.,Safety,4
1226,"The robot itself could not climb on a box or toggle a switch, but the planner was capable of ﬁnding and printing out plans that were beyond the robot’s abilities.",Safety,4
1227,"Shakey’s six actions are the following: • Go(x, y, r), which requires that Shakey be At x and that x and y are locations In the same room r.",Safety,4
1228,You will need the predicate Box and constants for the boxes.,Safety,4
1229,We will need the predicate On and the constant Floor .,Safety,4
1230,"To turn a light on or off, Shakey must be on top of a box at the light switch’s location.",Safety,4
1231,Show that the axioms predict that nothing will happen when an action is executed in a state where its preconditions are not satisﬁed.,Interface,4
1232,Consider a plan p that contains the actions required to achieve a goal but also includes illegal actions.,Safety,4
1233,"With ﬁrst-order successor-state axioms in situation calculus, is it possible to prove that a plan containing illegal actions will achieve the goal?",Constraint,4
1234,Explain how the situation calculus knowledge base must be modiﬁed.,Safety,4
1235,Will this always return a plan if one exists with length less than or equal to Tmax?,Constraint,4
1236,"Section 11.3 presents agent architectures that can handle uncertain envi- ronments and interleave deliberation with execution, and gives some examples of real-world systems.",Constraint,1
1237,"The approach we take in this section is “plan ﬁrst, schedule later”: that is, we divide the overall problem into a planning phase in which actions are selected, with some ordering constraints, to meet the goals of the problem, and a later scheduling phase, in which tempo- ral information is added to the plan to ensure that it meets resource and deadline constraints.",Constraint,4
1238,The notation A ≺ B means that action A must precede action B.,Safety,4
1239,"The automated methods of Chapter 10 can also be used for the planning phase, provided that they produce plans with just the minimal ordering constraints required for correctness.",Constraint,3
1240,Each action has a duration and a set of resource constraints required by the action.,Constraint,4
1241,"Each constraint speciﬁes a type of resource (e.g., bolts, wrenches, or pilots), the number of that resource required, and whether that resource is consumable (e.g., the bolts are no longer available for use) or reusable (e.g., a pilot is occupied during a ﬂight but is available again when the ﬂight is over).",Safety,4
1242,A solution to a job-shop scheduling problem must specify the start times for each action and must satisfy all the tem- poral ordering constraints and resource constraints.,Constraint,4
1243,"To minimize makespan (plan duration), we must ﬁnd the earliest start times for all the actions consistent with the ordering constraints supplied with the problem.",Constraint,4
1244,"We can see in Figure 11.2 that the whole plan will take 85 minutes, that each action in the top job has 15 minutes of slack, and that each action on the critical path has no slack (by deﬁnition).",Safety,4
1245,"Grey rectangles represent time intervals during which an action may be executed, provided that the ordering constraints are respected.",Constraint,4
1246,require the same EngineHoist and so cannot overlap.,Safety,4
1247,This is 30 minutes longer than the 85 minutes required for a schedule without resource constraints.,Constraint,4
1248,"Notice that there is no time at which both inspectors are required, so we can immediately move one of our two inspectors to a more productive position.",Constraint,4
1249,"Under these assumptions, every scheduling problem can be solved by a nonoverlapping sequence that avoids all resource conﬂicts, provided that each action is feasible by itself.",Constraint,4
1250,"To bridge this gap, AI systems will probably have to do what humans appear to do: plan at higher levels of abstraction.",Constraint,1
1251,"A reasonable plan for the Hawaii vacation might be “Go to San Francisco airport; take Hawaiian Airlines ﬂight 11 to Honolulu; do vacation stuff for two weeks; take Hawaiian Airlines ﬂight 12 back to San Francisco; go home.” Given such a plan, the action “Go to San Francisco airport” can be viewed as a planning task in itself, with a solution such as “Drive to the long-term parking lot; park; take the shuttle to the terminal.” Each of these actions, in turn, can be decomposed further, until we reach the level of actions that can be executed without deliberation to generate the required motor control sequences.",Constraint,4
1252,"Thus, that particular action will remain at an abstract level prior to the execution phase.",Interface,4
1253,REFINEMENT IMPLEMENTATION Deﬁnitions of possible reﬁnements for two high-level actions: going to San Figure 11.4 Francisco airport and navigating in the vacuum world.,Safety,4
1254,An HLA reﬁnement that contains only primitive actions is called an implementation of the HLA.,Interface,4
1255,"For example, in the vacuum world, the sequences [Right, Right, Down] and [Down, Right, Right ] both implement the HLA Navigate([1, 3], [3, 2]).",Safety,4
1256,An implementation of a high-level plan (a sequence of HLAs) is the concatenation of implementations of each HLA in the sequence.,Interface,4
1257,"Given the precondition–effect deﬁnitions of each primitive action, it is straightforward to determine whether any given implementation of a high-level plan achieves the goal.",Safety,4
1258,"We can say, then, that a high-level plan achieves the goal from a given state if at least one of its implementations achieves the goal from that state.",Safety,4
1259,"The “at least one” in this deﬁnition is crucial—not all implementations need to achieve the goal, because the agent gets 1 HTN planners often allow reﬁnement into partially ordered plans, and they allow the reﬁnements of two different HLAs in a plan to share actions.",Safety,4
1260,Planning and Acting in the Real World to decide which implementation it will execute.,Safety,4
1261,"Thus, the set of possible implementations in HTN planning—each of which may have a different outcome—is not the same as the set of possible outcomes in nondeterministic planning.",Safety,4
1262,"There, we required that a plan work for all outcomes because the agent doesn’t get to choose the outcome; nature does.",Safety,4
1263,The simplest case is an HLA that has exactly one implementation.,Interface,4
1264,"In that case, we can compute the preconditions and effects of the HLA from those of the implementation (see Exercise 11.2) and then treat the HLA exactly as if it were a primitive action itself.",Safety,4
1265,"When HLAs have multiple possible implementations, there are two options: one is to search among the implementations for one that works, as in Section 11.2.2; the other is to reason directly about the HLAs—despite the multiplicity of implementations—as explained in Section 11.2.3.",Safety,4
1266,"The latter method enables the derivation of provably correct abstract plans, without the need to consider their implementations.",Safety,3
1267,"11.2.2 Searching for primitive solutions HTN planning is often formulated with a single “top level” action called Act, where the aim is to ﬁnd an implementation of Act that achieves the goal.",Constraint,4
1268,"For example, classical planning problems can be deﬁned as follows: for each primitive action ai, provide one reﬁnement of Act with steps [ai, Act].",Constraint,4
1269,"This says that if the goal is already achieved, then the right implementation is to do nothing.",Safety,4
1270,One possible implementation based on breadth-ﬁrst tree search is shown in Figure 11.5.,Constraint,4
1271,It is straightforward to design a graph-search version of the algorithm as well as depth-ﬁrst and iterative deepening versions.,Safety,3
1272,"Hierarchical Planning 409 function HIERARCHICAL-SEARCH( problem, hierarchy ) returns a solution, or failure frontier ← a FIFO queue with [Act] as the only element loop do /* chooses the shallowest plan in frontier */ if EMPTY?( frontier ) then return failure plan ← POP( frontier ) hla ← the ﬁrst HLA in plan, or null if none preﬁx ,suﬃx ← the action subsequences before and after hla in plan outcome ← RESULT(problem.INITIAL-STATE, preﬁx ) if hla is null then /* so plan is primitive and outcome is its result */ if outcome satisﬁes problem.GOAL then return plan else for each sequence in REFINEMENTS(hla, outcome, hierarchy ) do frontier ← INSERT(APPEND( preﬁx , sequence, suﬃx ), frontier ) A breadth-ﬁrst implementation of hierarchical forward planning search.",Functional,3
1273,"The key to HTN planning, then, is the construction of a plan library containing known methods for implementing complex, high-level actions.",Safety,3
1274,"After the excruciating ex- perience of constructing a plan from scratch, the agent can save the plan in the library as a method for implementing the high-level action deﬁned by the task.",Safety,3
1275,"This contradicts common sense: one should be able to determine that the two-HLA high-level plan [Drive(Home, SFOLongTermParking ), Shuttle(SFOLongTermParking , SFO )] gets one to the airport without having to determine a precise route, choice of parking spot, and so on.",Safety,4
1276,"For this to work, it has to be the case that every high-level plan that “claims” to achieve the goal (by virtue of the descriptions of its steps) does in fact achieve the goal in the sense deﬁned earlier: it must have at least one implementation that does achieve the goal.",Safety,4
1277,"Writing HLA descriptions that satisfy the downward reﬁnement property is, in princi- ple, easy: as long as the descriptions are true, then any high-level plan that claims to achieve the goal must in fact do so—otherwise, the descriptions are making some false claim about what the HLAs do.",Safety,4
1278,We have already seen how to write true descriptions for HLAs that have exactly one implementation (Exercise 11.2); a problem arises when the HLA has multiple implementations.,Safety,4
1279,How can we describe the effects of an action that can be implemented in many different ways?,Interface,4
1280,One safe answer (at least for problems where all preconditions and goals are positive) is to include only the positive effects that are achieved by every implementation of the HLA and the negative effects of any implementation.,Safety,4
1281,"Consider again the HLA Go(Home, SFO ), which has two reﬁnements, and suppose, for the sake of argu- ment, a simple world in which one can always drive to the airport and park, but taking a taxi requires Cash as a precondition.",Safety,4
1282,Requiring that an effect hold for every implementation is equivalent to assuming that someone else—an adversary—will choose the implementation.,Interface,4
1283,"For our case, the agent itself will choose the implementation.",Safety,4
1284,"Black Figure 11.6 and gray arrows indicate possible implementations of h1 and h2, respectively.",Constraint,4
1285,"The basic concept required for understanding angelic se- mantics is the reachable set of an HLA: given a state s, the reachable set for an HLA h, written as REACH(s, h), is the set of states reachable by any of the HLA’s implementations.",Interface,4
1286,"We will come back to the algorithmic issues later; ﬁrst, we consider the question of how the effects of an HLA—the reachable set for each possible initial state—are represented.",Interface,3
1287,"(With conditional effects (see Section 11.3.1) there is a fourth possibility: ﬂipping a variable to its opposite.) An HLA under angelic semantics can do more: it can control the value of a variable, setting it to true or false depending on which implementation is chosen.",Constraint,4
1288,"Thus, we see that the descriptions of HLAs take a taxi), so it should have the effect e are derivable, in principle, from the descriptions of their reﬁnements—in fact, this is required if we want true HLA descriptions, such that the downward reﬁnement property holds.",Safety,4
1289,"Now, if only B is true in the initial state and the goal is A ∧ C then the sequence [h1, h2] achieves the goal: we choose an implementation of h1 that makes B false, then choose an implementation of h2 that leaves A true and makes C true.",Interface,4
1290,"It would be nice if this were always true, but in many cases we can only approximate the ef- fects because an HLA may have inﬁnitely many implementations and may produce arbitrarily wiggly reachable sets—rather like the wiggly-belief-state problem illustrated in Figure 7.21 on page 271.",Interface,4
1291,"For example, we said that Go(Home, SFO ) possibly deletes Cash; it also possibly adds At(Car , SFOLongTermParking ); but it cannot do both—in fact, it must do exactly one.",Constraint,4
1292,"We will +(s, h) of an HLA h may use two kinds of approximation: an optimistic description REACH −(s, h) may understate overstate the reachable set, while a pessimistic description REACH the reachable set.",Interface,4
1293,"With approximate descriptions, the test for whether a plan achieves the goal needs to be modiﬁed slightly.",Constraint,4
1294,Prudence would indicate that this ambitious plan needs to be reﬁned by adding details of inter-island transportation.,Constraint,4
1295,"As just explained, the algorithm can detect plans that will and won’t work by checking the intersections of the opti- mistic and pessimistic reachable sets with the goal.",Constraint,3
1296,"(Cleaning the room could be implemented with the repeated application of another HLA to clean each row.) Since there are ﬁve actions in this domain, the cost for BREADTH-FIRST-SEARCH grows as 5d, where d is the length of the shortest solution (roughly twice the total number of squares); the algorithm cannot manage even two 2 × 2 rooms.",Constraint,1
1297,"In this way, angelic search can ﬁnd provably optimal abstract plans without considering their implementations.",Constraint,4
1298,"(The variable c is universally quantiﬁed, just like all the other variables in an action schema.) Action(RemoveLid (can), PRECOND:Can(can) EFFECT:Open(can)) Action(Paint(x , can), PRECOND:Object(x) ∧ Can(can) ∧ Color (can, c) ∧ Open(can) EFFECT:Color (x , c)) To solve a partially observable problem, the agent will have to reason about the percepts it will obtain when it is executing the plan.",Interface,4
1299,"The percept will be supplied by the agent’s sensors when it is actually acting, but when it is planning it will need a model of its sensors.",Safety,3
1300,"For planning, we augment PDDL with a new type of schema, the percept schema: PERCEPT SCHEMA Percept (Color (x, c), PRECOND:Object(x) ∧ InView(x) Percept (Color (can, c), PRECOND:Can(can) ∧ InView (can) ∧ Open(can) The ﬁrst schema says that whenever an object is in view, the agent will perceive the color of the object (that is, for the object x, the agent will learn the truth value of Color (x, c) for all c).",Constraint,4
1301,"Because there are no exogenous events in this world, the color of an object will remain the same, even if it is not being perceived, until the agent performs an action to change the object’s color.",Interface,4
1302,"Of course, the agent will need an action that causes objects (one at a time) to come into view: Action(LookAt(x), PRECOND:InView(y) ∧ (x 6= y) EFFECT:InView(x) ∧ ¬InView(y)) For a fully observable environment, we would have a Percept axiom with no preconditions for each ﬂuent.",Constraint,4
1303,"Car manufacturers sell spare tires and air bags, which are physical embodiments of contingent plan branches designed to handle punctures or crashes.",Constraint,4
1304,"Thus, a car driver contemplating a trip across the Sahara desert should make explicit contingency plans for breakdowns, whereas a trip to the supermarket requires less advance planning.",Safety,4
1305,"To construct the new belief state b′, we must consider what happens to each literal ℓ in each physical state s in b when action a is applied.",Safety,4
1306,"If the action adds ℓ, then ℓ will be true in b′ regardless of its initial value.",Safety,4
1307,"If the action deletes ℓ, then ℓ will be false in b′ regardless of its initial value.",Safety,4
1308,"If the action does not affect ℓ, then ℓ will retain its initial value (which is unknown) and will not appear in b′.",Safety,4
1309,"We cannot quite use the set semantics because (1) we must make sure that b′ does not con- tain both ℓ and ¬ℓ, and (2) atoms may contain unbound variables.",Safety,4
1310,"That is, if the belief state starts as a conjunction of literals, then any update will yield a conjunction of literals.",Safety,4
1311,It says we can compactly represent all the subsets of those 2n states that we will ever need.,Safety,4
1312,It is this property that enables the preservation of the 1-CNF belief-state representation.,Safety,4
1313,"For such actions, our action schemas will need something new: a conditional effect.",Constraint,4
1314,"It seems inevitable, then, that nontrivial problems will involve wiggly belief states, just like those encountered when we considered the problem of state estimation for the wumpus world (see Figure 7.21 on page 271).",Safety,4
1315,"Planning and Acting in the Real World a clean square, then [Suck ] is a solution but a sensorless agent that insists on 1-CNF belief states will not ﬁnd it.",Safety,3
1316,"As a belief-state representation, it suffers from one drawback, how- ever: determining whether the goal is satisﬁed, or an action is applicable, may require a lot of computation.",Interface,4
1317,"The computation can be implemented as an entailment test: if Am represents the collec- tion of successor-state axioms required to deﬁne occurrences of the actions a1, .",Interface,4
1318,"For example, if none of the actions in the sequence has a particular goal ﬂuent in its add list, the solver will detect this immediately.",Safety,4
1319,"For the partially observable painting problem with the percept axioms given earlier, one possible contingent solution is as follows: [LookAt (Table), LookAt (Chair ), if Color (Table, c) ∧ Color (Chair , c) then NoOp else [RemoveLid (Can 1), LookAt (Can 1), RemoveLid (Can 2), LookAt (Can 2), if Color (Table, c) ∧ Color (can, c) then Paint(Chair , can) else if Color (Chair , c) ∧ Color (can, c) then Paint(Table, can) else [Paint(Chair , Can 1), Paint (Table, Can 1)]]] Variables in this plan should be considered existentially quantiﬁed; the second line says that if there exists some color c that is the color of the table and the chair, then the agent need not do anything to achieve the goal.",Constraint,4
1320,"(It is up to the contingent-planning algorithm to make sure that the agent will never end up in a be- lief state where the condition formula’s truth value is unknown.) Note that with ﬁrst-order conditions, the formula may be satisﬁed in more than one way; for example, the condition Color (Table, c) ∧ Color (can, c) might be satisﬁed by {can/Can 1} and by {can/Can 2} if both cans are the same color as the table.",Constraint,3
1321,"2 If cyclic solutions are required for a nondeterministic problem, AND–OR search must be generalized to a loopy version such as LAO∗ (Hansen and Zilberstein, 2001).",Constraint,4
1322,"The model for an action may have a missing precondition—for example, the agent may not know that removing the lid of a paint can often requires a screwdriver; the model may have a missing effect—for example, painting an object may get paint on the ﬂoor as well; or the model may have a missing state variable—for example, the model given earlier has no notion of the amount of paint in a can, of how its actions affect this amount, or of the need for the amount to be nonzero.",Interface,4
1323,"Exogenous events can also include changes in the goal, such as the addition of the requirement that the table and chair not be painted black.",Safety,4
1324,"PLAN MONITORING • Plan monitoring: before executing an action, the agent veriﬁes that the remaining plan will still succeed.",Interface,4
1325,It then needs to repair the plan by ﬁnding some point P on the original plan that it can get back to.,Safety,4
1326,But an online execution monitoring agent needs to check the preconditions of the remaining empty plan— that the table and chair are the same color.,Interface,4
1327,The agent then needs to ﬁgure out a position in whole plan to aim for and a repair action sequence to get there.,Safety,4
1328,This behavior will loop until the chair is perceived to be completely painted.,Safety,4
1329,Multiagent Planning 425 more complex to enable plan monitoring.,Constraint,4
1330,"If we mean, “Can we guarantee that the agent will always achieve the goal?” then the answer is no, because the agent could inadvertently arrive at a dead end from which there is no repair.",Safety,4
1331,"If we rule out dead ends—assume that there exists a plan to reach the goal from any state in the environment—and assume that the environment is really nondeterministic, in the sense that such a plan always has some chance of success on any given execution attempt, then the agent will eventually reach the goal.",Safety,4
1332,Every prediction failure is an opportunity for learning; an agent should be able to modify its model of the world to accord with its percepts.,Interface,4
1333,"From then on, the replanner will be able to come up with a repair that gets at the root problem, rather than relying on luck to choose a good repair.",Constraint,4
1334,"An agent with multiple effectors that can operate concurrently—for example, a human who can type and speak at the same time—needs to do multieffector planning to manage each effector while handling positive and negative interactions among the effectors.",Constraint,4
1335,"For example, multiple reconnaissance robots covering a wide area may often be out of radio contact with each other and should share their ﬁndings during times when communication is feasible.",Constraint,4
1336,"In a multibody robotic doubles team, a single plan dictates which body will go where on the court and which body will hit the ball.",Safety,4
1337,"Spectators could be viewed as agents if their support or disdain is a signiﬁcant factor and can be inﬂuenced by the players’ conduct; otherwise, they can be treated as an aspect of nature—just like the weather—that is assumed to be indifferent to the players’ intentions.6 Finally, some systems are a mixture of centralized and multiagent planning.",Interface,1
1338,"Also, the goals of the company and its employees are brought into alignment, to some extent, by the payment of incentives (salaries and bonuses)—a sure sign that this is a true multiagent system.",Safety,1
1339,"11.4.1 Planning with multiple simultaneous actions For the time being, we will treat the multieffector, multibody, and multiagent settings in the same way, labeling them generically as multiactor settings, using the generic term actor to cover effectors, bodies, and agents.",Constraint,4
1340,Note that each action must include the actor as an argument.,Interface,4
1341,"SYNCHRONIZATION JOINT ACTION LOOSELY COUPLED will know what plans would work if they did agree to execute them.) For simplicity, we assume perfect synchronization: each action takes the same amount of time and actions at each point in the joint plan are simultaneous.",Safety,4
1342,"In the single-agent setting, there might be b different choices for the action; b can be quite large, especially for ﬁrst-order representations with many objects to act on, but action schemas provide a concise representation nonetheless.",Constraint,4
1343,"Having put the actors together into a multiactor system with a huge branching factor, the principal focus of research on multiactor planning has been to decouple the actors to the extent possible, so that the complexity of the problem grows linearly with n rather than exponentially.",Constraint,1
1344,"In the real world, this won’t work, but the action schema for Hit says that the ball will be returned successfully.",Safety,4
1345,We solve this by augmenting action schemas with one new feature: a concurrent action list stating which actions must or must not be executed concurrently.,Constraint,4
1346,"Multiagent Planning 429 If both agents can agree on either plan 1 or plan 2, the goal will be achieved.",Safety,4
1347,"But if A chooses plan 2 and B chooses plan 1, then nobody will return the ball.",Safety,4
1348,"Conversely, if A chooses 1 and B chooses 2, then they will both try to hit the ball.",Safety,4
1349,"Similar considerations apply to the development of human language, where the important thing is not which language each individual should speak, but the fact that a community all speaks the same language.",Safety,4
1350,"The ants appear to have a convention on the importance of roles—foraging is the most important—and ants will easily switch into the more important roles, but not into the less important.",Safety,4
1351,Bibliographical and Historical Notes 431 • Hierarchical task network (HTN) planning allows the agent to take advice from the domain designer in the form of high-level actions (HLAs) that can be implemented in various ways by lower-level action sequences.,Safety,4
1352,"The effects of HLAs can be deﬁned with angelic semantics, allowing provably correct high-level plans to be derived without consideration of lower-level implementations.",Constraint,4
1353,HTN methods can create the very large plans required by many real-world applications.,Safety,1
1354,"Joint plans can be constructed, but must be augmented with some form of coordination if two agents are to agree on which joint plan to execute.",Constraint,4
1355,(1990) in the FORBIN system.,Safety,1
1356,"Planning and Acting in the Real World A number of hybrid planning-and-scheduling systems have been deployed: ISIS (Fox et al., 1982; Fox, 1990) has been used for job shop scheduling at Westinghouse, GARI (De- scotte and Latombe, 1985) planned the machining and construction of mechanical parts, FORBIN was used for factory control, and N ONLIN+ was used for naval logistics planning.",Safety,1
1357,"SPIKE (Johnston and Adorf, 1992) was used for observation planning at NASA for the Hubble Space Telescope, while the Space Shuttle Ground Processing Scheduling System (Deale et al., 1994) does job-shop scheduling of up to 16,000 worker-shifts.",Safety,1
1358,"Hierarchy was also used in the LAWALY system (Siklossy and Dreussi, 1973).",Safety,1
1359,"The ABSTRIPS system (Sacerdoti, 1974) introduced the idea of an ab- straction hierarchy, whereby planning at higher levels was permitted to ignore lower-level preconditions of actions in order to derive the general structure of a working plan.",Interface,1
1360,"The technique of explanation-based learning, described in depth in Chapter 19, has been applied in several systems as a means of generalizing previously computed plans, including SOAR (Laird et al., 1986) and PRODIGY (Carbonell et al., 1989).",Safety,1
1361,Kamb- hampati (1994) argues that case-based planning should be analyzed as a form of reﬁnement planning and provides a formal foundation for case-based partial-order planning.,Constraint,4
1362,"Ma- son (1993) argued that sensing often can and should be dispensed with in robotic planning, and described a sensorless plan that can move a tool into a speciﬁc position on a table by a sequence of tilting actions, regardless of the initial position.",Constraint,3
1363,"SIPE (System for Interactive Planning and Execution monitoring) (Wilkins, 1988, 1990) was the ﬁrst planner to deal systematically with the problem of replanning.",Constraint,1
1364,"In the mid-1980s, pessimism about the slow run times of planning systems led to the proposal of reﬂex agents called reactive planning systems (Brooks, 1986; Agre and Chap- man, 1987).",Safety,1
1365,A universal plan (or a policy) contains a mapping from any state to the action that should be taken in that state.,Safety,4
1366,"Brafman and Domshlak (2008) devise a mul- tiactor planning algorithm whose complexity grows only linearly with the number of actors, provided that the degree of coupling (measured partly by the tree width of the graph of inter- actions among agents) is bounded.",Constraint,3
1367,"(1991) describe a system for playing Diplomacy, a board game requiring negoti- ation, coalition formation, and dishonesty.",Safety,1
1368,"In a later article, Stone (2003) analyzes two competitive multiagent environments—RoboCup, a robotic soccer competition, and TAC, the auction-based Trading Agents Competition— and ﬁnds that the computational intractability of our current theoretically well-founded ap- proaches has led to many multiagent systems being designed by ad hoc methods.",Safety,1
1369,"Recent book on multiagent systems include those by Weiss (2000a), Young (2004), Vlassis (2008), and Shoham and Leyton-Brown (2009).",Constraint,1
1370,There is an annual conference on autonomous agents and multiagent systems (AAMAS).,Interface,1
1371,11.2 Suppose that a high-level action has exactly one implementation as a sequence of primitive actions.,Constraint,4
1372,"Now suppose we wanted to combine scheduling with nondeterministic planning, which requires nondeterministic and conditional effects.",Constraint,4
1373,"Consider each of the three ﬁelds and explain if they should remain separate ﬁelds, or if they should become effects of the action.",Safety,4
1374,Remember that the original value will be overwritten!,Safety,4
1375,"The previous chapters described the technology for knowledge-based agents: the syntax, semantics, and proof theory of propositional and ﬁrst-order logic, and the implementation of agents that use these logics.",Safety,4
1376,"We then return to consider the technology for reasoning with this content: Section 12.5 discusses reasoning systems designed for efﬁcient inference with categories, and Section 12.6 discusses reasoning with default information.",Constraint,1
1377,"12.1 ONTOLOGICAL ENGINEERING ONTOLOGICAL ENGINEERING In “toy” domains, the choice of representation is not that important; many choices will work.",Safety,4
1378,Complex domains such as shopping on the Internet or driving a car in trafﬁc require more general and ﬂexible representations.,Safety,4
1379,"Of course, we won’t actually write a complete description of everything—that would be far too much for even a 1000-page textbook—but we will leave placeholders where new knowledge for any domain can ﬁt in.",Constraint,4
1380,"For example, we will deﬁne what it means to be a physical object, and the details of different types of objects—robots, televisions, books, or whatever—can be ﬁlled in later.",Safety,4
1381,"This is analogous to the way that designers of an object-oriented programming framework (such as the Java Swing graphical framework) deﬁne general concepts like Window, expecting users to 437 438 Chapter 12.",Interface,1
1382,We will see in Section 12.3.3 why physical objects come under generalized events.,Constraint,4
1383,"Before considering the ontology further, we should state one important caveat.",Safety,4
1384,"This would allow us to simulate the timing properties of the circuit, and indeed such simulations are often carried out by circuit designers.",Safety,4
1385,Two major characteristics of general-purpose ontologies distinguish them from collections of special-purpose ontologies: • A general-purpose ontology should be applicable in more or less any special-purpose domain (with the addition of domain-speciﬁc axioms).,Constraint,4
1386,"• In any sufﬁciently demanding domain, different areas of knowledge must be uniﬁed, because reasoning and problem solving could involve several areas simultaneously.",Constraint,4
1387,"A robot circuit-repair system, for instance, needs to reason about circuits in terms of elec- trical connectivity and physical layout, and about time, both for circuit timing analysis and estimating labor costs.",Safety,1
1388,The sentences describing time therefore must be capable of being combined with those describing spatial layout and must work equally well for nanoseconds and minutes and for angstroms and meters.,Constraint,4
1389,We should say up front that the enterprise of general ontological engineering has so far had only limited success.,Safety,4
1390,"The CYC system was mostly built this way (Lenat and Guha, 1990).",Safety,1
1391,"The OPENMIND system was built by volunteers who proposed facts in English (Singh et al., 2002; Chklovski and Gil, 2005).",Safety,1
1392,"We could then say Member (b, Basketballs ), which we will abbre- viate as b ∈ Basketballs, to say that b is a member of the category of basketballs.",Safety,4
1393,"We will use subcategory, subclass, and subset interchangeably.",Constraint,4
1394,"The largest such taxonomy organizes about 10 million living and extinct species, many of them beetles,2 into a single hi- erarchy; library science has developed a taxonomy of all ﬁelds of knowledge, encoded as the Dewey Decimal system; and tax authorities and other government departments have devel- oped extensive taxonomies of occupations and commercial products.",Safety,1
1395,"Dogs ∈ DomesticatedSpecies Notice that because Dogs is a category and is a member of DomesticatedSpecies , the latter must be a category of categories.",Safety,4
1396,"And even if we know that males and females are disjoint, we will not know that an animal that is not a male must be a female, unless we say that males and females constitute an exhaustive decomposition of the animals.",Interface,4
1397,"The notation for “exactly two” is a little awkward; we are forced to say that there are two legs, that they are not the same, and that if anyone proposes a third leg, it must be the same as one of the other two.",Safety,4
1398,"Instead, we need a new concept, which we will call a bunch.",Constraint,4
1399,"In other words, BunchOf (s) must be part of any object that has all the elements of s as parts: ∀ y [∀ x x ∈ s ⇒ PartOf (x, y)] ⇒ PartOf (BunchOf (s), y) .",Safety,4
1400,"So in addition to the category Tomatoes , we will also have the category Typical (Tomatoes ).",Safety,4
1401,Most knowledge about natural kinds will actually be about their typical instances: x ∈ Typical (Tomatoes ) ⇒ Red (x) ∧ Round (x) .,Constraint,4
1402,"(One does, however, have to discover who wrote which exer- cises.) These sorts of monotonic relationships among measures form the basis for the ﬁeld of qualitative physics, a subﬁeld of AI that investigates how to reason about physical systems without plunging into detailed equations and numerical simulations.",Safety,1
1403,"Informally, its elements will be all those things of which one might say “It’s butter,” including Butter 3.",Constraint,4
1404,"Situation calculus is limited in its applicability: it was designed to describe a world in which actions are discrete, instantaneous, and happen one at a time.",Constraint,4
1405,We will consider two kinds of time intervals: moments and extended intervals.,Constraint,4
1406,"For example, suppose Alice asks “what is the square root of 1764” and Bob replies “I don’t know.” If Alice insists “think harder,” Bob should realize that with some more thought, this question can in fact be answered.",Constraint,4
1407,"On the other hand, if the question were “Is your mother sitting down right now?” then Bob should realize that thinking harder is unlikely to help.",Safety,4
1408,"Knowledge about the knowledge of other agents is also important; Bob should realize that his mother knows whether she is sitting or not, and that asking her would be a way to ﬁnd out.",Safety,4
1409,We do not have to be able to predict how many milliseconds it will take for a particular agent to make a deduction.,Constraint,4
1410,We will be happy just to be able to conclude that mother knows whether or not she is sitting.,Constraint,4
1411,"A more serious problem is that, if it is true that Superman is Clark Kent, then we must conclude that Lois knows that Clark can ﬂy: (Superman = Clark ) ∧ Knows(Lois, CanFly (Superman)) |= Knows(Lois, CanFly (Clark )) .",Constraint,4
1412,Modal logic is designed to address this problem.,Constraint,4
1413,"Therefore, we will need a more com- plicated model, one that consists of a collection of possible worlds rather than just one true world.",Constraint,4
1414,"Reasoning Systems for Categories 453 that there is a particular someone who Bond knows is a spy; we can write this as ∃ x KBond Spy(x) , which in modal logic means that there is an x that, in all accessible worlds, Bond knows to be a spy.",Interface,1
1415,"On the other hand, (KAP ) ∨ (KA¬P ) is not a tautology; in general, there will be lots of propositions that an agent does not know to be true and does not know to be false.",Interface,4
1416,"That means that if you know something, it must be true, and we have the axiom: KaP ⇒ P .",Safety,4
1417,"Furthermore, logical agents should be able to introspect on their own knowledge.",Constraint,4
1418,LOGICAL OMNISCIENCE 12.5 REASONING SYSTEMS FOR CATEGORIES Categories are the primary building blocks of large-scale knowledge representation schemes.,Safety,1
1419,This section describes systems specially designed for organizing and reasoning with cate- gories.,Constraint,1
1420,There are two closely related families of systems: semantic networks provide graph- ical aids for visualizing a knowledge base and efﬁcient algorithms for inferring properties 454 Chapter 12.,Interface,1
1421,Knowledge Representation EXISTENTIAL GRAPHS of an object on the basis of its category membership; and description logics provide a for- mal language for constructing and combining category deﬁnitions and efﬁcient algorithms for deciding subset and superset relationships between categories.,Interface,3
1422,"The notation that semantic networks provide for certain kinds of sentences is often more convenient, but if we strip away the “human interface” issues, the underlying concepts—objects, relations, quantiﬁcation, and so on—are the same.",Interface,2
1423,"There are many variants of semantic networks, but all are capable of representing in- dividual objects, categories of objects, and relations among objects.",Constraint,4
1424,The simplicity and efﬁciency of this inference 5 Several early systems failed to distinguish between properties of members of a category and properties of the category as a whole.,Safety,1
1425,Reasoning Systems for Categories 455 Mammals SubsetOf Persons Legs 2 SubsetOf SubsetOf Male Persons HasMother Female Persons MemberOf MemberOf Mary SisterOf John Legs 1 Figure 12.5 gories.,Constraint,1
1426,"Designers can build a large network and still have a good idea about what queries will be efﬁcient, because (a) it is easy to visualize the steps that the inference procedure will go through and (b) in some cases the query language is so simple that difﬁcult queries cannot be posed.",Safety,4
1427,"In cases where the expressive power proves to be too limiting, many semantic network systems provide for procedural attachment to ﬁll in the gaps.",Interface,1
1428,Procedural attachment is a technique whereby a query about (or sometimes an assertion of) a certain relation results in a call to a special procedure designed for that relation rather than a general inference algorithm.,Interface,3
1429,"For a ﬁxed network, this is semantically adequate but will be much less concise than the network notation itself if there are lots of exceptions.",Safety,4
1430,"For a network that will be updated with more assertions, however, such an approach fails—we really want to say that any persons as yet unknown with one leg are exceptions too.",Constraint,4
1431,DEFAULT VALUE OVERRIDING 12.5.2 Description logics DESCRIPTION LOGIC SUBSUMPTION CLASSIFICATION The syntax of ﬁrst-order logic is designed to make it easy to say things about objects.,Safety,4
1432,De- scription logics are notations that are designed to make it easier to describe deﬁnitions and properties of categories.,Constraint,4
1433,Description logic systems evolved from semantic networks in re- sponse to pressure to formalize what the networks mean while retaining the emphasis on taxonomic structure as an organizing principle.,Interface,1
1434,Some systems also include consistency of a cate- gory deﬁnition—whether the membership criteria are logically satisﬁable.,Safety,1
1435,"Reasoning Systems for Categories 457 Concept → Thing | ConceptName | And(Concept , .",Constraint,1
1436,"In standard ﬁrst-order logic systems, predicting the solution time is often impossible.",Safety,1
1437,It is frequently left to the user to engineer the represen- tation to detour around sets of sentences that seem to be causing the system to take several weeks to solve a problem.,Safety,1
1438,"The thrust in description logics, on the other hand, is to ensure that subsumption-testing can be solved in time polynomial in the size of the descriptions.7 6 Notice that the language does not allow one to simply state that one concept, or category, is a subset of another.",Safety,4
1439,This is a deliberate policy: subsumption between categories must be derivable from some aspects of the descriptions of the categories.,Safety,4
1440,"7 CLASSIC provides efﬁcient subsumption testing in practice, but the worst-case run time is exponential.",Safety,4
1441,"Knowledge Representation This sounds wonderful in principle, until one realizes that it can only have one of two consequences: either hard problems cannot be stated at all, or they require exponentially large descriptions!",Constraint,4
1442,Each forces ﬁrst- order logical systems to go through a potentially exponential case analysis in order to ensure completeness.,Safety,1
1443,We saw that the inheritance mechanism in semantic networks implements the overriding of defaults in a simple and natural way.,Safety,4
1444,"It seems that humans often “jump to conclusions.” For example, when one sees a car parked on the street, one is normally willing to believe that it has four wheels even though only three are visible.",Safety,4
1445,"Now, probability theory can certainly provide a conclusion that the fourth wheel exists with high probability, yet, for most people, the possi- bility of the car’s not having four wheels does not arise unless some new evidence presents itself.",Constraint,4
1446,We will look at two such logics that have been studied extensively: circumscription and default logic.,Constraint,4
1447,8 Recall that monotonicity requires all entailed sentences to remain entailed after new sentences are added to the KB.,Safety,4
1448,"In such logics, a sentence is entailed (with default status) if it is true in all preferred models of the KB, as opposed to the requirement of truth in all models in classical logic.",Constraint,4
1449,Knowledge Representation EXTENSION BELIEF REVISION appears in Ji or C must also appear in P .,Constraint,4
1450,"Decisions often involve tradeoffs, and one therefore needs to compare the strengths of belief in the outcomes of different actions, and the costs of making a wrong decision.",Safety,4
1451,"12.6.2 Truth maintenance systems We have seen that many of the inferences drawn by a knowledge representation system will have only default status, rather than being absolutely certain.",Safety,1
1452,"Inevitably, some of these in- ferred facts will turn out to be wrong and will have to be retracted in the face of new informa- tion.",Safety,4
1453,"To avoid cre- ating a contradiction, we must ﬁrst execute RETRACT(KB, P ).",Constraint,4
1454,"Reasoning with Default Information 461 TRUTH MAINTENANCE SYSTEM JTMS JUSTIFICATION Problems arise, however, if any additional sentences were inferred from P and asserted in the KB.",Constraint,1
1455,"Truth maintenance systems, or TMSs, are designed to handle exactly these kinds of complications.",Constraint,1
1456,"When the call RETRACT(KB, Pi) is made, the system reverts to the state just before Pi was added, thereby removing both Pi and any inferences that were derived from Pi.",Safety,1
1457,"This is simple, and it guarantees that the knowledge base will be consistent, but retracting Pi requires retracting and reasserting n − i sentences as well as undoing and redoing all the inferences drawn from those sentences.",Safety,4
1458,For systems to which many facts are being added—such as large commercial databases—this is impractical.,Constraint,1
1459,"A more efﬁcient approach is the justiﬁcation-based truth maintenance system, or JTMS.",Safety,1
1460,"For example, if the knowledge base already contains P ⇒ Q, then TELL(P ) will cause Q to be added with the justiﬁcation {P, P ⇒ Q}.",Constraint,4
1461,"Given the call RETRACT(P ), the JTMS will delete exactly those sentences for which P is a member of every justiﬁcation.",Safety,4
1462,"In this way, the time required for retraction of P depends only on the number of sentences derived from P rather than on the number of other sentences added since P entered the knowledge base.",Safety,4
1463,"The JTMS assumes that sentences that are considered once will probably be considered again, so rather than deleting a sentence from the knowledge base entirely when it loses If a all justiﬁcations, we merely mark the sentence as being out of the knowledge base.",Safety,4
1464,"A great deal of reasoning must then be done to work out the logistical consequences and If we want to consider Site(Athletics, Sibiu) in- hence the desirability of this selection.",Safety,4
1465,"Inference chains generated from the choice of Bucharest can be reused with Sibiu, provided that the conclusions are the same.",Constraint,4
1466,"Knowledge Representation An assumption-based truth maintenance system, or ATMS, makes this type of context- switching between hypothetical worlds particularly efﬁcient.",Interface,1
1467,Truth maintenance systems also provide a mechanism for generating explanations.,Interface,1
1468,"If the sentences in E are already known to be true, then E simply provides a sufﬁcient ba- sis for proving that P must be the case.",Safety,4
1469,"In most cases, we will prefer an explanation E that is minimal, meaning that there is no proper subset of E that is also an explanation.",Interface,4
1470,"The exact algorithms used to implement truth maintenance systems are a little compli- cated, and we do not cover them here.",Safety,1
1471,"Therefore, you should not expect truth maintenance to be a panacea.",Constraint,4
1472,"When used carefully, however, a TMS can provide a substantial increase in the ability of a logical system to handle complex environments and hypotheses.",Interface,1
1473,"buyer’s product description will be precise, as in Canon Rebel XTi digital camera, and the task is then to ﬁnd the store(s) with the best offer.",Constraint,4
1474,"In other cases the description will be only partially speciﬁed, as in digital camera for under $300, and the agent will have to compare different products.",Safety,4
1475,"Web user would see pages displayed as an array of pixels on a screen, the shopping agent will perceive a page as a character string consisting of ordinary words interspersed with for- matting commands in the HTML markup language.",Constraint,4
1476,"If the query is “laptops,” then a Web page with a review of the latest high-end laptop would be relevant, but if it doesn’t provide a way to buy, it isn’t an offer.",Interface,4
1477,"12.7.1 Following links The strategy is to start at the home page of an online store and consider all pages that can be reached by following relevant links.11 The agent will have knowledge of a number of stores, for example: Amazon ∈ OnlineStores ∧ Homepage(Amazon, “amazon.com”) .",Interface,4
1478,"These stores classify their goods into product categories, and provide links to the major cat- egories from their home page.",Interface,4
1479,"Minor categories can be reached through a chain of relevant links, and eventually we will reach offers.",Constraint,4
1480,Now we must deﬁne what it means for text to be a RelevantCategoryName for query.,Constraint,4
1481,"Suppose that query is “laptops.” Then RelevantCategoryName (query, text ) is true when one of the following holds: • The text and query name the same category—e.g., “notebooks” and “laptops.” 11 An alternative to the link-following strategy is to use an Internet search engine; the technology behind Internet search, information retrieval, will be covered in Section 22.3.",Interface,4
1482,"It will not be feasible to list all possible shopping categories, because a buyer could always come up with some new desire and manufacturers will always come out with new products to satisfy them (electric kneecap warmers?).",Constraint,4
1483,"Nonetheless, an ontology of about a thousand categories will serve as a very useful tool for most buyers.",Interface,4
1484,"For example, if we add the sentence Name(“CDs”, CertiﬁcatesOfDeposit ) to the knowledge base in Figure 12.9(b), then “CDs” will name two different categories.",Safety,4
1485,"Knowledge Representation PROCEDURAL ATTACHMENT WRAPPER I can ﬁt on the tray table of an economy-class airline seat.” It is impossible to enumerate in advance all the ways a category can be named, so the agent will have to be able to do addi- tional reasoning in some cases to determine if the Name relation holds.",Interface,4
1486,"In the worst case, this requires full natural language understanding, a topic that we will defer to Chapter 22.",Safety,4
1487,"To compare those offers, the agent must extract the rele- vant information—price, speed, disk size, weight, and so on—from the offer pages.",Safety,4
1488,"The general topic of preferences among multiple attributes is addressed in Section 16.4; for now, our shopping agent will simply return a list of all undominated offers that meet the buyer’s description.",Performance,4
1489,"Some attributes, such as screen size on a notebook, depend on the user’s particular preference (portability versus visibility); for these, the shopping agent will just have to ask the user.",Safety,4
1490,The major points are as follows: • Large-scale knowledge representation requires a general-purpose ontology to organize and tie together the various speciﬁc domains of knowledge.,Safety,4
1491,"• A general-purpose ontology needs to cover a wide variety of knowledge and should be capable, in principle, of handling any domain.",Constraint,4
1492,Such representations enable an agent to construct plans by logical inference.,Interface,4
1493,"• Special-purpose representation systems, such as semantic networks and description logics, have been devised to help in organizing a hierarchy of categories.",Constraint,1
1494,"• The closed-world assumption, as implemented in logic programs, provides a simple way to avoid having to specify lots of negative information.",Safety,4
1495,• Truth maintenance systems handle knowledge updates and revisions efﬁciently.,Constraint,1
1496,"Early discussions of representation in AI tended to focus on “problem representation” rather than “knowledge representation.” (See, for example, Amarel’s (1968) discussion of the Missionaries and Cannibals problem.) In the 1970s, AI emphasized the development of “ex- pert systems” (also called “knowledge-based systems”) that could, if given the appropriate domain knowledge, match or exceed the performance of human experts on narrowly deﬁned tasks.",Performance,1
1497,"For example, the ﬁrst expert system, D ENDRAL (Feigenbaum et al., 1971; Lindsay et al., 1980), interpreted the output of a mass spectrometer (a type of instrument used to ana- lyze the structure of organic chemical compounds) as accurately as expert chemists.",Safety,1
1498,"Over time, researchers became interested in standardized knowledge representation formalisms and ontologies that could streamline the process of creating new expert systems.",Safety,1
1499,"Our present system of biological classiﬁcation, including the use of “binomial nomenclature” (classiﬁcation via genus and species in the technical sense), was invented by the Swedish biologist Carolus Linnaeus, or Carl von Linne (1707–1778).",Safety,1
1500,"RDF (Brickley and Guha, 2004) allows for assertions to be made in the form of relational triples, and provides some means for evolving the meaning of names over time.",Safety,4
1501,The conferences on Formal Ontology in Information Systems (FOIS) contain many interesting papers on both general and domain-speciﬁc ontologies.,Safety,1
1502,Allen (1991) systematically investigates the wide variety of techniques available for time representation; van Beek and Manchak (1996) analyze algorithms for tem- poral reasoning.,Safety,1
1503,Harry Bunt (1985) has provided an extensive analysis of its use in knowl- edge representation.,Interface,4
1504,"The book Reasoning about Knowledge (Fagin et al., 1995) provides a thorough introduction.",Safety,4
1505,Ernie Davis (1990) provides an excellent comparison of the syntactic and modal theories of knowledge.,Interface,4
1506,"Patrick Hayes’s (1979) “The Logic of Frames” cut even deeper, claiming that “Most of ‘frames’ is just a new syntax for parts of ﬁrst-order logic.” Drew McDermott’s (1978b) “Tarskian Semantics, or, No Notation without Denota- tion!” argued that the model-theoretic approach to semantics used in ﬁrst-order logic should be applied to all knowledge representation formalisms.",Safety,4
1507,"Building on the KL-ONE system (Schmolze and Lipkis, 1983), several researchers developed systems that incorporate theoretical complex- ity analysis, most notably KRYPTON (Brachman et al., 1983) and Classic (Borgida et al., 1989).",Safety,1
1508,"Knowledge Representation scription; the underlying theory of stable model semantics was introduced by Gelfond and Lifschitz (1988), and the leading answer set programming systems are DLV (Eiter et al., 1998) and SMODELS (Niemel¨a et al., 2000).",Safety,1
1509,Recent years have seen renewed interest in applica- tions of nonmonotonic logics to large-scale knowledge representation systems.,Constraint,1
1510,"The BENINQ systems for handling insurance-beneﬁt inquiries was perhaps the ﬁrst commercially success- ful application of a nonmonotonic inheritance system (Morgenstern, 1998).",Safety,1
1511,A variety of nonmonotonic reasoning systems based on logic programming are documented in the proceedings of the conferences on Logic Programming and Nonmonotonic Reasoning (LPNMR).,Safety,1
1512,"The study of truth maintenance systems began with the TMS (Doyle, 1979) and RUP (McAllester, 1980) systems, both of which were essentially JTMSs.",Constraint,1
1513,Nayak and Williams (1997) show how an efﬁcient incremental TMS called an ITMS makes it feasible to plan the operations of a NASA spacecraft in real time.,Interface,4
1514,"Fahlman discovered in the process of designing it that most of the effort (80%, by his estimate) went into modeling the physics of the blocks world to calculate the stability of various subassemblies of blocks, rather than into planning per se.",Safety,4
1515,Hayes was the ﬁrst to prove that a bath with the plug in will eventually overﬂow if the tap keeps running and that a person who falls into a lake will get wet all over.,Constraint,4
1516,"De Kleer and Brown (1985), Ken Forbus (1985), and Benjamin Kuipers (1985) inde- pendently and almost simultaneously developed systems that can reason about a physical system based on qualitative abstractions of the underlying equations.",Safety,1
1517,"Qualitative physics soon developed to the point where it became possible to analyze an impressive variety of complex physical systems (Yip, 1991).",Interface,1
1518,"Qualitative techniques have been used to construct novel designs for clocks, windshield wipers, and six-legged walkers (Subramanian and Wang, 1994).",Constraint,4
1519,"The collection Readings in Qualitative Reasoning about Physical Systems (Weld and QUALITATIVE PHYSICS 473 de Kleer, 1990) an encyclopedia article by Kuipers (2001), and a handbook article by Davis (2007) introduce to the ﬁeld.",Interface,1
1520,"(1997) supports a form of qualitative spatial reasoning and has led to new kinds of geographical information systems; see also (Davis, 2006).",Constraint,1
1521,"When AI researchers provide their artiﬁcial agents with psychological theories for reasoning about other agents, the theories are frequently based on the researchers’ description of the logical agents’ own design.",Constraint,4
1522,The proceedings of the international conferences on Principles of Knowledge Representation and Reasoning provide the most up-to-date sources for work in this area.,Interface,4
1523,"Davis (1990), Steﬁk (1995), and Sowa (1999) provide textbook in- troductions to knowledge representation, van Harmelen et al.",Interface,4
1524,"The biennial conference on Theoretical Aspects of Reasoning About Knowledge (TARK) covers applications of the theory of knowledge in AI, economics, and distributed systems.",Safety,1
1525,"The ontology should contain situations, actions, squares, players, marks (X, O, or blank), and the notion of winning, losing, or drawing a game.",Safety,4
1526,Knowledge Representation 12.2 You are to create a system for advising computer science undergraduates on what courses to take over an extended period in order to satisfy the program requirements.,Interface,1
1527,"(Use whatever requirements are appropriate for your institution.) First, decide on a vocabulary for representing all the information, and then represent it; then formulate a query to the system that will return a legal program of study as a solution.",Safety,1
1528,"You should allow for some tailoring to individual students, in that your system should ask what courses or equivalents the student has already taken, and not generate programs that repeat those courses.",Safety,1
1529,"Suggest ways in which your system could be improved—for example to take into ac- count knowledge about student preferences, the workload, good and bad instructors, and so on.",Safety,1
1530,12.3 Develop a representational system for reasoning about windows in a window-based computer interface.,Interface,1
1531,"In particular, your representation should be able to describe: • The state of a window: minimized, displayed, or nonexistent.",Safety,4
1532,"You may (and should) use symbols deﬁned in the text, but be sure to list these explicitly.",Safety,4
1533,"You should write sentences that have straightforward logical structure (e.g., statements that objects have Exercises 475 certain properties, that objects are related in certain ways, that all objects satisfying one prop- erty satisfy another).",Constraint,4
1534,"(You will need events and temporal ordering, among other things.) • Where would they ﬁt in a more general hierarchy?",Constraint,4
1535,• How detailed must you be about each of the various concepts?,Safety,4
1536,"To answer the questions below, your knowledge base must include background knowledge.",Safety,4
1537,"You’ll have to deal with what kind of things are at a supermarket, what is involved with purchasing the things one selects, what the purchases will be used for, and so on.",Constraint,4
1538,"If possible, use a logical reasoning system to demonstrate the sufﬁciency of your knowledge base.",Safety,1
1539,But you should be able to put together a consistent set of axioms for the limited questions posed here.,Safety,4
1540,"PartwiseDisjoint These should be analogous to the deﬁnitions for ExhaustiveDecomposition , Partition, and Disjoint.",Safety,4
1541,The robot begins in Arad and must reach Bucharest.,Safety,4
1542,Write a suitable logical query whose solutions provide possible paths to the goal.,Interface,4
1543,What should Carlos say?,Constraint,4
1544,What should Carlos say on his second turn?,Constraint,4
1545,What should Carlos say?,Constraint,4
1546,Prove that there will always be a winner to this game.,Constraint,4
1547,"For instance, if player A has much more time left than player B, then A will sometimes make a move that greatly complicates the situation, in the hopes of gaining an advantage because he has more time to work out the proper strategy.",Interface,4
1548,"Explain how forward chaining allows a logic-based system to solve the same problem efﬁciently, assuming that the KB contains only the 11,000 sentences about prices.",Safety,1
1549,Describe a situation in which neither forward nor backward chaining on the sentences will allow the price query for an individual car to be handled efﬁciently.,Interface,4
1550,Can you suggest a solution enabling this type of query to be solved efﬁciently in all cases in logic systems?,Constraint,1
1551,12.25 A complete solution to the problem of inexact matches to the buyer’s description in shopping is very difﬁcult and requires a full array of natural language processing and information retrieval techniques.,Safety,4
1552,"The buyer must use the following grammar for product descriptions: Description → Category [Connector Modiﬁer ]∗ Connector → “with” | “and” | “,” Modiﬁer Op → Attribute | Attribute Op Value → “=” | “>” | “<” Here, Category names a product category, Attribute is some feature such as “CPU” or “price,” and Value is the target value for the attribute.",Constraint,4
1553,So the query “computer with at least a 2.5 GHz CPU for under $500” must be re-expressed as “computer with CPU > 2.5 GHz and price < $500.” Implement a shopping agent that accepts descriptions in this language.,Constraint,4
1554,"Provide a formal logical description of buying, using event calculus.",Interface,4
1555,An agent may never know for certain what state it’s in or where it will end up after a sequence of actions.,Interface,4
1556,We have seen problem-solving agents (Chapter 4) and logical agents (Chapters 7 and 11) designed to handle uncertainty by keeping track of a belief state—a representation of the set of all possible world states that it might be in—and generating a contingency plan that han- dles every possible eventuality that its sensors may report during execution.,Safety,3
1557,"Despite its many virtues, however, this approach has signiﬁcant drawbacks when taken literally as a recipe for creating agent programs: • When interpreting partial sensor information, a logical agent must consider every log- ically possible explanation for the observations, no matter how unlikely.",Safety,3
1558,• A correct contingent plan that handles every eventuality can grow arbitrarily large and must consider arbitrarily unlikely contingencies.,Constraint,4
1559,• Sometimes there is no plan that is guaranteed to achieve the goal—yet the agent must act.,Safety,4
1560,It must have some way to compare the merits of plans that are not guaranteed.,Safety,4
1561,"Even though the airport is only about 5 miles away, a logical taxi agent will not be able to conclude with certainty that “Plan A90 will get us to the airport in time.” Instead, it reaches the weaker conclusion “Plan A90 will get us to the airport in time, as long as the car doesn’t break down or run out of gas, and I don’t get into an accident, and there are no accidents on the bridge, and the plane doesn’t leave early, and no meteorite hits the car, and .",Constraint,4
1562,"The agent’s knowledge cannot guarantee any of these outcomes for A90, but it can provide some degree of belief that they will be achieved.",Interface,4
1563,"Other plans, such as A180, might increase the agent’s belief that it will get to the airport on time, but also increase the likelihood of a long wait.",Safety,4
1564,"The right thing to do—the rational decision—therefore depends on both the relative importance of various goals and the likeli- hood that, and degree to which, they will be achieved.",Safety,4
1565,The only way to ﬁx the rule is to make it logically exhaustive: to augment the left-hand side with all the qualiﬁcations required for a cavity to cause a toothache.,Constraint,4
1566,Trying to use logic to cope with a domain like medical diagnosis thus fails for three main reasons: LAZINESS THEORETICAL IGNORANCE PRACTICAL IGNORANCE • Laziness: It is too much work to list the complete set of antecedents or consequents needed to ensure an exceptionless rule and too hard to use such rules.,Safety,4
1567,"This is typical of the medical domain, as well as most other judgmental domains: law, business, design, automobile repair, gardening, dating, and so on.",Safety,4
1568,Quantifying Uncertainty can at best provide only a degree of belief in the relevant sentences.,Interface,4
1569,"Probability provides a way of summarizing the uncertainty that comes from our lazi- ness and ignorance, thereby solving the qualiﬁcation problem.",Safety,4
1570,"That is, we expect that out of all the situations that are indistinguishable from the current situation as far as our knowledge goes, the patient will have a cavity in 80% of them.",Safety,4
1571,Shouldn’t it be either 0 or 1?,Constraint,4
1572,"To make such choices, an agent must ﬁrst have preferences between the different pos- sible outcomes of the various plans.",Interface,4
1573,"(The term utility is used here in the sense of “the quality of being useful,” not in the sense of the electric company or water works.) Utility theory says that every state has a degree of usefulness, or utility, to an agent and that the agent will prefer states with higher utility.",Constraint,4
1574,"The possible worlds are mutually exclusive and exhaustive—two possible worlds cannot both be the case, and one possible world must be the case.",Safety,4
1575,Note that probability theory does not require complete knowledge of the probabilities of each possible world.,Safety,4
1576,"When making decisions, an agent needs to condition on all the evidence it has observed.",Interface,4
1577,Sometimes we will want to talk about the probabilities of all the possible values of a random variable.,Safety,4
1578,"We could write: P (Weather = sunny) = 0.6 P (Weather = rain) = 0.1 P (Weather = cloudy) = 0.29 P (Weather = snow ) = 0.01 , but as an abbreviation we will allow P(Weather ) = h0.6, 0.1, 0.29, 0.01i , Section 13.2.",Interface,4
1579,"Saying that the probability density is uniform from 18C to 26C means that there is a 100% chance that the temperature will fall somewhere in that 8C-wide region and a 50% chance that it will fall in any 4C-wide region, and so on.",Safety,4
1580,"We will sometimes use P notation to derive results about individual P values, and when we say “P(sunny) = 0.6” it is really an abbreviation for “P(sunny) is the one-element vector h0.6i, which means that P (sunny) = 0.6.” Now we have deﬁned a syntax for propositions and probability assertions and we have given part of the semantics: Equation (13.2) deﬁnes the probability of a proposition as the sum of the probabilities of worlds in which it holds.",Interface,4
1581,It is easy to see that this deﬁnition satisﬁes the basic requirement that possible worlds be mutually exclusive and exhaustive (Exercise 13.5).,Safety,4
1582,"One argument for the axioms of probability, ﬁrst stated in 1931 by Bruno de Finetti (and translated into English in de Finetti (1993)), is as follows: If an agent has some degree of belief in a proposition a, then the agent should be able to state odds at which it is indifferent to a bet for or against a.3 Think of it as a game between two agents: Agent 1 states, “my degree of belief in event a is 0.4.” Agent 2 is then free to choose whether to wager for or against a at stakes that are consistent with the stated degree of belief.",Constraint,4
1583,"That is, Agent 2 could choose to accept Agent 1’s bet that a will occur, offering $6 against Agent 1’s $4.",Constraint,4
1584,"Or Agent 2 could accept Agent 1’s bet that ¬a will occur, offering $4 against Agent 1’s $6.",Constraint,4
1585,But de Finetti proved something much stronger: If Agent 1 expresses a set of degrees of belief that violate the axioms of probability theory then there is a combination of bets by Agent 2 that guarantees that Agent 1 will lose money every time.,Safety,4
1586,"They each construct a set of axioms for reasoning with degrees of beliefs: no contradictions, correspondence with ordinary logic (for example, if belief in A goes up, then belief in ¬A must go down), and so on.",Constraint,4
1587,"The only controversial axiom is that degrees of belief must be numbers, or at least act like numbers in that they must be transitive (if belief in A is greater than belief in B, which is greater than belief in C, then belief in A must be greater than C) and comparable (the belief in A must be one of equal to, greater than, or less than belief in B).",Safety,4
1588,The success of reasoning systems based on probability theory has been much more effective in making converts.,Safety,1
1589,"27) wrote, “every event in life is unique, and every real-life probability that we esti- mate in practice is that of an event that has never occurred before.” For example, given a particular patient, a frequentist who wants to estimate the probability of a cavity will consider a reference class of other patients who are similar in important ways—age, symptoms, diet—and see what proportion of them had a cavity.",Interface,4
1590,The principle of indifference attributed to Laplace (1816) states that propo- sitions that are syntactically “symmetric” with respect to the evidence should be accorded equal probability.,Constraint,4
1591,"Notice that the probabilities in the joint distribution sum to 1, as required by the axioms of probability.",Safety,4
1592,"NORMALIZATION The two values sum to 1.0, as they should.",Safety,4
1593,"It does not scale well, however: for a domain described by n Boolean variables, it requires an input table of size O(2n) and takes O(2n) time to process the 494 Chapter 13.",Interface,4
1594,The full joint distribution in tabular form is just not a practical tool for building reasoning systems.,Safety,1
1595,"Instead, it should be viewed as the theoretical foundation on which more effective approaches may be built, just as truth tables formed a theoretical foundation for more practical algorithms like DPLL.",Safety,3
1596,The remainder of this chapter introduces some of the basic ideas required in preparation for the development of realistic systems in Chapter 14.,Safety,1
1597,"Now, unless one is in the deity business, one should not imagine that one’s dental problems inﬂuence the weather.",Safety,4
1598,"In a more practical vein, the independence of dentistry and meteorology is a good thing, because otherwise the practice of dentistry might require intimate knowledge of meteorology, and vice versa.",Safety,4
1599,"Whenever a connection, however indirect, exists between two variables, independence will fail to hold.",Constraint,4
1600,This simple equation underlies most modern AI systems for probabilistic inference.,Constraint,1
1601,"We will also have occasion to use a more general version conditionalized on some background evidence e: P(Y | X, e) = P(X | Y, e)P(Y | e) P(X | e) .",Constraint,4
1602,"If there is a sudden epidemic of meningitis, the unconditional probability of meningitis, P (m), will go up.",Safety,4
1603,"The doctor who derived the diagnostic proba- bility P (m | s) directly from statistical observation of patients before the epidemic will have no idea how to update the value, but the doctor who computes P (m | s) from the other three values will see that P (m | s) should go up proportionately with P (m).",Constraint,4
1604,The use of this kind of direct causal or model-based knowledge provides the crucial robustness needed to make probabilistic systems feasible in the real world.,Safety,1
1605,"Quantifying Uncertainty CONDITIONAL INDEPENDENCE approximate methods for evidence combination that, while giving incorrect answers, require fewer numbers to give any answer at all.",Constraint,3
1606,"Rather than taking this route, we need to ﬁnd some additional assertions about the domain that will enable us to simplify the expressions.",Safety,4
1607,"The notion of independence in Sec- tion 13.4 provides a clue, but needs reﬁning.",Safety,4
1608,"Each is directly caused by the cavity, but neither has a direct effect on the other: toothache depends on the state of the nerves in the tooth, whereas the probe’s accuracy depends on the dentist’s skill, to which the toothache is irrelevant.5 Mathematically, this property is written as P(toothache ∧ catch | Cavity) = P(toothache | Cavity)P(catch | Cavity) .",Safety,4
1609,"(13.18) Now the information requirements are the same as for inference, using each piece of evi- dence separately: the prior probability P(Cavity) for the query variable and the conditional probability of each effect, given its cause.",Safety,4
1610,"The Wumpus World Revisited 499 independent numbers (23 = 8 entries in the table, but they must sum to 1, so 7 are indepen- dent).",Safety,4
1611,"That means that conditional independence assertions can allow probabilistic systems to scale up; moreover, they are much more com- monly available than absolute independence assertions.",Constraint,1
1612,"(The naive Bayes model is sometimes called a Bayesian classiﬁer, a somewhat careless usage that has prompted true Bayesians to call it the idiot Bayes model.) In practice, naive Bayes systems can work surprisingly well, even when the conditional independence assumption is not true.",Safety,1
1613,We will see that a probabilistic agent can do much better than the logical agent.,Safety,4
1614,This decomposition makes it easy to see what the joint probability values should be.,Safety,4
1615,"The wumpus agent should deﬁnitely avoid [2,2]!",Safety,4
1616,"To get efﬁcient solutions, independence and conditional independence relationships can be used to simplify the summations required.",Safety,4
1617,These relationships often correspond to our natural understanding of how the problem should be decomposed.,Safety,4
1618,Summary 13.7 SUMMARY 503 This chapter has suggested probability theory as a suitable foundation for uncertain reasoning and provided a gentle introduction to its use.,Constraint,4
1619,"It is inescapable in complex, nondeterministic, or partially observable environments.",Constraint,4
1620,An agent that violates the axioms must behave irrationally in some cases.,Interface,4
1621,"In Europe, the ﬁrst signiﬁcant systematic analyses were produced by Girolamo Cardano around 1565, although publication was posthumous (1663).",Safety,1
1622,"Pascal used probability in ways that required both the objective interpretation, as a prop- erty of the world based on symmetry or relative frequency, and the subjective interpretation, based on degree of belief—the former in his analyses of probabilities in games of chance, the latter in the famous “Pascal’s wager” argument about the possible existence of God.",Safety,4
1623,"Their analyses of degree of belief were closely tied to utili- ties and to behavior—speciﬁcally, to the willingness to place bets.",Safety,4
1624,"Rudolf Carnap, following Leibniz and Laplace, offered a different kind of subjective interpretation of probability— not as any actual individual’s degree of belief, but as the degree of belief that an idealized individual should have in a particular proposition a, given a particular body of evidence e.",Interface,4
1625,"Carnap was not able to extend his inductive logic much beyond the propositional case, and Putnam (1963) showed by adversar- ial arguments that some fundamental difﬁculties would prevent a strict extension to languages capable of expressing arithmetic.",Safety,4
1626,Cox’s theorem (1946) shows that any system for uncertain reasoning that meets his set of assumptions is equivalent to probability theory.,Safety,1
1627,"This gave renewed conﬁdence to those who already favored probability, but others were not convinced, pointing to the assumptions (primarily that belief must be represented by a single number, and thus the belief in ¬p must be a function of the belief in p).",Functional,3
1628,"One system outperformed human experts in the diagnosis of acute abdominal illnesses (de Dombal et al., 1974).",Safety,1
1629,"These early Bayesian systems suffered from a number of problems, however.",Constraint,1
1630,Domingos and Pazzani (1997) provide an explanation 506 Chapter 13.,Interface,4
1631,"13.2 Using the axioms of probability, prove that any probability distribution on a discrete random variable must sum to 1.",Safety,4
1632,We will work with propositions that correspond to exactly one possible world because they pin down the assignments of all the variables.,Constraint,4
1633,"In his letter of August 24, 1654, Pascal was trying to show how a pot of money should 13.9 be allocated when a gambling game must end prematurely.",Constraint,4
1634,How should the money be fairly split in this case?,Safety,4
1635,"(Fermat and Pascal made several errors before solving the problem, but you should be able to get it right the ﬁrst time.) 13.10 Deciding to put our knowledge of probability to good use, we encounter a slot ma- chine with three independently turning reels, each producing one of the four symbols BAR, BELL, LEMON, or CHERRY with equal probability.",Constraint,4
1636,Compute the probability that playing the slot machine once will result in a win.,Safety,4
1637,Suppose we want to ensure that the correct message is received with probability at least 1 − δ.,Safety,4
1638,"In this exercise, you will complete the normalization calculation for the meningitis 13.18 example.",Safety,4
1639,"Implement a hybrid probabilistic agent for the wumpus world, based on the hybrid 13.25 agent in Figure 7.20 and the probabilistic inference procedure outlined in this chapter.",Safety,4
1640,This chapter introduces a systematic way to represent such relation- ships explicitly in the form of Bayesian networks.,Safety,1
1641,"The topology of the network—the set of nodes and links—speciﬁes the conditional indepen- dence relationships that hold in the domain, in a way that will be made precise shortly.",Safety,2
1642,"The intuitive meaning of an arrow is typically that X has a direct inﬂuence on Y, which suggests that causes should be parents of effects.",Interface,4
1643,We will see that the combination of the topology and the conditional distributions sufﬁces to specify (implicitly) the full joint distribution for all the variables.,Safety,4
1644,"Each row must sum to 1, because the entries represent an exhaustive set of cases for the variable.",Interface,4
1645,"For Boolean variables, once you know that the prob- ability of a true value is p, the probability of false must be 1 – p, so we often omit the second number, as in Figure 14.2.",Safety,4
1646,"The two views are equivalent, but the ﬁrst turns out to be helpful in understanding how to construct networks, whereas the second is helpful in designing inference procedures.",Safety,4
1647,"We said that those parameters correspond to conditional probabilities P(Xi | P arents(Xi)); this is a true statement, but until we assign semantics to the network as a whole, we should think of them just as numbers θ(Xi | P arents(Xi)).",Safety,4
1648,We will now show that Equation (14.2) implies certain conditional independence relationships that can be used to guide the knowledge engineer in constructing the topology of the network.,Safety,4
1649,", X1) = P(Xi | P arents(Xi)) , (14.3) provided that P arents(Xi) ⊆ {Xi−1, .",Constraint,4
1650,Nodes: First determine the set of variables that are required to model the domain.,Safety,2
1651,"Any order will work, but the resulting network will be more compact if the variables are ordered such that causes precede effects.",Safety,4
1652,"The Semantics of Bayesian Networks 515 LOCALLY STRUCTURED SPARSE Intuitively, the parents of node Xi should contain all those nodes in X1, .",Safety,2
1653,"Thus, Alarm will be the only parent node for MaryCalls.",Safety,2
1654,The compactness of Bayesian net- works is an example of a general property of locally structured (also called sparse) systems.,Interface,1
1655,"In a locally structured system, each subcomponent interacts directly with only a bounded number of other components, regardless of the total number of components.",Constraint,1
1656,"If we assume n Boolean variables for simplicity, then the amount of information needed to specify each conditional probability table will be at most 2k numbers, and the complete network can be speciﬁed by n2k numbers.",Safety,4
1657,"Then the Bayesian network requires 960 numbers, but the full joint distribution requires over a billion.",Safety,4
1658,"In some domains, there will be slight dependencies that should strictly be included by adding a new link.",Constraint,4
1659,"But if these dependencies are tenuous, then it may not be worth the additional complexity in the network for the small gain in accuracy.",Safety,4
1660,"In each network, we Even in a locally structured domain, we will get a compact Bayesian network only if we choose the node ordering well.",Safety,2
1661,"What’s worse, some of the links represent tenuous relationships that require difﬁcult and unnatural probability judgments, such as as- Section 14.2.",Safety,4
1662,"If we stick to a causal model, we end up having to specify fewer numbers, and the numbers will often be easier to come up with.",Constraint,4
1663,This network requires 31 distinct probabilities to be speciﬁed—exactly the same number as the full joint distribution.,Safety,4
1664,"14.2.2 Conditional independence relations in Bayesian networks We have provided a “numerical” semantics for Bayesian networks in terms of the represen- tation of the full joint distribution, as in Equation (14.2).",Safety,4
1665,"(a) A node X is conditionally independent of its non-descendants (e.g., the (b) A node X is conditionally 14.3 EFFICIENT REPRESENTATION OF CONDITIONAL DISTRIBUTIONS CANONICAL DISTRIBUTION DETERMINISTIC NODES NOISY OR Even if the maximum number of parents k is smallish, ﬁlling in the CPT for a node requires up to O(2k) numbers and perhaps a great deal of experience with all the possible conditioning cases.",Constraint,2
1666,The simplest example is provided by deterministic nodes.,Safety,2
1667,"With 448 nodes and 906 links, it requires only 8,254 values instead of 133,931,430 for a network with full CPTs.",Constraint,2
1668,"Discretization is sometimes an adequate solution, but often results in a considerable loss of accuracy and very large CPTs.",Interface,4
1669,It seems reasonable to assume that the customer will buy if the cost is low and will not buy if it is high and that the probability of buying varies smoothly in some intermediate region.,Safety,4
1670,"14.4 EXACT INFERENCE IN BAYESIAN NETWORKS EVENT HIDDEN VARIABLE The basic task for any probabilistic inference system is to compute the posterior probability distribution for a set of query variables, given some observed event—that is, some assign- ment of values to a set of evidence variables.",Safety,1
1671,"To simplify the presentation, we will consider only one query variable at a time; the algorithms can easily be extended to queries with mul- tiple variables.",Constraint,3
1672,"We will use the notation from Chapter 13: X denotes the query variable; E denotes the set of evidence variables E1, .",Safety,4
1673,", Em, and e is a particular observed event; Y will denotes the nonevidence, nonquery variables Y1, .",Safety,4
1674,In this section we discuss exact algorithms for computing posterior probabilities and will consider the complexity of this task.,Safety,3
1675,"When E is Boolean, there is an ambiguity in that P (e) is used to mean both P (E = true) and P (E = e), but it should be clear from context which is intended; in particular, in the context of a sum the latter is intended.",Interface,4
1676,"(cid:19) f3(A, B, E) will be a 2 × 2 × 2 matrix, which is hard to show on the printed page.",Safety,4
1677,"Examining this sequence, we see that two basic computational operations are required: point- wise product of a pair of factors, and summing out a variable from a product of factors.",Constraint,4
1678,"For example, in the calculation shown previously, we eliminated A before E; if we do it the other way, the calculation becomes P(B | j, m) = α f1(B) × X a f4(A) × f5(A) × X e f2(E) × f3(A, B, E) , during which a new factor f6(A, B) will be generated.",Safety,4
1679,"In general, the time and space requirements of variable elimination are dominated by the size of the largest factor constructed during the operation of the algorithm.",Safety,3
1680,"Here, the size is deﬁned as the number of CPT entries; if the number of parents of each node is bounded by a constant, then the complexity will also be linear in the number of nodes.",Safety,2
1681,"Probabilistic Reasoning Once the network is in polytree form, a special-purpose inference algorithm is required, because ordinary inference methods cannot handle meganodes that share variables with each other.",Constraint,2
1682,"Essentially, the algorithm is a form of constraint propagation (see Chapter 6) where the constraints ensure that neighboring meganodes agree on the posterior probability of any vari- ables that they have in common.",Safety,2
1683,"However, the NP-hardness of the problem has not disappeared: if a network requires exponential time and space with variable elimination, then the CPTs in the clustered network will necessarily be exponentially large.",Constraint,4
1684,"This section describes randomized sampling algorithms, also called Monte Carlo algorithms, that provide approximate answers whose accuracy depends on the number of samples generated.",Interface,3
1685,"Sampling from this distribution is exactly like ﬂipping the coin: with probability 0.5 it will return heads, and with probability 0.5 it will return tails.",Constraint,4
1686,"This expression should look familiar, because it is also the probability of the event according to the Bayesian net’s repre- sentation of the joint distribution, as stated in Equation (14.2).",Safety,4
1687,"As more samples are collected, the estimate will converge to the true answer.",Safety,4
1688,"The standard deviation of the error in each probability will be proportional to 1/ n, where n is the number of samples used in the estimate.",Safety,4
1689,"Intuitively, events in which the actual evidence appears unlikely should be given less weight.",Safety,4
1690,"(Any topological ordering will do.) The process goes as follows: First, the weight w is set to 1.0.",Safety,4
1691,"Un- like the prior distribution P (z), the distribution SWS pays some attention to the evidence: the sampled values for each Zi will be inﬂuenced by evidence among Zi’s ancestors.",Safety,4
1692,"On the other hand, SWS pays less attention to the evidence than does the true posterior distribution P (z | e), because the sampled values for each Zi ignore evidence among Zi’s non-ancestors.5 For example, when sampling Sprinkler and Rain the algorithm ignores the evidence in the child variable WetGrass = true; this means it will generate many samples with Sprinkler = false and Rain = false despite the fact that the evidence actually rules out this case.",Constraint,3
1693,"If it could, then we could approximate the desired probability to arbitrary accuracy with a polynomial number of samples.",Constraint,4
1694,"It will, however, suffer a degradation in performance as the number of evidence variables increases.",Performance,4
1695,This is because most samples will have very low weights and hence the weighted estimate will be dominated by the tiny fraction of samples that accord more than an inﬁnitesimal likelihood to the evidence.,Interface,4
1696,"The problem is exacerbated if the evidence variables occur late in the variable ordering, because then the nonevidence variables will have no evidence in their parents and ancestors to guide the generation of sam- ples.",Safety,4
1697,This means the samples will be simulations that bear little resemblance to the reality suggested by the evidence.,Safety,4
1698,"(Other forms, some of them signiﬁcantly more powerful, are discussed in the notes at the end of the chapter.) We will ﬁrst describe what the algorithm does, then we will explain why it works.",Safety,3
1699,"(Shortly, we will show how to calculate this distribution.) Suppose the result is Cloudy = false.",Safety,4
1700,Why Gibbs sampling works We will now show that Gibbs sampling returns consistent estimates for posterior probabil- ities.,Constraint,4
1701,"state x (Markov chains also ﬁgure prominently in Chapters 15 and 17.) Now suppose that we run the Markov chain for t steps, and let πt(x) be the probability that the system is in state x at ′ at time t + 1.",Safety,1
1702,"Similarly, let πt+1(x ′) by summing, for all states the system could be in at time t, πt(x), we can calculate πt+1(x the probability of being in that state times the probability of making the transition to x ′) be the probability of being in state x ′: πt+1(x ′) = X x πt(x)q(x → x ′) .",Safety,1
1703,"(14.10) ERGODIC Provided the transition probability distribution q is ergodic—that is, every state is reachable from every other and there are no strictly periodic cycles—there is exactly one distribution π satisfying this equation for any given q.",Safety,4
1704,"Hence, the samples generated by Gibbs sampling will eventually be drawn from the true posterior distribution.",Safety,4
1705,"(14.12) i | parents(Xi)) × Y i | mb(Xi)) = α P (x′ Hence, to ﬂip each variable Xi conditioned on its Markov blanket, the number of multiplica- tions required is equal to the number of Xi’s children.",Safety,4
1706,"For example, suppose that an online book retailer would like to provide overall evalu- ations of products based on recommendations received from its customers.",Interface,4
1707,"The evaluation will take the form of a posterior distribution over the quality of the book, given the avail- able evidence.",Safety,4
1708,"(See Section 8.2.) The model also needs to deﬁne a probability for each such possible world, just as a Bayesian network deﬁnes a probability for each assignment of values to variables.",Safety,4
1709,"We will call models deﬁned in this way relational probability models, or RPMs.7 The most signiﬁcant difference be- tween the semantics of RPMs and the database semantics introduced in Section 8.2.8 is that RPMs do not make the closed-world assumption—obviously, assuming that every unknown fact is false doesn’t make sense in a probabilistic reasoning system!",Safety,1
1710,"(It turns out to be easier to view predicates as functions that return true or false.) We will also assume a type signature for each function, that is, a speciﬁcation of the type of each argument and the function’s value.",Functional,3
1711,"For the book-recommendation domain, the types are Customer and Book , and the type signatures for the functions and predicates are as follows: Honest : Customer → {true, false}Kindness : Customer → {1, 2, 3, 4, 5} Quality : Book → {1, 2, 3, 4, 5} Recommendation : Customer × Book → {1, 2, 3, 4, 5} The constant symbols will be whatever customer and book names appear in the retailer’s data set.",Safety,3
1712,8 Some technical conditions must be observed to guarantee that the RPM deﬁnes a proper distribution.,Safety,4
1713,"First, the dependencies must be acyclic, otherwise the resulting Bayesian network will have cycles and will not deﬁne a proper distribution.",Safety,4
1714,"Second, the dependencies must be well-founded, that is, there can be no inﬁnite ancestor chains, such as might arise from recursive dependencies.",Safety,4
1715,"For example, suppose that an honest customer who is a fan of a book’s author always gives the book a 5, regardless of quality: Recommendation (c, b) ∼ if Honest(c) then if Fan(c, Author (b)) then Exactly(5) else HonestRecCPT (Kindness(c), Quality (b)) else h0.4, 0.1, 0.0, 0.1, 0.4i Again, the conditional test Fan(c, Author (b)) is unknown, but if a customer gives only 5s to a particular author’s books and is not otherwise especially kind, then the posterior probability that the customer is a fan of that author will be high.",Interface,4
1716,"Furthermore, the posterior distribution will tend to discount the customer’s 5s in evaluating the quality of that author’s books.",Safety,4
1717,"How can the system reason about whether, say, C1 is a fan of Author (B2) when Author (B2) is unknown?",Safety,1
1718,The answer is that the system may have to reason about all possible authors.,Safety,1
1719,"In case you are wondering how the system can possibly work out who the author of B2 is: consider the possibility that three other customers are fans of A1 (and have no other favorite authors in common) and all three have given B2 a 5, even though most other cus- tomers ﬁnd it quite dismal.",Safety,1
1720,"First, the presence of repeated substructure in the unrolled Bayes net means that many of the factors constructed during variable elimination (and similar kinds of tables constructed by cluster- ing algorithms) will be identical; effective caching schemes have yielded speedups of three orders of magnitude for large networks.",Safety,3
1721,"(See page 322.) Resolution theorem-provers and logic programming systems avoid propositionalizing by instantiating the logical variables only as needed to make the inference go through; that is, they lift the inference process above the level of ground propositional sentences and make each lifted step do the work of many ground steps.",Safety,1
1722,"(In partic- ular, all observations about an object are correctly associated with the constant symbol that names it.) In many real-world settings, however, these assumptions are simply untenable.",Constraint,4
1723,"We gave the examples of multiple ISBNs and sibyl attacks in the book-recommendation domain (to which we will return in a moment), but the phenomenon is far more pervasive: Section 14.6.",Safety,4
1724,"Relational and First-Order Probability Models 545 OPEN UNIVERSE • A vision system doesn’t know what exists, if anything, around the next corner, and may not know if the object it sees now is the same one it saw a few minutes ago.",Safety,1
1725,"• A text-understanding system does not know in advance the entities that will be featured in a text, and must reason about whether phrases such as “Mary,” “Dr.",Safety,1
1726,"In fact, a major part of human cognition seems to require learning what objects exist and being able to connect observations—which almost never come with unique IDs attached—to hypothesized objects in the world.",Constraint,4
1727,"A language for OUPMs provides a way of writing such models easily while guaranteeing a unique, consistent probability distribution over the inﬁnite space of possible worlds.",Safety,4
1728,There are some tricky issues involved in designing these algorithms.,Constraint,3
1729,"Moreover, transitions must allow for merging two objects into one or splitting one into two.",Constraint,4
1730,"Research in this area is still at an early stage, but already it is becoming clear that ﬁrst- order probabilistic reasoning yields a tremendous increase in the effectiveness of AI systems at handling uncertain information.",Interface,1
1731,"The earliest expert systems of the 1970s ignored uncertainty and used strict logical reasoning, but it soon became clear that this was impractical for most real-world domains.",Safety,1
1732,The next generation of expert systems (especially in medical domains) used prob- abilistic techniques.,Safety,1
1733,"Initial results were promising, but they did not scale up because of the exponential number of probabilities required in the full joint distribution.",Safety,4
1734,"(Neither are we aware of doing uniﬁcation, yet we seem to be capable of some kind of logical reasoning.) It might be that we have some kind of numerical degrees of belief encoded directly in strengths of connections and activations in our neurons.",Constraint,4
1735,One should also note that qualitative reason- Section 14.7.,Constraint,4
1736,"Such approaches hope to build on the success of logical rule-based systems, but add a sort of “fudge factor” to each rule to accommodate uncertainty.",Safety,1
1737,These methods were developed in the mid-1970s and formed the basis for a large number of expert systems in medicine and other areas.,Safety,1
1738,"We will not provide detailed technical material, but we cite references for further study.",Interface,4
1739,14.7.1 Rule-based methods for uncertain reasoning Rule-based systems emerged from early work on practical and intuitive systems for logical inference.,Constraint,1
1740,"Logical systems in general, and logical rule-based systems in particular, have three desirable properties: • Locality: In logical systems, whenever we have a rule of the form A ⇒ B, we can conclude B, given evidence A, without worrying about any other rules.",Safety,1
1741,"In probabilistic systems, we need to consider all the evidence.",Safety,1
1742,"The bad news for rule-based systems is that the properties of locality, detachment, and truth-functionality are simply not appropriate for uncertain reasoning.",Safety,1
1743,"Clearly, all three events have the same probability, 0.5, and so a truth-functional system must assign the same belief to the disjunction of any two of them.",Safety,1
1744,Truth-functional systems have rules of the form A 7→ B that allow us to compute the belief in B as a function of the belief in the rule and the belief in A.,Functional,1
1745,Both forward- and backward-chaining systems can be devised.,Constraint,1
1746,"Clearly, uncertain reasoning systems need to keep track of the paths along which evidence is propagated.",Safety,1
1747,"Chaining forward through our rules, this increases the belief that the grass will be wet, which in turn increases the belief that it is raining.",Safety,4
1748,But this is ridiculous: the fact that the sprinkler is on explains away the wet grass and should reduce the belief in rain.,Safety,4
1749,A truth-functional system acts as if it also believes Sprinkler 7→ Rain.,Safety,1
1750,"Given these difﬁculties, how can truth-functional systems be made useful in practice?",Constraint,1
1751,"The most famous example of a truth-functional system for uncertain reasoning is the certainty factors model, which was developed for the MYCIN medical diagnosis program and was widely used in expert systems of the late 1970s and 1980s.",Safety,1
1752,14.7.2 Representing ignorance: Dempster–Shafer theory The Dempster–Shafer theory is designed to deal with the distinction between uncertainty and ignorance.,Constraint,4
1753,"Given that the coin might or might not be fair, what belief should you ascribe to the event that it comes up heads?",Safety,4
1754,This makes Dempster–Shafer reasoning systems skeptical in a way that has some intuitive appeal.,Constraint,1
1755,The masses still must add to 1 over all possible events.,Safety,4
1756,"Whenever there is a gap in the beliefs, then a decision problem can be deﬁned such that a Dempster– Shafer system is unable to make a decision.",Safety,1
1757,Pearl (1988) has argued that Bel (A) should be interpreted not as a degree of belief in A but as the probability assigned to all the possible worlds (now interpreted as logical theories) in which A is provable.,Safety,4
1758,"If the coin came from a bank, then seeing it come up heads three times running would have almost no effect on our strong prior belief in its fairness; but if the coin comes from the magician’s pocket, the same evidence will lead to a stronger posterior belief that the coin is biased toward heads.",Safety,4
1759,FUZZY SET THEORY FUZZY LOGIC Fuzzy logic is therefore a truth-functional system—a fact that causes serious difﬁculties.,Safety,1
1760,Fuzzy control is a methodology for constructing control systems in which the mapping between real-valued input and output parameters is represented by fuzzy rules.,Safety,1
1761,"Critics (see, e.g., Elkan, 1993) argue that these applications are successful because they have small rule bases, no chaining of inferences, and tunable parameters that can be adjusted to improve the system’s performance.",Performance,1
1762,"The fact that they are implemented with fuzzy operators might be incidental to their success; the key is simply to provide a concise and intuitive way to specify a smoothly interpolated, real-valued function.",Functional,3
1763,There have been attempts to provide an explanation of fuzzy logic in terms of probabil- ity theory.,Interface,4
1764,• Bayesian networks provide a concise way to represent conditional independence rela- tionships in the domain.,Interface,4
1765,• Probability theory can be combined with representational ideas from ﬁrst-order logic to produce very powerful systems for reasoning under uncertainty.,Constraint,1
1766,• Various alternative systems for reasoning under uncertainty have been suggested.,Constraint,1
1767,"Gen- erally speaking, truth-functional systems are not well suited for such reasoning.",Constraint,1
1768,"Judea Pearl developed the message-passing method for carrying out inference in tree networks (Pearl, 1982a) and poly- tree networks (Kim and Pearl, 1983) and explained the importance of causal rather than di- agnostic probability models, in contrast to the certainty-factor systems then in vogue.",Safety,1
1769,"The ﬁrst expert system using Bayesian networks was CONVINCE (Kim, 1983).",Safety,1
1770,"Early applications in medicine included the MUNIN system for diagnosing neuromuscular disorders (Andersen et al., 1989) and the PATHFINDER system for pathology (Heckerman, 1991).",Safety,1
1771,"The CPCS system (Pradhan et al., 1994) is a Bayesian network for internal medicine consisting 10 I.",Safety,1
1772,"Perhaps the most widely used Bayesian network systems have been the diagnosis- and-repair modules (e.g., the Printer Wizard) in Microsoft Windows (Breese and Heckerman, 1996) and the Ofﬁce Assistant in Microsoft Ofﬁce (Horvitz et al., 1998).",Safety,1
1773,"This ap- proach is implemented in the HUGIN system, an efﬁcient and widely used tool for uncertain reasoning (Andersen et al., 1989).",Interface,1
1774,The inclusion of discrete variables has been investigated by Lauritzen and Wermuth (1989) and implemented in the MARKOV NETWORK NONSERIAL DYNAMIC PROGRAMMING 554 Chapter 14.,Safety,4
1775,"Probabilistic Reasoning cHUGIN system (Olesen, 1993).",Safety,1
1776,"The query is answered by summing over all the instantiations of the cutset, so the overall space requirement is still lin- ear (Pearl, 1988).",Safety,4
1777,"The reduced problem is described by some variational pa- rameters λ that are adjusted to minimize a distance function D between the original and the reduced problem, often by solving the system of equations ∂D/∂λ = 0.",Functional,1
1778,The remarkable paper by Wainwright and Jordan (2008) provides a uni- fying theoretical analysis of the literature on variational methods.,Safety,3
1779,"(1998) observed that message passing in a multiply connected Bayesian network was exactly the computation performed by the turbo decoding algorithm (Berrou et al., 1993), which pro- vided a major breakthrough in the design of efﬁcient error-correcting codes.",Safety,3
1780,"(2003) developed a complex generative model for authors, papers, and citation strings, involving both relational and identity uncertainty, and demonstrated high accuracy for citation information extraction.",Constraint,4
1781,"RECORD LINKAGE Bibliographical and Historical Notes 557 POSSIBILITY THEORY As explained in Chapter 13, early probabilistic systems fell out of favor in the early 1970s, leaving a partial vacuum to be ﬁlled by alternative methods.",Safety,1
1782,"Certainty factors were invented for use in the medical expert system MYCIN (Shortliffe, 1976), which was intended both as an engineering solution and as a model of human judgment under uncertainty.",Interface,1
1783,"The collection Rule-Based Expert Systems (Buchanan and Shortliffe, 1984) provides a complete overview of MYCIN and its descendants (see also Steﬁk, 1995).",Safety,1
1784,"The PROSPECTOR expert system (Duda et al., 1979) used a rule-based approach in which the rules were justiﬁed by a (seldom tenable) global independence assumption.",Safety,1
1785,Fuzzy sets were developed by Lotﬁ Zadeh (1965) in response to the perceived difﬁculty of providing exact inputs to intelligent systems.,Safety,1
1786,The text by Zimmermann (2001) provides a thorough introduction to fuzzy set theory; papers on fuzzy applications are collected in Zimmermann (1999).,Safety,1
1787,"Possibility theory (Zadeh, 1978) was introduced to handle uncertainty in fuzzy systems and has much in common with probability.",Constraint,1
1788,"The development of qualitative probabilistic networks (Wellman, 1990a) provided a purely qualitative abstraction of Bayesian networks, using the notion of positive and negative inﬂuences between variables.",Safety,4
1789,"The most important single publication in the growth of Bayesian networks was undoubt- edly the text Probabilistic Reasoning in Intelligent Systems (Pearl, 1988).",Safety,1
1790,"Several excellent texts (Lauritzen, 1996; Jensen, 2001; Korb and Nicholson, 2003; Jensen, 2007; Darwiche, 2009; Koller and Friedman, 2009) provide thorough treatments of the topics we have cov- ered in this chapter.",Interface,4
1791,"The proceedings of the conferences on Uncertainty in Artiﬁcial Intelligence (UAI), Neural Infor- mation Processing Systems (NIPS), and Artiﬁcial Intelligence and Statistics (AISTATS) are excellent sources for current research.",Safety,1
1792,"Arc reversal may require introducing new arcs: all the parents of X also become parents of Y , and all parents of Y also become parents of X.",Safety,4
1793,"Use this to calculate the value of q, and, given what you know about handedness in humans, explain why the hypothesis described at the beginning of this question must be wrong.",Safety,4
1794,Battery Radio Ignition Gas Starts Moves A Bayesian network describing some features of a car’s electrical system Figure 14.21 and engine.,Safety,1
1795,"Each telescope can also (with a much smaller probability f ) be badly out of focus (events F1 and F2), in which case the scientist will undercount by three or more stars (or if N is less than 3, fail to detect any stars at all).",Constraint,4
1796,Each entry in the conditional distribution should be expressed as a function of the parameters e and/or f .,Functional,3
1797,"METROPOLIS– HASTINGS 14.19 The Metropolis–Hastings algorithm is a member of the MCMC family; as such, it is designed to generate samples x (eventually) according to target probabilities π(x).",Safety,3
1798,"In Section 16.5, we describe the implementation of decision-making systems.",Safety,1
1799,The remainder of the chapter discusses issues that arise in applications of decision theory to expert systems.,Safety,1
1800,"(16.1) MAXIMUM EXPECTED UTILITY The principle of maximum expected utility (MEU) says that a rational agent should choose the action that maximizes the agent’s expected utility: action = argmax a EU (a|e) In a sense, the MEU principle could be seen as deﬁning all of AI.",Safety,4
1801,"The MEU principle formalizes the general notion that the agent should “do the right thing,” but goes only a small distance toward a full operationalization of that advice.",Safety,4
1802,"Es- timating the state of the world requires perception, learning, knowledge representation, and inference.",Safety,4
1803,"Computing P (RESULT(a) | a, e) requires a complete causal model of the world and, as we saw in Chapter 14, NP-hard inference in (very large) Bayesian networks.",Safety,4
1804,"Comput- ing the outcome utilities U (s′) often requires searching or planning, because an agent may not know how good a state is until it knows where it can get to from that state.",Interface,4
1805,"So, decision theory is not a panacea that solves the AI problem—but it does provide a useful framework.",Interface,1
1806,"Consider the environments that could lead to an agent having a given percept history, and consider the different agents that we could design.",Interface,4
1807,"If an agent acts so as to maximize a utility function that correctly reﬂects the performance measure, then the agent will achieve the highest possible performance score (averaged over all the possible environments).",Performance,3
1808,"After all, why should maximizing the average utility be so special?",Safety,4
1809,"Finally, why should a utility function with the required properties exist at all?",Functional,3
1810,16.2.1 Constraints on rational preferences These questions can be answered by writing down some constraints on the preferences that a rational agent should have and then showing that the MEU principle can be derived from the constraints.,Constraint,4
1811,"To address this issue we list six constraints that we require any reasonable preference relation to obey: • Orderability: Given any two lotteries, a rational agent must either prefer one to the other or else rate the two as equally preferable.",Constraint,4
1812,"• Transitivity: Given any three lotteries, if an agent prefers A to B and prefers B to C, then the agent must prefer A to C.",Interface,4
1813,"• Continuity: If some lottery B is between A and C in preference, then there is some probability p for which the rational agent will be indifferent between getting B for sure and the lottery that yields A with probability p and C with probability 1 − p.",Constraint,4
1814,"If an agent prefers A to B, then the agent must prefer the lottery that has a higher probability for A (and vice versa).",Interface,4
1815,Each axiom can be motivated by showing that an agent that violates it will exhibit patently irrational behavior in some situations.,Interface,4
1816,"The agent prefers C, and so would be willing to make this trade.",Safety,4
1817,You get the same results in either measurement system.,Safety,1
1818,We know there are some axioms on utilities that all rational agents must obey.,Constraint,4
1819,"Fortunately, the preferences of real agents are usually more systematic, and thus easier to deal with.",Constraint,1
1820,"16.3.1 Utility assessment and utility scales If we want to build a decision-theoretic system that helps the agent make decisions or acts on his or her behalf, we must ﬁrst work out what the agent’s utility function is.",Functional,1
1821,"If you ask people how much they would pay to avoid a risk—for example, to avoid playing Russian roulette with a million-barreled revolver—they will respond with very large numbers, perhaps tens of thousands of dollars, but their actual behavior reﬂects a much lower monetary value for a micromort.",Constraint,4
1822,"People appear to be willing to pay about $10,000 (at 2009 prices) more for a safer car that halves the risk of death, or about $50 per micromort.",Safety,4
1823,Patients with a disability are willing to accept a shorter life expectancy to be restored to full health.,Constraint,4
1824,"16.3.2 The utility of money Utility theory has its roots in economics, and economics provides one obvious candidate for a utility measure: money (or more speciﬁcally, an agent’s total net assets).",Interface,4
1825,"It will usually be the case that an agent prefers more money to less, all other things being equal.",Interface,4
1826,"We should not assume that this is the deﬁnitive utility function for monetary value, but it is likely that most people have a utility function that is concave for positive wealth.",Functional,3
1827,Making Simple Decisions The value an agent will accept in lieu of a lottery is called the certainty equivalent of the lottery.,Interface,4
1828,"Studies have shown that most people will accept about $400 in lieu of a gamble that gives $1000 half the time and $0 the other half—that is, the certainty equivalent of the lottery is $400, while the EMV is $500.",Safety,4
1829,"Notice that for small changes in wealth relative to the current wealth, almost any curve will be approximately linear.",Safety,4
1830,"If we have calculated the expected utility correctly according to our probability model, and if the probability model correctly reﬂects the underlying stochastic processes that generate the outcomes, then, on average, we will get the utility we expect if the whole process is repeated many times.",Safety,4
1831,"We will assume, kindly perhaps, that the estimates d estimates are unbiased, that is, the expected value of the error, E( EU (a|e) − EU (a|e))), is d zero.",Safety,4
1832,"Unfortunately, the real outcome will usually be signiﬁcantly worse than we estimated, even though the estimate was unbiased!",Safety,4
1833,"Now, as we actually start to generate the estimates, some of the errors will be negative (pessimistic) and some will be positive (optimistic).",Safety,4
1834,"The curve in Figure 16.3 for k = 3 has a mean around 0.85, so the average disappointment will be about 85% of the standard deviation in the utility estimates.",Safety,4
1835,"Plot of the error in each of k utility estimates and of the distribution of the With more choices, extremely optimistic estimates are more likely to arise: for k = 30, the disappointment will be around twice the standard deviation in the estimates.",Constraint,4
1836,Serious manifestations include believing that an exciting new drug that has cured 80% patients in a trial will cure 80% of patients (it’s been chosen from k = thousands of candidate drugs) or that a mutual fund advertised as having above- average returns will continue to have them (it’s been chosen to appear in the advertisement out of k = dozens of funds in the company’s overall portfolio).,Interface,4
1837,16.3.4 Human judgment and irrationality NORMATIVE THEORY DESCRIPTIVE THEORY Decision theory is a normative theory: it describes how a rational agent should act.,Constraint,4
1838,So perhaps people who choose B over A and C over D are not being irrational; they are just saying that they are willing to give up $200 of EMV to avoid a 20% chance of feeling like an idiot.,Interface,4
1839,Your payoff will depend on the color of a ball chosen from an urn.,Interface,4
1840,It should be clear that if you think there are more red than black balls then you should prefer A over B and C over D; if you think there are fewer red than black you should prefer the opposite.,Safety,4
1841,"The restaurant takes advantage of this by offering a $200 bottle that it knows nobody will buy, but which serves to skew upward the customer’s estimate of the value of all wines and make the $55 bottle seem like a bargain.",Safety,4
1842,"Let us grant, for the sake of argument, that the brain has built-in neural mechanism for computing with probabilities and utilities, or something functionally equivalent; if so, the required inputs would be obtained through accumulated experience of outcomes and rewards rather than through linguistic presentations of numerical values.",Constraint,3
1843,"For example, in deciding what levels of harmful emissions to allow from a power plant, pol- icy makers must weigh the prevention of death and disability against the beneﬁt of the power and the economic burden of mitigating the emissions.",Safety,4
1844,Siting a new airport requires consid- eration of the disruption caused by construction; the cost of land; the distance from centers of population; the noise of ﬂight operations; safety issues arising from local topography and weather conditions; and so on.,Safety,4
1845,"We will call the attributes X = X1, .",Safety,4
1846,", Xn; a complete vector of assignments will be x = hx1, .",Constraint,4
1847,"We will assume that higher values of an attribute correspond to higher utilities, all other things being equal.",Interface,4
1848,"(See Figure 16.4(b).) Of course, this will probably occur even less often than in the deterministic case.",Safety,4
1849,"If S1 is closer than S2, then S1 will dominate S2 on cost.",Constraint,4
1850,"Although we will not present them here, there exist algorithms for propagating this kind of qualitative information among uncertain variables in qualitative probabilistic networks, enabling a system to make rational decisions based on stochastic dominance, without using any numeric values.",Safety,1
1851,"For n attributes, assessing an additive value function requires assessing n separate one-dimensional value functions rather than one n-dimensional function; typically, this repre- sents an exponential reduction in the number of preference experiments that are needed.",Functional,3
1852,"Even when MPI does not strictly hold, as might be the case at extreme values of the attributes, an additive value function might still provide a good approximation to the agent’s preferences.",Interface,3
1853,"The hunting dogs are very valuable, but if you don’t have enough cages for the chickens, the dogs will eat the chickens; hence, the tradeoff between dogs and chickens depends strongly on the number of cages, and MPI is violated.",Safety,4
1854,"Each of the single-attribute utility functions can be developed independently of the other attributes, and this combination will be guaranteed to generate the correct overall preferences.",Safety,3
1855,Additional assumptions are required to obtain a purely additive utility function.,Functional,3
1856,"The notation is often called an inﬂuence diagram (Howard and Matheson, 1984), but we will use the more descriptive term decision network.",Interface,4
1857,"CHANCE NODES 16.5.1 Representing a decision problem with a decision network In its most general form, a decision network represents information about the agent’s current state, its possible actions, the state that will result from the agent’s action, and the utility of that state.",Constraint,2
1858,It therefore provides a substrate for implementing utility-based agents of the type ﬁrst introduced in Section 2.4.5.,Safety,4
1859,"The choice inﬂuences the cost, safety, and noise that will result.",Safety,4
1860,Chapter 17 deals with cases in which more than one decision must be made.,Constraint,4
1861,This is a straightforward extension of the Bayesian network algorithm and can be incorpo- rated directly into the agent design given in Figure 13.1 on page 484.,Safety,3
1862,We will see in Chap- ter 17 that the possibility of executing several actions in sequence makes the problem much more interesting.,Safety,4
1863,"16.6 THE VALUE OF INFORMATION In the preceding analysis, we have assumed that all relevant information, or at least all avail- able information, is provided to the agent before it makes its decision.",Safety,4
1864,"For example, a doctor cannot expect to be provided with the results of all possible diagnostic tests and questions at the time a patient ﬁrst enters the consulting room.10 Tests are often expensive and sometimes hazardous (both directly and because of associated delays).",Constraint,4
1865,"This section describes information value theory, which enables an agent to choose what information to acquire.",Interface,4
1866,The value of any particular observation must derive from the potential to affect the agent’s eventual physical action; and this potential can be estimated directly from the decision model itself.,Safety,4
1867,"If the company is risk-neutral, then it will be indifferent between buying a block and not buying one.",Safety,4
1868,How much should the company be willing to pay for the information?,Safety,4
1869,"The way to answer this question is to examine what the company would do if it had the information: • With probability 1/n, the survey will indicate oil in block 3.",Constraint,4
1870,"In this case, the company will buy block 3 for C/n dollars and make a proﬁt of C − C/n = (n − 1)C/n dollars.",Safety,4
1871,"• With probability (n−1)/n, the survey will show that the block contains no oil, in which case the company will buy a different block.",Constraint,4
1872,"Therefore, the company should be willing to pay the seismologist up to C/n dollars for the information: the information is worth as much as the block itself.",Safety,4
1873,"Then the value of the current best action α is deﬁned by EU (α|e) = max a X s′ P (RESULT(a) = s′ | a, e) U (s′) , and the value of the new best action (after the new evidence Ej = ej is obtained) will be EU (αej |e, ej) = max a X s′ P (RESULT(a) = s′ | a, e, ej) U (s′) .",Safety,4
1874,"But Ej is a random variable whose value is currently unknown, so to determine the value of discovering Ej, given current information e we must average over all possible values ejk that we might discover for Ej, using our current beliefs about its value: VPI e(Ej) = X k P (Ej = ejk|e) EU (αejk |e, Ej = ejk) !",Safety,4
1875,"The information Ej = ejk will yield some new expected utilities U ′ 2 for the actions, but before we obtain Ej, we will have some probability distributions over the possible values of 1 and U ′ U ′ 2 (which we assume are independent).",Safety,4
1876,"Obviously, in this case, it is not worth the expense of obtaining satellite reports, because it is unlikely that the information derived from them will change the plan.",Safety,4
1877,"There is a signiﬁcant possibility that the second route will turn out to be clear while the ﬁrst is blocked, and in this 1 and U ′ 11 There is no loss of expressiveness in requiring perfect information.",Safety,4
1878,"In (a), a1 will almost cer- Figure 16.8 tainly remain superior to a2, so the information is not needed.",Safety,4
1879,(Note: The fact that U2 has a high peak in (c) means that its expected value is known with higher certainty than U1.) case the difference in utilities will be very high.,Constraint,4
1880,"In this case, however, the difference in value between the two routes is still likely to be very small, so we will not bother to obtain the reports.",Safety,4
1881,"In sum, information has value to the extent that it is likely to cause a change of plan and to the extent that the new plan will be signiﬁcantly better than the old plan.",Safety,4
1882,"Intuitively, one should expect this to be impossible.",Constraint,4
1883,"For example, a medical test that gives a false positive result may lead to unnecessary surgery; but that does not mean that the test shouldn’t be done.",Safety,4
1884,"For any given piece of evidence Ej, the value of acquiring it can go down (e.g., if another variable strongly constrains the posterior for Ej) or up (e.g., if another variable provides a clue on which Ej builds, enabling a new and better plan to be devised).",Safety,4
1885,"16.6.4 Implementation of an information-gathering agent A sensible agent should ask questions in a reasonable order, should avoid asking questions that are irrelevant, should take into account the importance of each piece of information in relation to its cost, and should stop asking questions when that is appropriate.",Interface,4
1886,Figure 16.9 shows the overall design of an agent that can gather information intel- ligently before acting.,Interface,4
1887,We assume that the result of the action Request (Ej) is that the next percept provides the value of Ej.,Safety,4
1888,The agent algorithm we have described implements a form of information gathering that is called myopic.,Safety,3
1889,"This is because it uses the VPI formula shortsightedly, calculating the value of information as if only a single evidence variable will be acquired.",Safety,4
1890,"(For example, it has been shown to outperform expert physicians in selecting diagnostic tests.) function INFORMATION-GATHERING-AGENT( percept) returns an action persistent: D , a decision network integrate percept into D j ← the value that maximizes VPI (Ej) / Cost (Ej ) if VPI (Ej) > Cost (Ej ) return REQUEST(Ej) else return the best action from D Design of a simple information-gathering agent.",Functional,3
1891,"Decision-Theoretic Expert Systems 633 However, if there is no single evidence variable that will help a lot, a myopic agent might hastily take an action when it would have been better to request two or more variables ﬁrst and then take action.",Interface,1
1892,One ﬁnal consideration is the effect a series of questions will have on a human respon- dent.,Safety,4
1893,"People may respond better to a series of questions if they “make sense,” so some expert systems are built to take this into account, asking questions in an order that maximizes the total utility of the system and human rather than an order that maximizes value of information.",Interface,1
1894,"16.7 DECISION-THEORETIC EXPERT SYSTEMS DECISION ANALYSIS DECISION MAKER DECISION ANALYST The ﬁeld of decision analysis, which evolved in the 1950s and 1960s, studies the application of decision theory to actual decision problems.",Safety,1
1895,"It is used to help make rational decisions in important domains where the stakes are high, such as business, government, law, military strategy, medical diagnosis and public health, engineering design, and resource management.",Safety,4
1896,"As more and more decision processes become automated, decision analysis is increasingly used to ensure that the automated processes are behaving as desired.",Safety,4
1897,"Early expert system research concentrated on answering questions, rather than on mak- ing decisions.",Safety,1
1898,"Those systems that did recommend actions rather than providing opinions on matters of fact generally did so using condition-action rules, rather than with explicit rep- resentations of outcomes and preferences.",Constraint,1
1899,The emergence of Bayesian networks in the late 1980s made it possible to build large-scale systems that generated sound probabilistic infer- ences from evidence.,Safety,1
1900,"The addition of decision networks means that expert systems can be developed that recommend optimal decisions, reﬂecting the preferences of the agent as well as the available evidence.",Safety,1
1901,A system that incorporates utilities can avoid one of the most common pitfalls associ- ated with the consultation process: confusing likelihood and importance.,Constraint,1
1902,"A common strategy in early medical expert systems, for example, was to rank possible diagnoses in order of like- lihood and report the most likely.",Safety,1
1903,"Obviously, a testing or treatment plan should depend both on probabilities and utilities.",Constraint,4
1904,"Current medical expert systems can take into account the value of information to recommend tests, and then describe a differential diagnosis.",Safety,1
1905,"The problem is to decide what treatment to use and when to do it: the younger the infant, the greater the risks of certain treatments, but one mustn’t wait too long.",Safety,4
1906,A decision-theoretic expert system for this problem can be created by a team consisting of at least one domain expert (a pediatric cardiologist) and one knowledge engineer.,Safety,1
1907,"Some of this will be well known to the domain expert, and some will come from the literature.",Safety,4
1908,Often the model will match well with the informal graphical descriptions given in medical textbooks.,Constraint,4
1909,Sometimes variables will have to be split or joined to match the expert’s intuitions.,Safety,4
1910,Note that a diagnostic system will reason from symp- toms and other observations to the disease or other cause of the problems.,Safety,1
1911,"Thus, in the early years of building these systems, experts were asked for the probability of a cause given an effect.",Interface,1
1912,"So modern systems usually assess causal knowledge and encode it directly in the Bayesian network structure of the model, leaving the diagnostic reasoning to the Bayesian network inference algorithms (Shachter and Heckerman, 1987).",Safety,1
1913,"To evaluate the system we need a set of correct (input, output) pairs; a so-called gold standard to compare against.",Safety,1
1914,"For medical expert systems this usually means assembling the best available doctors, presenting them with a few cases, Section 16.7.",Constraint,1
1915,Decision-Theoretic Expert Systems 635 Sex Postcoarctectomy Syndrome Paradoxical Hypertension Aortic Aneurysm Paraplegia Late Result CVA Failure To Thrive Intercostal Recession Treatment Intermediate Result Tachypnea Tachycardia Dyspnea Heart Failure Age Hepato- megaly Pulmonary Crepitations Cardiomegaly Aortic Dissection Myocardial Infarction Figure 16.10 Inﬂuence diagram for aortic coarctation (courtesy of Peter Lucas).,Constraint,1
1916,We then see how well the system matches their recommendations.,Safety,1
1917,"It can be useful to run the system “backward.” Instead of presenting the system with symptoms and asking for a diagnosis, we can present it with a diagnosis such as “heart failure,” examine the predicted probability of symptoms such as tachycardia, and compare with the medical literature.",Constraint,1
1918,This important step checks whether the best decision is sensitive to small changes in the assigned probabilities and utilities by systematically varying If small changes lead to signiﬁcantly those parameters and running the evaluation again.,Safety,1
1919,"If all variations lead to the same decision, then the agent will have more conﬁdence that it is the right decision.",Safety,4
1920,Making Simple Decisions criticisms of probabilistic approaches to expert systems is that it is too difﬁcult to assess the numerical probabilities required.,Safety,1
1921,16.8 SUMMARY This chapter shows how to combine utility theory with probability to enable an agent to select actions that will maximize its expected performance.,Constraint,4
1922,"• Probability theory describes what an agent should believe on the basis of evidence, utility theory describes what an agent wants, and decision theory puts the two together to describe what an agent should do.",Interface,4
1923,• We can use decision theory to build a system that makes decisions by considering all possible actions and choosing the one that leads to the best expected outcome.,Safety,1
1924,Such a system is known as a rational agent.,Safety,1
1925,• Decision networks provide a simple formalism for expressing and solving decision problems.,Interface,4
1926,• Expert systems that incorporate utility information have additional capabilities com- pared with pure inference systems.,Constraint,1
1927,"BIBLIOGRAPHICAL AND HISTORICAL NOTES The book L’art de Penser, also known as the Port-Royal Logic (Arnauld, 1662) states: To judge what one must do to obtain a good or avoid an evil, it is necessary to consider not only the good and the evil in itself, but also the probability that it happens or does not happen; and to view geometrically the proportion that all these things have together.",Interface,4
1928,"Bibliographical and Historical Notes 637 Modern texts talk of utility rather than good and evil, but this statement correctly notes that one should multiply utility by probability (“view geometrically”) to give expected utility, and maximize that over all outcomes (“all these things”) to “judge what one must do.” It is remarkable how much this got right, 350 years ago, and only 8 years after Pascal and Fermat showed how to use probability correctly.",Constraint,4
1929,"Petersburg paradox (see Exercise 16.3), was the ﬁrst to realize the importance of preference measurement for lotteries, writing “the value of an item must not be based on its price, but rather on the utility that it yields” (ital- ics his).",Interface,4
1930,Von Winterfeldt and Edwards (1986) provide a modern perspective on decision analysis and its relationship to human preference structures.,Interface,4
1931,"However, Richard Thaler (1992) found irrational fram- ing effects on the price one is willing to pay to avoid a risk of death versus the price one is willing to be paid to accept a risk.",Safety,4
1932,How much are people willing to pay for a QALY?,Constraint,4
1933,But how should he feel if he won against 50 others?,Constraint,4
1934,"Ill.” Finally, behind both curses is the general phenomenon of regression to the mean, whereby individuals selected on the basis of exceptional characteristics previously exhibited will, with high probability, become less exceptional in future.",Constraint,4
1935,They describe early computer implementations of methods for eliciting the necessary parameters for a multiattribute utility function and include extensive accounts of real appli- cations of the theory.,Functional,3
1936,"In AI, the principal reference for MAUT is Wellman’s (1985) paper, which includes a system called URP (Utility Reasoning Package) that can use a collection of statements about preference independence and conditional independence to analyze the structure of decision problems.",Safety,1
1937,"Wellman and Doyle (1992) provide a preliminary sketch of how a complex set of utility-independence re- lationships might be used to provide a structured model of a utility function, in much the same way that Bayesian networks provide a structured model of joint probability distribu- tions.",Interface,3
1938,This algorithm was also one of the ﬁrst to provide complete inference for multiply connected Bayesian networks.,Interface,3
1939,"His paper ends with the remark “If information value theory and associated decision theoretic structures do not in the future occupy a large part of the education of engineers, then the engineering profession will ﬁnd that its traditional role of managing scientiﬁc and economic resources for the beneﬁt of man has been forfeited to another profession.” To date, the implied revolution in managerial methods has not occurred.",Constraint,3
1940,"After the resurgence of interest in probabilistic methods in AI in the 1980s, decision-theoretic expert systems gained widespread acceptance (Horvitz et al., 1988; Cowell et al., 2002).",Safety,1
1941,"In fact, from 1991 onward, the cover design of the journal Artiﬁcial Intelligence has depicted a decision network, although some artistic license appears to have been taken with the direction of the arrows.",Constraint,4
1942,"(Thus, you should give three estimates in all—low, median, and high—for each question.) a.",Constraint,4
1943,"But if you’re like most people, you will be more sure of yourself than you should be, and fewer than half the answers will fall within the bounds.",Safety,4
1944,"Year in which Roger Williams founded Providence, Rhode Island.",Constraint,4
1945,16.16 Modify and extend the Bayesian network code in the code repository to provide for creation and evaluation of decision networks and the calculation of information value.,Interface,4
1946,"Provide reasonable variable domains, probabilities, and utilities for the network, assum- ing that there are three possible sites.",Interface,4
1947,"We will assume that the buyer is deciding whether to buy car c1, that there is time to carry out at most one test, and that t1 is the test of c1 and costs $50.",Safety,4
1948,"Car c1 costs $1,500, and its market value is $2,000 if it is in good shape; if not, $700 in repairs will be needed to make it in good shape.",Constraint,4
1949,Tests can be described by the probability that the car will pass or fail the test given that the car is in good or bad shape.,Safety,4
1950,"We have the following information: P (pass(c1, t1)|q+(c1)) = 0.8 P (pass(c1, t1)|q−(c1)) = 0.35 Use Bayes’ theorem to calculate the probability that the car will pass (or fail) its test and hence the probability that it is in good (or bad) shape given each possible test outcome.",Safety,4
1951,"Section 17.4 extends these ideas to the case of partially observable environments, and Section 17.4.3 develops a complete design for decision-theoretic agents in partially observable environments, combining dynamic Bayesian networks from Chapter 15 with decision networks from Chapter 16.",Constraint,4
1952,Section 17.6 looks at how multiagent systems can be designed so that multiple agents can achieve a common goal.,Constraint,1
1953,"Beginning in the start state, it must choose an action at each time step.",Interface,4
1954,"We will assume that transitions are Markovian in the sense of Chapter 15, that is, the probability of reaching s′ from s depends only on s and not on the history of earlier states.",Safety,4
1955,"Later, in Section 17.4.3, we will see that the transition model can be represented as a dynamic Bayesian network, just as in Chapter 15.",Safety,4
1956,"To complete the deﬁnition of the task environment, we must specify the utility function for the agent.",Functional,3
1957,"Because the decision problem is sequential, the utility function will depend on a sequence of states—an environment history—rather than on a single state.",Functional,3
1958,"Later in this section, we investigate how such utility functions can be speciﬁed in general; for now, we simply stipulate that in each state s, the agent receives a reward R(s), which may be positive or negative, but must be bounded.",Safety,3
1959,"For example, if the agent reaches the +1 state after 10 steps, its total utility will be 0.6.",Safety,4
1960,"Therefore, a solution must specify what the agent should do for any state that the agent might reach.",Safety,4
1961,"If the agent has a complete policy, then no matter what the outcome of any action, the agent will always know what to do next.",Safety,4
1962,"When −0.4278 ≤ R(s) ≤ −0.0850, life is quite unpleasant; the agent takes the shortest route to the +1 state and is willing to risk falling into the –1 state by acci- dent.",Safety,4
1963,"First, however, we must complete our investigation of utilities and policies for sequential decision problems.",Constraint,4
1964,"Then, to have any chance of reaching the +1 state, the agent must head directly for it, and the optimal action is to go Up.",Safety,4
1965,"(We will see later that for partially ob- servable environments, the inﬁnite-horizon case is not so simple.) Note that “inﬁnite horizon” does not necessarily mean that all state sequences are inﬁnite; it just means that there is no ﬁxed deadline.",Safety,4
1966,STATIONARY PREFERENCE The next question we must decide is how to calculate the utility of state sequences.,Safety,4
1967,"To obtain a simple expression in terms of the attributes, we will need to make some sort of preference-independence assumption.",Safety,4
1968,".] begin with the same state (i.e., s0 = s′ 0), then the two sequences should be preference-ordered the same way as the sequences [s1, s2, .",Constraint,4
1969,"In English, this means that if you prefer one future to another starting tomorrow, then you should still prefer that future if it were to start today instead.",Constraint,4
1970,"For reasons that will shortly become clear, we assume discounted rewards in the remainder of the chapter, although sometimes we allow γ = 1.",Safety,4
1971,"Lurking beneath our choice of inﬁnite horizons is a problem: if the environment does not contain a terminal state, or if the agent never reaches one, then all environment histories will be inﬁnitely long, and utilities with additive, undiscounted rewards will generally be 650 Chapter 17.",Constraint,4
1972,"If the environment contains terminal states and if the agent is guaranteed to get to one eventually, then we will never need to compare inﬁnite sequences.",Safety,4
1973,"The existence of improper policies can cause the standard algorithms for solving MDPs to fail with additive rewards, and so provides a good reason for using discounted rewards.",Constraint,3
1974,"Then a policy that does its best to stay in (1,1) will have higher average reward than one that stays elsewhere.",Constraint,4
1975,"Now, out of all the policies the agent could choose to execute starting in s, one (or more) will have higher expected utilities than all the others.",Safety,4
1976,"Notice that the utilities are higher for states closer to the +1 exit, because fewer steps are required to reach the exit.",Safety,4
1977,"Whereas systems of linear equations can be solved quickly using linear algebra techniques, systems of nonlinear equations are more problematic.",Constraint,1
1978,"(b) The number of value iterations k required to guarantee an error of at most ǫ = c · Rmax, for different values of c, as a function of the discount factor γ.",Functional,3
1979,"If we apply the Bellman update inﬁnitely often, we are guaranteed to reach an equilibrium (see Section 17.2.3), in which case the ﬁnal utility values must be solutions to the Bellman equations.",Interface,4
1980,"• When the function is applied to any argument, the value must get closer to the ﬁxed point (because the ﬁxed point does not move), so repeated application of a contraction always reaches the ﬁxed point in the limit.",Functional,1
1981,"We will use the max norm, which measures the “length” of a vector by the absolute value of its biggest component: ||U || = max s |U (s)| .",Safety,4
1982,"We can calculate the number of iterations required to reach a speciﬁed error bound ǫ as follows: First, recall from Equation (17.1) that the utilities of all states are bounded by ±Rmax/(1 − γ).",Safety,4
1983,"Then, because the error is reduced by at least γ each time, we require γN · 2Rmax/(1 − γ) ≤ ǫ.",Safety,4
1984,"What the agent really cares about, however, is how well it will do if it makes its decisions on the basis of this utility function.",Functional,3
1985,Will the resulting behavior be nearly as good as the optimal behavior?,Safety,4
1986,"At this point, we know that the utility function Ui is a ﬁxed point of the Bellman update, so it is a solution to the Bellman equations, and πi must be an optimal policy.",Functional,3
1987,"Because there are only ﬁnitely many policies for a ﬁnite state space, and each iteration can be shown to yield a better policy, policy iteration must terminate.",Constraint,4
1988,"The policy improvement step is obviously straightforward, but how do we implement the POLICY-EVALUATION routine?",Safety,4
1989,Making Complex Decisions ASYNCHRONOUS POLICY ITERATION The algorithms we have described so far require updating the utility or policy for all states at once.,Safety,3
1990,"The freedom to choose any states to work on means that we can design much more efﬁcient heuristic algorithms—for example, algorithms that concentrate on updating the values of states that are likely to be reached by a good policy.",Safety,3
1991,"This makes a lot of sense in real life: if one has no intention of throwing oneself off a cliff, one should not spend time worrying about the exact value of the resulting states.",Safety,4
1992,"17.4.1 Deﬁnition of POMDPs To get a handle on POMDPs, we must ﬁrst deﬁne them properly.",Constraint,4
1993,The fundamental insight required to understand POMDPs is this: the optimal action depends only on the agent’s current belief state.,Safety,4
1994,"Now, if we knew the action and the subsequent percept, then Equation (17.11) would provide a deterministic update to the belief state: b′ = FORWARD(b, a, e).",Interface,4
1995,"The next two subsec- tions describe a value iteration algorithm designed speciﬁcally for POMDPs and an online decision-making algorithm, similar to those developed for games in Chapter 5.",Interface,3
1996,"At any given belief state b, the optimal policy will choose to execute the conditional plan with highest expected utility; and the expected utility of b under the optimal policy is just the utility of that conditional plan: U (b) = U π∗ (b) = max p b · αp .",Constraint,4
1997,"If the optimal policy π∗ chooses to execute p starting at b, then it is reasonable to expect that it might choose to execute p in belief states that are very close to b; in fact, if we bound the depth of the conditional plans, then there are only ﬁnitely many such plans and the continuous space of belief states will generally be divided into regions, each corresponding to a particular conditional plan that is optimal in that region.",Safety,4
1998,"From these two observations, we see that the utility function U (b) on belief states, being the maximum of a collection of hyperplanes, will be piecewise linear and convex.",Functional,3
1999,For now we will assume the discount factor γ = 1.,Safety,4
2000,"Obviously, the agent should Stay when it thinks it’s in state 1 and Go when it thinks it’s in state 0.",Safety,4
2001,"The advantage of a two-state world is that the belief space can be viewed as one- In Figure 17.8(a), the x-axis dimensional, because the two probabilities must sum to 1.",Safety,4
2002,The Figure 17.9 REMOVE-DOMINATED-PLANS step and MAX-DIFFERENCE test are typically implemented as linear programs.,Safety,4
2003,"The current time is t and the agent must decide what to do—that is, choose a value for At.",Safety,4
2004,"17.4.3 Online agents for POMDPs In this section, we outline a simple approach to agent design for partially observable, stochas- tic environments.",Constraint,4
2005,"The basic elements of the design are already familiar: • The transition and sensor models are represented by a dynamic Bayesian network (DBN), as described in Chapter 15.",Safety,3
2006,The agent design is therefore a practical implementation of the utility-based agent sketched in Chapter 2.,Safety,4
2007,"We will use At to refer to the action at time t, so the transition model becomes P(Xt+1|Xt, At) and the sensor model becomes P(Et|Xt).",Safety,3
2008,We will use Rt to refer to the reward received at time t and Ut to refer to the utility of the state at time t.,Safety,4
2009,10 4 6 3 Figure 17.11 will be taken in the belief state indicated.,Safety,4
2010,"This is because the agent must maximize the (discounted) sum of all future rewards, and U (Xt+3) represents the reward for Xt+3 and all subsequent rewards.",Safety,4
2011,"In this way, the algorithm takes into account the fact that, for decision At+i, the agent will have available percepts Et+1, .",Safety,3
2012,", Et+i, even though at time t it does not know what those percepts will be.",Constraint,4
2013,"In this way, a decision-theoretic agent automatically takes into account the value of information and will execute information-gathering actions where appropriate.",Safety,4
2014,"Making Complex Decisions plans generated by value iteration.) For problems in which the discount factor γ is not too close to 1, a shallow search is often good enough to give near-optimal decisions.",Safety,4
2015,"Decision-theoretic agents based on dynamic decision networks have a number of advan- tages compared with other, simpler agent designs presented in earlier chapters.",Constraint,4
2016,Agent design: Game theory can analyze the agent’s decisions and compute the expected utility for each decision (under the assumption that other agents are acting optimally according to game theory).,Safety,4
2017,"Mechanism design: When an environment is inhabited by many agents, it might be possible to deﬁne the rules of the environment (i.e., the game that the agents must play) so that the collective good of all agents is maximized when each agent adopts the game-theoretic solution that maximizes its own utility.",Interface,4
2018,"For example, game theory can help design the protocols for a collection of Internet trafﬁc routers so that each router has an incentive to act in such a way that global throughput is maximized.",Interface,4
2019,Mechanism design can also be used to construct intelligent multiagent systems that solve complex problems in a distributed fashion.,Constraint,1
2020,A single-move game is deﬁned by three components: • Players or agents who will be making decisions.,Constraint,4
2021,"We will give actions lowercase names, like one or testify.",Constraint,4
2022,STRATEGY PURE STRATEGY MIXED STRATEGY STRATEGY PROFILE OUTCOME Each player in a game must adopt and then execute a strategy (which is the name used in game theory for a policy).,Safety,4
2023,We will see that the most important issue in game theory is to deﬁne what “rational” means when each agent chooses only part of the strategy proﬁle that determines the outcome.,Safety,4
2024,We will see that some games have a solution only in mixed strategies.,Constraint,4
2025,But that does not mean that a player must literally be adopting a mixed strategy to be rational.,Constraint,4
2026,"A prosecutor offers each a deal: if you testify against your partner as the leader of a burglary ring, you’ll go free for being the cooperative one, while your partner will serve 10 years in prison.",Safety,4
2027,Alice and Bob also know that if both refuse to testify they will serve only 1 year each for the lesser charge of possessing stolen property.,Safety,4
2028,Now Alice and Bob face the so-called prisoner’s dilemma: should they testify or refuse?,Safety,4
2029,"Let’s assume that Alice is callously unconcerned about her partner’s fate, so her utility decreases in proportion to the number of years she will spend in prison, regardless of what happens to Bob.",Safety,4
2030,"So in either case, it’s better for me to testify, so that’s what I must do.” Alice has discovered that testify is a dominant strategy for the game.",Safety,4
2031,"If Alice is clever as well as rational, she will continue to reason as follows: Bob’s dominant strategy is also to testify.",Constraint,4
2032,"Therefore, he will testify and we will both get ﬁve years.",Constraint,4
2033,Either player contemplating playing refuse will realize that he or she would do better by playing testify.,Constraint,4
2034,"For example, we could change to a repeated game in which the players know that they will meet again.",Performance,4
2035,"We will see later that agents with limited computational powers, rather than the ability to reason absolutely rationally, can reach non-equilibrium outcomes, as can an agent that knows that the other agent has limited rationality.",Constraint,4
2036,"Acme, a video game console manufacturer, has to decide whether its next game machine will use Blu-ray discs or DVDs.",Constraint,4
2037,"Meanwhile, the video game software producer Best needs to decide whether to produce its next game on Blu-ray or DVD.",Safety,4
2038,"The proﬁts for both will be positive if they agree and negative if they disagree, as shown in the following payoff matrix: Best:bluray Best:dvd Acme:bluray A = +9, B = +9 A = −3, B = −1 Acme:dvd A = −4, B = −1 A = +5, B = +5 There is no dominant strategy equilibrium for this game, but there are two Nash equilibria: (bluray, bluray) and (dvd, dvd).",Safety,4
2039,"We know these are Nash equilibria because if either player unilaterally moves to a different strategy, that player will be worse off.",Constraint,4
2040,"Now the agents have a problem: there are multiple acceptable solutions, but if each agent aims for a different solution, then both agents will suffer.",Safety,4
2041,"One answer is that both should choose the Pareto-optimal solution (bluray, bluray); that is, we can restrict the deﬁnition of “solution” to the unique Pareto-optimal Nash equilibrium provided that one exists.",Safety,4
2042,A game can have more than one Nash equilibrium; how do we know that every game must have at least one?,Constraint,4
2043,"If the total number of ﬁngers is even, then O will want to switch; on the other hand (so to speak), if the total is odd, then E will want to switch.",Safety,4
2044,"Therefore, no pure strategy proﬁle can be an equilibrium and we must look to mixed strategies instead.",Interface,4
2045,"For two-player, zero-sum games, we know that the payoffs are equal and opposite, so we need consider the payoffs of only one player, who will be the maximizer (just as in Chapter 5).",Safety,4
2046,"Combining these two arguments, we see that the true utility U of the solution to the original game must satisfy UE,O ≤ U ≤ UO,E or in this case, − 3 ≤ U ≤ 2 .",Safety,4
2047,"(e) and (f): For any particular value of the probability parameter, the second player will choose the “better” of the two actions, so the value of the ﬁrst player’s mixed strategy is given by the heavy lines.",Safety,4
2048,The ﬁrst player will choose the probability parameter for the mixed strategy at the intersection point.,Safety,4
2049,"O, the minimizer, will always choose the lower of the two lines, as shown by the heavy lines in the ﬁgure.",Safety,4
2050,"(The moral is that it is better to be O than E if you are playing this game.) Furthermore, the true utility is attained by the mixed strategy [7/12: one; 5/12: two], which should be played by both players.",Safety,4
2051,"The question remains, what should a rational agent actually do in playing a single game of Morra?",Safety,4
2052,"The rational agent will have derived the fact that [7/12: one; 5/12: two] is the maximin equilibrium strategy, and will assume that this is mutual knowledge with a rational opponent.",Constraint,4
2053,"Will Alice and Bob work together and refuse to testify, knowing they will meet again?",Performance,4
2054,"For example, suppose Alice and Bob know that they must play exactly 100 rounds of prisoner’s dilemma.",Constraint,4
2055,"Then they both know that the 100th round will not be a repeated game—that is, its outcome can have no effect on future rounds—and therefore they will both choose the dominant strategy, testify, in that round.",Safety,4
2056,"But once the 100th round is determined, the 99th round can have no effect on subsequent rounds, so it too will have a dominant strategy equilibrium at (testify, testify).",Safety,4
2057,"By induction, both players will choose testify on every round, earning a total jail sentence of 500 years each.",Constraint,4
2058,"For example, suppose that after each round there is a 99% chance that the players will meet again.",Performance,4
2059,Making Complex Decisions will be the last.,Safety,4
2060,"A player who deviates from the strategy and chooses testify will gain a score of 0 rather than −1 on the very next move, but from then on both players will play testify and the player’s total expected future payoff becomes 0 + ∞ X t=1 0.99t · (−5) = −495 .",Safety,4
2061,"Perpetual punishment is the “mutually assured destruction” strategy of the prisoner’s dilemma: once either player decides to testify, it ensures that both players suffer a great deal.",Safety,4
2062,"The agents are thus incapable of representing the number of remaining steps, and must treat it as an unknown.",Interface,4
2063,"To represent simultaneous moves, as in the prisoner’s dilemma or two-ﬁnger Morra, we impose an arbitrary order on the players, but we have the option of asserting that the earlier player’s actions are not observable to the subsequent players: e.g., Alice must choose refuse or testify ﬁrst, then Bob chooses, but Bob does not know what choice Alice made at that time (we can also represent the fact that the move is revealed later).",Interface,4
2064,"But if a player has I information sets and a actions per set, then that player will have aI pure strategies.",Constraint,4
2065,"The resulting system can solve poker variants with 25,000 states in a minute or two.",Constraint,1
2066,"For example, if I hold an ace and am considering the possibility that the next card will give me a pair of aces, then I don’t care about the suit of the next card; any suit will do equally well.",Interface,4
2067,The resulting game tree will be smaller by a factor of 4!,Safety,4
2068,Suppose I can solve this smaller game; how will the solution to that game relate to the original game?,Safety,4
2069,"If no player is going for a ﬂush (or blufﬁng so), then the suits don’t matter to any player, and the solution for the abstraction will also be a solution for the original game.",Safety,4
2070,"However, if any player is contemplating a ﬂush, then the abstraction will be only an approximate solution (but it is possible to compute bounds on the error).",Interface,4
2071,"Once you add opponent modeling to it, it will kill everyone.” However, good models of human fallability remain elusive.",Constraint,4
2072,"Parts of the game may be speciﬁed as unobservable to some of the players, but it must be known what parts are unobservable.",Safety,4
2073,Part of the problem is anticipating what action the virus writers will try next.,Safety,4
2074,"For example, how do I know if it is rational to order the Chef’s salad if I don’t know how much I will like it?",Safety,4
2075,Next we shall see how it can help design environments.,Safety,4
2076,"Mechanism Design 17.6 MECHANISM DESIGN 679 In the previous section, we asked, “Given a game, what is a rational strategy?” In this sec- tion, we ask, “Given that agents pick rational strategies, what game should we design?” More speciﬁcally, we would like to design a game whose solutions, consisting of each agent pursu- ing its own rational strategy, result in the maximization of some global utility function.",Functional,3
2077,"This problem is called mechanism design, or sometimes inverse game theory.",Constraint,4
2078,"Capitalism 101 says that if everyone tries to get rich, the total wealth of society will increase.",Safety,4
2079,But the examples we will discuss show that proper mechanism design is necessary to keep the invisible hand on track.,Safety,4
2080,"For collections of agents, mechanism design allows us to construct smart systems out of a collection of more limited systems—even uncooperative systems—in much the same way that teams of humans can achieve goals beyond the reach of any individual.",Safety,1
2081,"Examples of mechanism design include auctioning off cheap airline tickets, routing TCP packets between computers, deciding how medical interns will be assigned to hospitals, and deciding how robotic soccer players will cooperate with their teammates.",Constraint,4
2082,"Mechanism design became more than an academic subject in the 1990s when several nations, faced with the problem of auctioning off licenses to broadcast in various frequency bands, lost hundreds of millions of dollars in potential revenue as a result of poor mechanism design.",Constraint,4
2083,"In other cases, such as auctioning drilling rights for an oil tract, the item has a common value—the tract will produce some amount of money, X, and all bidders value a dollar equally—but there is uncertainty as to what the actual value of X is.",Interface,4
2084,"The highest bid, bmax wins the item, but the price paid need not be bmax; that’s part of the mechanism design.",Safety,4
2085,MECHANISM DESIGN MECHANISM CENTER AUCTION ASCENDING BID ENGLISH AUCTION 680 Chapter 17.,Constraint,4
2086,"Making Complex Decisions willing to pay that amount, the center then asks for bmin + d, for some increment d, and continues up from there.",Safety,4
2087,"The auction ends when nobody is willing to bid anymore; then the last bidder wins the item, paying the price he bid.",Safety,4
2088,"These goals overlap to some extent, because one aspect of maximizing global utility is to ensure that the winner of the auction is the agent who values the item the most (and thus is willing to pay the most).",Safety,4
2089,"For example, in 1999, Germany auctioned ten blocks of cell-phone spectrum with a simultaneous auction (bids were taken on all ten blocks at the same time), using the rule that any bid must be a minimum of a 10% raise over the previous bid on a block.",Constraint,4
2090,"Perhaps the 10% rule was an error in mechanism design, because it facilitated the precise signaling from Mannesman to T-Mobile.",Interface,4
2091,"After all, if it requires too much research or computation on the part of the bidders, they may decide to take their money elsewhere.",Safety,4
2092,"Mechanism Design 681 nism can be transformed into an equivalent truth-revealing mechanism, so part of mechanism design is ﬁnding these equivalent mechanisms.",Interface,4
2093,"An alternative mechanism, which requires much less communication, is the sealed- bid auction.",Interface,4
2094,"If your value is vi and you believe that the maximum of all the other agents’ bids will be bo, then you should bid bo + ǫ, for some small ǫ, if that is less than vi.",Safety,4
2095,"This simple modiﬁ- cation completely eliminates the complex deliberations required for standard (or ﬁrst-price) sealed-bid auctions, because the dominant strategy is now simply to bid vi; the mechanism is truth-revealing.",Safety,4
2096,"10 Named after William Vickrey (1914–1996), who won the 1996 Nobel Prize in economics for this work and died of a heart attack three days later.",Safety,4
2097,"Because of its simplicity and the minimal computation requirements for both seller and bidders, the Vickrey auction is widely used in constructing distributed AI systems.",Safety,1
2098,"This is actually a very general result: the revenue equivalence theorem states that, with a few minor caveats, any auction mechanism where risk-neutral bidders have values vi known only to themselves (but know a probability distribution from which those values are sampled), will yield the same expected revenue.",Constraint,4
2099,"In general, bidders in this multislot auction must spend a lot of energy analyzing the bids of others to determine their best strategy; there is no simple dominant strategy.",Safety,4
2100,"Mechanism Design 683 17.6.2 Common goods Now let’s consider another type of game, in which countries set their policy for controlling air pollution.",Constraint,4
2101,"Each country has a choice: they can reduce pollution at a cost of -10 points for implementing the necessary changes, or they can continue to pollute, which gives them a net utility of -5 (in added health costs, etc.) and also contributes -1 points to every other country (because the air is shared across countries).",Safety,4
2102,"More generally, we need to ensure that all externalities—effects on global utility that are not recognized in the in- dividual agents’ transactions—are made explicit.",Safety,4
2103,"In the limit, this approach amounts to creating a mechanism in which each agent is effectively required to maximize global utility, but can do so by making a local decision.",Safety,4
2104,"For this example, a carbon tax would be an example of a mechanism that charges for use of the commons in a way that, if implemented well, maximizes global utility.",Interface,4
2105,"All winners should be happy because they pay a tax that is less than their value, and all losers are as happy as they can be, because they value the goods less than the required tax.",Safety,4
2106,"Agent i knows that the center will maximize global utility using the reported values, bj(A) bj(A) = bi(A) + X j6=i X j whereas agent i wants the center to maximize (17.14), which can be rewritten as vi(A) + X j6=i bj(A) − W−i .",Safety,4
2107,"• Mechanism design can be used to set the rules by which agents will interact, in order to maximize some global utility through the operation of individually rational agents.",Safety,4
2108,"We shall return to the world of MDPs and POMDP in Chapter 21, when we study rein- forcement learning methods that allow an agent to improve its behavior from experience in sequential, uncertain environments.",Interface,3
2109,"Asynchronous policy iteration was analyzed by Williams and Baird (1993), who also proved the policy loss bound in Equation (17.9).",Safety,4
2110,"The texts by Bertsekas (1987), Puterman (1994), and Bertsekas and Tsitsiklis (1996) provide a rigorous introduction to sequential decision problems.",Interface,4
2111,"(Earlier work by Werbos (1977) contained many similar ideas, but was not taken up to the same extent.) The connection between MDPs and AI planning problems was made ﬁrst by Sven Koenig (1991), who showed how probabilistic STRIPS operators provide a compact representation for transition models (see also Wellman, 686 Chapter 17.",Interface,4
2112,Heuristics based on the value of information can be used to select areas of the state space where a local expansion of the horizon will yield a signiﬁcant improvement in decision qual- ity.,Safety,4
2113,"Because POMDPs are PSPACE-hard (Papadimitriou and Tsitsiklis, 1987), further progress may require taking advantage of various kinds of structure within a factored representation.",Constraint,4
2114,(1996); the paper by Koller and Pfeffer (1997) provides a readable introduction to the ﬁeld and describe a working system for representing and solving sequential games.,Safety,1
2115,"(2009) show that the abstraction approach is vulnerable to making systematic errors in approximating the equilibrium solution, meaning that the whole approach is on shaky ground: it works for some games but not others.",Safety,1
2116,"Making Complex Decisions tionary game theory (Smith, 1982; Weibull, 1995) looks at strategy drift over time: if your opponent’s strategy is changing, how should you react?",Constraint,4
2117,"The 2007 Nobel Memorial Prize in Economics went to Hurwicz, Maskin, and Myerson “for having laid the foundations of mechanism design theory” (Hurwicz, 1973).",Safety,4
2118,"Mechanism design is used in multiagent planning (Hunsberger and Grosz, 2000; Stone et al., 2009) and scheduling (Rassenti et al., 1982).",Constraint,4
2119,You will need a way to calcu- late the optimal policy and its value for ﬁxed R(s).,Safety,4
2120,"You will ﬁnd it helpful to arrange the states (sA, sB) on a two-dimensional grid, using sA and sB as “coordinates.” d.",Safety,4
2121,Implement value iteration for this world for each value of r below.,Constraint,4
2122,"Assuming a discounted reward function, for what values of the discount γ should the agent choose Up and for which Down?",Functional,3
2123,"(Note that this simple example actually reﬂects many real-world situations in which one must weigh the value of an immediate action versus the potential continual long-term consequences, such as choosing to dump pollutants into a lake.) 17.10 Consider an undiscounted MDP having three states, (1, 2, 3), with rewards −1, −2, 0, respectively.",Interface,4
2124,"Implement an environment simulator for this environment, such that the speciﬁc geog- raphy of the environment is easily altered.",Interface,4
2125,"17.15 Consider a version of the two-state POMDP on page 661 in which the sensor is 90% reliable in state 0 but provides no information in state 1 (that is, it reports 0 or 1 with equal probability).",Constraint,3
2126,Suppose both players choose the perpetual punishment strategy (where each will choose refuse unless the other player has ever played testify).,Safety,4
2127,"17.19 A Dutch auction is similar in an English auction, but rather than starting the bidding at a low price and increasing, in a Dutch auction the seller starts at a high price and gradually lowers the price until some buyer is willing to accept that price.",Interface,4
2128,"Assuming all bidders act rationally, is it true that for arbitrarily small d, a Dutch auction will always result in the bidder with the highest value for the item obtaining the item?",Constraint,4
2129,"So in 1999 the ofﬁcials experimented in mechanism design: the rules were changed, giving a team that loses in overtime 1 point, not 0.",Safety,4
2130,"Imagine that it were legal and ethical for the two teams to enter into a pact where they agree that they will skate to a tie in regulation time, and then both try in earnest to win in overtime.",Safety,4
2131,"Doing this amounts to saying that before you can learn something new, you must ﬁrst forget (almost) everything you know.",Constraint,4
2132,"Example descriptions and classiﬁcations will also be logical sentences, and a new example can be classiﬁed by inferring a classiﬁcation sentence from the hypothesis and the example description.",Safety,4
2133,It enables us to go well beyond the simple learning methods of Chapter 18 by using the full power of logical inference in the service of learning.,Safety,2
2134,"We will use the notation Di(Xi) to refer to the description of Xi, where Di can be any logical expression taking a single argument.",Safety,4
2135,"The classiﬁcation of the example is given by a literal using the goal predicate, in this case WillWait(X1) or ¬WillWait(X1) .",Safety,4
2136,"Here we are concerned with hypotheses expressed in logic; each hypothesis hj will have the form ∀ x Goal (x) ⇔ Cj(x) , where Cj(x) is a candidate deﬁnition—some expression involving the attribute predicates.",Constraint,4
2137,"Thus, the tree in Figure 18.6 (page 702) expresses the following logical deﬁnition (which we will call hr for future reference): ∀ r WillWait(r) ⇔ Patrons (r, Some) ∨ Patrons (r, Full ) ∧ Hungry(r) ∧ Type(r, French) ∨ Patrons (r, Full ) ∧ Hungry(r) ∧ Type(r, Thai ) (19.1) ∧ Fri /Sat (r) ∨ Patrons (r, Full ) ∧ Hungry(r) ∧ Type(r, Burger ) .",Safety,4
2138,"Each hypothesis predicts that a certain set of examples—namely, those that satisfy its candi- date deﬁnition—will be examples of the goal predicate.",Safety,4
2139,", hn} that the learning algo- rithm is designed to entertain.",Safety,4
2140,"For example, the D ECISION-TREE-LEARNING algorithm can entertain any decision tree hypothesis deﬁned in terms of the attributes provided; its hypoth- esis space therefore consists of all these decision trees.",Safety,3
2141,"Knowledge in Learning • An example can be a false negative for the hypothesis, if the hypothesis says it should be negative but in fact it is positive.",Interface,4
2142,∧ WillWait(X13) would be a false negative for the hypothesis hr given earlier.,Safety,4
2143,"From hr and the example description, we can deduce both WillWait(X13), which is what the example says, and ¬WillWait(X13), which is what the hypothesis predicts.",Safety,4
2144,"FALSE POSITIVE • An example can be a false positive for the hypothesis, if the hypothesis says it should be positive but in fact it is negative.1 If an example is a false positive or false negative for a hypothesis, then the example and the hypothesis are logically inconsistent with each other.",Interface,4
2145,"An ordinary logical inference system therefore could, in principle, learn from the example by eliminating one or more hypotheses.",Interface,1
2146,"Then if I1 is inconsistent with h2 and h3, the logical inference system can deduce the new hypothesis space h1 ∨ h4.",Constraint,1
2147,"Because the hypothesis space is usually vast (or even inﬁnite in the case of ﬁrst-order logic), we do not recommend trying to build a learning system using resolution- based theorem proving and a complete enumeration of the hypothesis space.",Safety,1
2148,"Instead, we will describe two approaches that ﬁnd logically consistent hypotheses with much less effort.",Constraint,4
2149,"The examples that have actually been seen so far are shown as “+” or “–”, and we see that hr correctly categorizes all the examples as positive or negative examples of WillWait.",Safety,4
2150,"In Figure 19.1(b), a new example (circled) is a false negative: the hypothesis says it should be negative but it is actually positive.",Safety,4
2151,The extension of the hypothesis must be increased to include it.,Safety,4
2152,"Then in Figure 19.1(d), we see a false positive: the hypothesis says the new example (circled) should be positive, but it actually is 1 The terms “false positive” and “false negative” are used in medicine to describe erroneous results from lab tests.",Safety,4
2153,"To start the algorithm, any hypothesis can be passed in; it will be specialized or gneralized as needed.",Safety,3
2154,The extension of the hypothesis must be decreased to exclude the example.,Safety,4
2155,The “more general than” and “more speciﬁc than” relations between hypotheses provide the logical structure on the hypothesis space that makes efﬁcient search possible.,Interface,4
2156,"Notice that each time we consider generalizing or specializing the hypothesis, we must check for consistency with the other examples, because an arbitrary increase/decrease in the exten- sion might include/exclude previously seen negative/positive examples.",Constraint,4
2157,"Now we need to determine exactly how they can be implemented as syntactic operations that change the candidate deﬁnition associated with the hypothesis, so that a program can carry them out.",Constraint,4
2158,"If hypothesis h1, with deﬁnition C1, is a generalization of hypothesis h2 with deﬁnition C2, then we must have ∀ x C2(x) ⇒ C1(x) .",Constraint,4
2159,"The attribute Alternate(X1) is true, so let the initial hypothesis be h1 : ∀ x WillWait(x) ⇔ Alternate(x) .",Safety,4
2160,"This can be done by adding an extra condition that will rule out X2, while continuing to classify X1 as positive.",Interface,4
2161,"One possibility is h2 : ∀ x WillWait(x) ⇔ Alternate(x) ∧ Patrons (x, Some) .",Constraint,4
2162,"We drop the Alternate condition, yielding h3 : ∀ x WillWait(x) ⇔ Patrons (x, Some) .",Safety,4
2163,"One possibility is to add a disjunct: h4 : ∀ x WillWait(x) ⇔ Patrons (x, Some) ∨ (Patrons (x, Full ) ∧ Fri /Sat (x)) .",Constraint,4
2164,"Obviously, there are other possibilities consistent with the ﬁrst four examples; here are two of them: h′ 4 : ∀ x WillWait(x) ⇔ ¬WaitEstimate(x, 30-60) .",Constraint,4
2165,"h′′ 4 : ∀ x WillWait(x) ⇔ Patrons(x, Some) ∨ (Patrons(x, Full ) ∧ WaitEstimate(x, 10-30)) .",Constraint,4
2166,"It ﬁnds a subset of V that is consistent choices that are made will not necessarily lead to the simplest hypothesis, and may lead to an unrecoverable situation where no simple modiﬁcation of the hypothesis is consistent with all of the data.",Constraint,4
2167,"In such cases, the program must backtrack to a previous choice point.",Safety,4
2168,"The CURRENT-BEST-LEARNING algorithm and its variants have been used in many machine learning systems, starting with Patrick Winston’s (1970) “arch-learning” program.",Constraint,1
2169,Each new example will either have no effect or will get rid of some of the hypotheses.,Safety,4
2170,"Assuming that the original hypothesis space does in fact contain the right answer, the reduced disjunction must still contain the right an- swer because only incorrect hypotheses have been removed.",Interface,4
2171,"This is a partial ordering, which means that each boundary will not be a point but rather a set of hypotheses called a boundary set.",Constraint,4
2172,"(That is, there are no “holes” be- tween the boundaries.) Any h between S and G must reject all the negative examples rejected by each member of G (because it is more speciﬁc), and must accept all the pos- itive examples accepted by any member of S (because it is more general).",Safety,4
2173,"Thus, h must agree with all the examples, and therefore cannot be inconsistent.",Constraint,4
2174,"Figure 19.5 shows the situation: there are no known examples outside S but inside G, so any hypothesis in the gap must be consistent.",Safety,4
2175,"We have therefore shown that if S and G are maintained according to their deﬁnitions, then they provide a satisfactory representation of the version space.",Interface,4
2176,"False negative for Si: This means Si is too speciﬁc, so we replace it by all its immediate generalizations, provided they are more speciﬁc than some member of G.",Constraint,4
2177,"False positive for Gi: This means Gi is too general, so we replace it by all its immediate specializations, provided they are more general than some member of S.",Constraint,4
2178,"There are two principal drawbacks to the version-space approach: • If the domain contains noise or insufﬁcient attributes for exact classiﬁcation, the version space will always collapse.",Safety,4
2179,"• If we allow unlimited disjunction in the hypothesis space, the S-set will always contain a single most-speciﬁc hypothesis, namely, the disjunction of the descriptions of the positive examples seen to date.",Safety,4
2180,"Similarly, the G-set will contain just the negation of the disjunction of the descriptions of the negative examples.",Safety,4
2181,"The pure version space algorithm was ﬁrst applied in the Meta-DENDRAL system, which was designed to learn rules for predicting how molecules would break into pieces in a mass spectrometer (Buchanan and Mitchell, 1978).",Safety,1
2182,"It was also used in the elegant LEX system (Mitchell et al., 1983), which was able to learn to solve symbolic integra- tion problems by studying its own successes and failures.",Safety,1
2183,"Although version space methods are probably not practical in most real-world learning problems, mainly because of noise, they provide a good deal of insight into the logical structure of hypothesis space.",Interface,3
2184,Then a Hypothesis that “explains the observations” must satisfy the following property (recall that |= means “logically entails”): Hypothesis ∧ Descriptions |= Classiﬁcations .,Safety,4
2185,"For example, if we consider a decision tree as a logical formula (see Equation (19.1) on page 769), then a decision tree that is consistent with all the examples will satisfy Equation (19.3).",Constraint,4
2186,The modern approach is to design agents that already know something and are trying to learn some more.,Safety,4
2187,"This may not sound like a terriﬁcally deep insight, but it makes quite a difference to the way we design agents.",Safety,4
2188,"An autonomous learning agent that uses background knowledge must somehow obtain the background knowledge in the ﬁrst place, in order for it to be used in the new learning episodes.",Interface,4
2189,This method must itself be a learning process.,Constraint,3
2190,"The agent’s life history will there- fore be characterized by cumulative, or incremental, development.",Safety,4
2191,"But once it has eaten from the Tree of Knowledge, it can no longer pursue such naive speculations and should use its background knowledge to learn more and more effectively.",Safety,4
2192,We will now look at what kinds of entailment constraints are operat- ing in each case.,Constraint,4
2193,"The constraints will involve the Background knowledge, in addition to the Hypothesis and the observed Descriptions and Classiﬁcations .",Constraint,4
2194,"But because it requires that the background knowledge be sufﬁcient to explain the Hypothesis, which in turn explains the observations, the agent does not actually learn any- thing factually new from the example.",Safety,4
2195,"The agent could have derived the example from what it already knew, although that might have required an unreasonable amount of computation.",Interface,4
2196,"The student needs to propose another rule, namely, that M generally is effective against D.",Safety,4
2197,"As with pure inductive learning, the learning algorithm should propose hypotheses that are as simple as possible, consistent with this constraint.",Constraint,3
2198,"In ILP systems, prior knowl- edge plays two key roles in reducing the complexity of learning: KNOWLEDGE BASED INDUCTIVE LEARNING INDUCTIVE LOGIC PROGRAMMING 780 Chapter 19.",Safety,1
2199,"Because any hypothesis generated must be consistent with the prior knowledge as well as with the new observations, the effective hypothesis space size is reduced to include only those theories that are consistent with what is already known.",Constraint,4
2200,"For any given set of observations, the size of the hypothesis required to construct an explanation for the observations can be much reduced, because the prior knowledge will be available to help out the new rules in explaining the observations.",Interface,4
2201,"In addition to allowing the use of prior knowledge in induction, ILP systems can formulate hypotheses in general ﬁrst-order logic, rather than in the restricted attribute-based language of Chapter 18.",Safety,1
2202,This means that they can learn in environments that cannot be understood by simpler systems.,Constraint,1
2203,"(We use a capital letter for the arithmetic unknown X, to distinguish it from the logical variable x.) In a logical reasoning system, the goal might be expressed as ASK(Derivative(X 2, X) = d, KB), with solution d = 2X.",Constraint,1
2204,"A student encountering such problems for the ﬁrst time, or a program with no experience, will have a much more difﬁcult job.",Constraint,4
2205,"In the authors’ logic programming implementation, this takes 136 proof steps, of which 99 are on dead-end branches in the proof.",Safety,4
2206,This deﬁnition provides the basis for a rule covering all of the cases in the class.,Safety,4
2207,We will use for our reasoning system the simple backward-chaining theorem prover described in Chapter 9.,Safety,1
2208,"The proof tree for Derivative(X 2, X) = 2X is too large to use as an example, so we will use a simpler problem to illustrate the generalization method.",Interface,3
2209,"For example, in order to use the rule Rewrite(1 × u, u), the variable x in the subgoal Rewrite(x × (y + z), v) must be bound to 1.",Safety,4
2210,"Similarly, y must be bound to 0 in the subgoal Rewrite(y + z, v′) in order to use the rule Rewrite(0 + u, u).",Safety,4
2211,"In general, conditions can be dropped from the ﬁnal rule if they impose no constraints on the variables on the right-hand side of the rule, because the resulting rule will still be true and will be more efﬁcient.",Constraint,4
2212,"Values other than arithmetic unknowns might require different forms of simpliﬁcation: for example, if z were 2 × 3, then the correct simpliﬁcation of 1 × (0 + (2 × 3)) would be 6 and not 2 × 3.",Safety,4
2213,"Adding large numbers of rules can slow down the reasoning process, because the in- ference mechanism must still check those rules even in cases where they do not yield a solution.",Safety,4
2214,"To compensate for the slowdown in reasoning, the derived rules must offer signiﬁcant increases in speed for the cases that they do cover.",Safety,4
2215,"Derived rules should be as general as possible, so that they apply to the largest possible set of cases.",Safety,4
2216,"Thus, EBL systems really face a very complex optimization problem in trying to maximize the efﬁciency of a given initial knowledge base.",Safety,1
2217,"If the EBL system is carefully engineered, it is possible to obtain signiﬁcant speedups.",Safety,1
2218,"For example, a very large Prolog-based natural language system designed for speech-to-speech translation between Swedish and English was able to achieve real-time performance only by the appli- cation of EBL to the parsing process (Samuelsson and Rayner, 1991).",Performance,1
2219,"They occur so commonly in certain kinds of applications (e.g., deﬁning database designs) that a special syntax is used to write them.",Constraint,1
2220,"Intuitively, it is clear that a reduction in the hypothesis space size should make it eas- ier to learn the target predicate.",Safety,4
2221,"First, recall that for Boolean functions, log(|H|) examples are required to converge to a reasonable hypothesis, where |H| is the size of the hypothesis space.",Safety,3
2222,"If the determination contains d predicates in the left-hand side, the learner will require only O(2d) examples, a reduction of O(2n−d).",Safety,4
2223,"In order to provide a complete story of relevance-based learning, we must therefore provide a learning algorithm for determinations.",Interface,3
2224,"A determination P ≻ Q says that if any examples match on P , then they must also match on Q.",Constraint,4
2225,"We will assume a simple attribute-based representation, like that used for decision tree learning in Chapter 18.",Constraint,4
2226,"A determination d will be represented by the set of attributes on the left-hand side, because the target predicate is assumed to be ﬁxed.",Safety,4
2227,Then the algorithm will not ﬁnd it until searching the subsets of A of size p.,Safety,3
2228,"In most domains, however, there will be sufﬁcient local structure (see Chapter 14 for a deﬁnition of locally structured domains) that p will be small.",Constraint,4
2229,"We expect that RBDTL will learn faster than DECISION-TREE-LEARNING, and this is in fact the case.",Safety,4
2230,"Obviously, in cases where all the available attributes are relevant, RBDTL will show no advantage.",Safety,4
2231,"This means that inductive logic programming systems can participate in the scientiﬁc cycle of experimentation, hypothesis generation, debate, and refutation.",Safety,1
2232,"Such participation would not be possible for systems that generate “black-box” classiﬁers, such as neural networks.",Constraint,1
2233,"To illustrate this, we will use the problem of learning family relationships from examples.",Safety,4
2234,"The descriptions will consist of an extended family tree, described in terms of Mother , Father , and Married relations and Male and Female properties.",Interface,4
2235,"As an example, we will use the family tree from Exercise 8.15, shown here in Figure 19.11.",Interface,4
2236,"From these descriptions and from classiﬁcations such as Fold (FOUR-HELICAL-UP-AND-DOWN-BUNDLE, D2mhr ), the ILP system PROGOL (Muggleton, 1995) learned the following rule: Fold (FOUR-HELICAL-UP-AND-DOWN-BUNDLE, p) ⇐ in the domain of protein folding.",Safety,1
2237,"∨ ∨ ∨ Notice that an attribute-based learning algorithm, such as DECISION-TREE-LEARNING, will get nowhere in solving this problem.",Interface,3
2238,Attribute-based learning algorithms are incapable of learning relational predicates.,Constraint,3
2239,The reader will certainly have noticed that a little bit of background knowledge would help in the representation of the Grandparent deﬁnition.,Safety,4
2240,"Given the example data shown earlier, it is entirely reasonable for the ILP program to propose an additional predicate, which we would call George Mum Spencer Kydd Elizabeth Philip Margaret Diana Charles Anne Mark Andrew Sarah Edward Sophie William Harry Peter Zara Beatrice Eugenie Louise James Figure 19.11 A typical family tree.",Interface,4
2241,"It has been one of the hardest problems in machine learning, but some ILP techniques provide effective mechanisms for achieving it.",Interface,4
2242,"In the rest of this chapter, we will study the two principal approaches to ILP.",Safety,4
2243,"The clauses must classify the 12 positive examples as instances of the Grandfather (x, y) relationship, while ruling out the 388 negative examples.",Safety,4
2244,"This clause classiﬁes every example as positive, so it needs to be specialized.",Constraint,4
2245,"FOIL will ﬁnd and choose this literal, thereby solving the learning task.",Safety,4
2246,"Note that each of these clauses covers some of the positive examples, that together they cover all the positive examples, and that NEW-CLAUSE is designed in such a way that no clause will incorrectly cover a negative example.",Safety,4
2247,In general F OIL will have to search through many unsuccessful clauses before ﬁnding a correct solution.,Constraint,4
2248,"Literals using predicates: the literal can be negated or unnegated, any existing predicate (including the goal predicate) can be used, and the arguments must all be variables.",Safety,4
2249,"Any variable can be used for any argument of the predicate, with one restriction: each literal must include at least one variable from an earlier literal or from the head of the clause.",Constraint,4
2250,This technique provides a way to avoid overcomplex clauses that ﬁt noise in the data.,Safety,4
2251,"Inverse resolution is based on the observation that if the example Classiﬁcations follow from Background ∧ Hypothesis ∧ Descriptions, then one must be able to prove this fact by resolution (because resolution is complete).",Safety,4
2252,We will show a backward proof process for inverse resolution that consists of individual backward steps.,Constraint,4
2253,A number of approaches to taming the search have been tried in implemented ILP systems: 1.,Safety,1
2254,"The PROGOL system (Muggleton, 1995) uses a form of model checking to limit the search.",Safety,1
2255,"The LINUS system (Lavrauc and Duzeroski, 1994) works by translating ﬁrst-order the- ories into propositional logic, solving them with a propositional learning system, and then translating back.",Constraint,1
2256,Would an inverse reso- lution program be theoretically capable of inferring the law of gravity?,Interface,4
2257,One thing that inverse resolution systems will do for you is invent new predicates.,Constraint,1
2258,"Hence, by including the ability to invent new predicates, inverse resolution systems can often solve learning problems that are infeasible with other techniques.",Constraint,1
2259,"The difﬁcult part lies in realizing that some new entity, with a speciﬁc relationship to existing entities, will allow an entire body of observations to be explained with a much Section 19.6.",Constraint,4
2260,"As yet, ILP systems have not made discoveries on the level of Galileo or Joule, but their discoveries have been deemed publishable in the scientiﬁc literature.",Safety,1
2261,"(2009) endowed a robot with the ability to perform molecular biology experiments and extended ILP techniques to include experiment design, thereby creating an autonomous scientist that actually discovered new knowledge about the functional genomics of yeast.",Functional,3
2262,"Because much prior knowledge is expressed in terms of rela- tional models rather than attribute-based models, we have also covered systems that allow learning of relational models.",Constraint,1
2263,"It provides a deductive method for turning ﬁrst-principles knowledge into useful, efﬁcient, special-purpose expertise.",Constraint,3
2264,ILP methods can learn relational knowledge that is not expressible in attribute-based systems.,Constraint,1
2265,• ILP methods naturally generate new predicates with which concise new theories can be expressed and show promise as general-purpose scientiﬁc theory formation systems.,Constraint,1
2266,"Consider, for example, the hypothesis “All emeralds are grue,” where grue means “green if observed before time t, but blue if observed thereafter.” At any time up to t, we might have observed millions of instances conﬁrming the rule that emeralds are grue, and no disconﬁrming instances, and yet we are unwilling to adopt the rule.",Safety,4
2267,"The approach was used in the Meta-DENDRAL Bibliographical and Historical Notes 799 ANALOGICAL REASONING expert system for chemistry (Buchanan and Mitchell, 1978), and later in Mitchell’s (1983) LEX system, which learns to solve calculus problems.",Safety,1
2268,"Hirsh (1987) introduced the EBL algorithm described in the text, showing how it could be incorporated directly into a logic programming system.",Safety,1
2269,"Van Harmelen and Bundy (1988) explain EBL as a variant of the partial evaluation method used in program analysis systems (Jones et al., 1993).",Safety,1
2270,"Work on rule induction, such as the ID3 (Quinlan, 1986) and CN2 (Clark and Niblett, 1989) systems, led to FOIL (Quinlan, 1990), which for the ﬁrst time allowed practical induc- tion of relational rules.",Safety,1
2271,"The ﬁeld of relational learning was reinvigorated by Muggleton and Buntine (1988), whose CIGOL program incorporated a slightly incomplete version of inverse resolution and was capable of generating new predicates.",Safety,4
2272,"ITOU (Rouveirol and Puget, 1989) and CLINT (De Raedt, 1992) were other systems of that era.",Constraint,1
2273,"So-called discovery systems aim to model the process of scientiﬁc discovery of new concepts, usually by a direct search in the space of concept deﬁnitions.",Safety,1
2274,"Doug Lenat’s Automated Mathematician, or AM (Davis and Lenat, 1982), used discovery heuristics expressed as expert system rules to guide its search for concepts and conjectures in elementary number theory.",Safety,1
2275,"Unlike most systems designed for mathematical reasoning, AM lacked a concept of proof and could only make conjectures.",Constraint,1
2276,"AM’s architecture was generalized in the EURISKO system (Lenat, 1983) by adding a mechanism capable of rewrit- ing the system’s own discovery heuristics.",Safety,1
2277,Another class of discovery systems aims to operate with real scientiﬁc data to ﬁnd new laws.,Constraint,1
2278,"The systems DALTON, GLAUBER, and STAHL (Langley et al., 1987) are rule-based systems that look for quantitative relationships in experimental data from physical systems; in each case, the system has been able to recapitulate a well-known discovery from the his- tory of science.",Safety,1
2279,Discovery systems based on probabilistic techniques—especially clustering algorithms that discover new categories—are discussed in Chapter 20.,Constraint,1
2280,"DISCOVERY SYSTEM Exercises EXERCISES 801 19.1 Show, by translating into conjunctive normal form and applying resolution, that the conclusion drawn on page 784 concerning Brazilians is sound.",Safety,1
2281,Design and denomination determine the mass of a coin.,Safety,4
2282,"If there is more than one possible solution, provide one example of each different kind.",Interface,4
2283,Will this succeed in generating the appropriate results of an inverse resolution step?,Interface,4
2284,Would you need any special modiﬁcations to the logic programming system for this to work?,Safety,1
2285,"In this chapter, we will study how agents can learn what to do in the absence of labeled examples of what to do.",Safety,4
2286,"A supervised learning agent needs to be told the correct move for each position it encounters, but such feedback is seldom available.",Safety,4
2287,"In the absence of feedback from a teacher, an agent can learn a transition model for its own moves and can perhaps learn to predict the opponent’s moves, but without some feedback about what is good and what is bad, the agent will have no grounds for decid- ing which move to make.",Interface,4
2288,"The agent needs to know that something good has happened when it (accidentally) checkmates the opponent, and that something bad has happened when it is checkmated—or vice versa, if the game is suicide chess.",Safety,4
2289,"Our framework for agents regards the reward as part of the input percept, but the agent must be “hardwired” to recognize that part as a reward rather than as just another sensory input.",Safety,1
2290,"For example, in game playing, it is very hard for a human to provide accurate and consistent evaluations of large numbers of positions, which would be needed to train an evaluation function directly from examples.",Interface,3
2291,Reinforcement learning might be considered to encompass all of AI: an agent is placed in an environment and must learn to behave successfully therein.,Interface,4
2292,"To keep the chapter man- ageable, we will concentrate on simple environments and simple agent designs.",Safety,4
2293,"For the most part, we will assume a fully observable environment, so that the current state is supplied by each percept.",Safety,4
2294,"On the other hand, we will assume that the agent does not know how the en- vironment works or what its actions do, and we will allow for probabilistic action outcomes.",Safety,4
2295,We will consider three of the agent designs ﬁrst introduced in Chapter 2: • A utility-based agent learns a utility function on states and uses it to select actions that maximize the expected outcome utility.,Functional,3
2296,"A utility-based agent must also have a model of the environment in order to make decisions, because it must know the states to which its actions will lead.",Safety,4
2297,"For example, in order to make use of a backgammon evaluation function, a backgammon program must know what its legal moves are and how they affect the board position.",Functional,3
2298,"On the other hand, because they do not know where their actions lead, Q-learning agents cannot look ahead; this can seriously restrict their ability to learn, as we shall see.",Safety,4
2299,"Section 21.3 covers active learning, where the agent must also learn what to do.",Safety,4
2300,The principal issue is exploration: an agent must experience as much as possible of its environment in order to learn how to behave in it.,Interface,4
2301,We will use as our example the 4 × 3 world introduced in Chapter 17.,Safety,4
2302,"We will include a discount factor γ in all of our equations, but for the 4 × 3 world we will set γ = 1.",Safety,4
2303,"The idea is that the utility of a state is the expected total reward from that state onward (called the expected reward-to-go), and each trial provides a sample of this quantity for each state visited.",Safety,4
2304,"For example, the ﬁrst trial in the set of three given earlier provides a sample total reward of 0.72 for state (1,1), two samples of 0.76 and 0.84 for (1,2), two samples of 0.80 and 0.88 for (1,3), and so on.",Safety,4
2305,"In the limit of inﬁnitely many trials, the sample average will converge to the true expectation in Equation (21.1).",Safety,4
2306,"More broadly, we can view direct utility estimation as searching for U in a hypothesis space that is much larger than it needs to be, in that it includes many functions that violate the Bellman equations.",Safety,3
2307,"Because the model usually changes only slightly with each observation, the value iteration process can use the previous utility estimates as initial values and should converge quite quickly.",Constraint,4
2308,"In this sense, it provides a standard against which to measure other reinforcement learning algorithms.",Constraint,3
2309,"A reader familiar with the Bayesian learning ideas of Chapter 20 will have noticed that the algorithm in Figure 21.2 is using maximum-likelihood estimation to learn the transition model; moreover, by choosing a policy based solely on the estimated model it is acting as if the model were correct.",Constraint,3
2310,"If the agent will continue learning in the future, however, then ﬁnding an optimal policy becomes considerably more difﬁcult, because the agent must consider the effects of future observations on its beliefs about the transition model.",Interface,4
2311,This concept provides an analytical foundation for understanding the exploration problem described in Section 21.3.,Interface,4
2312,"Often, the set H will be the set of models that exceed some likelihood threshold on P (h | e), so the robust and Bayesian approaches are related.",Safety,4
2313,"Thus, its current estimate of 0.84 might be a little low and should be increased.",Constraint,4
2314,"One might think that this causes an improperly large change in U π(s) when a very rare transition occurs; but, in fact, because rare transitions occur only rarely, the average value of U π(s) will converge to the correct value.",Interface,4
2315,"Furthermore, if we change α from a ﬁxed parameter to a function that decreases as the number of times a state has been visited increases, then U π(s) itself will converge to the TEMPORAL DIFFERENCE Section 21.2.",Functional,3
2316,"The step-size function α(n) is chosen to ensure convergence, as described in the text.",Safety,3
2317,"It does not learn quite as fast as the ADP agent and shows much higher variability, but it is much simpler and requires much less computation per observation.",Safety,4
2318,"A more important difference is that whereas TD makes a single adjustment per ob- served transition, ADP makes as many as it needs to restore consistency between the utility estimates U and the environment model P .",Safety,4
2319,Only the ﬁrst 100 trials are shown to enable comparison with Figure 21.3.,Constraint,4
2320,"In this way, the resulting utility estimates will approximate more and more closely those of ADP—of course, at the expense of increased computation time.",Safety,4
2321,"However, many of the necessary adjustments to the state values on each iteration will be extremely tiny.",Safety,4
2322,(See Exercise 21.2.) This enables them to handle state spaces that are far too large for full ADP.,Constraint,4
2323,"Approximate ADP algorithms have an additional advantage: in the early stages of learning a new environment, the environment model P often will be far from correct, so there is little point in calculating an exact utility function to match it.",Interface,3
2324,An active agent must decide what actions to take.,Interface,4
2325,Let us begin with the adaptive dynamic programming agent and consider how it must be modiﬁed to handle this new freedom.,Constraint,4
2326,"First, the agent will need to learn a complete model with outcome probabilities for all actions, rather than just the model for the ﬁxed policy.",Constraint,4
2327,The simple learning mechanism used by PASSIVE-ADP-AGENT will do just ﬁne for this.,Safety,4
2328,"The utilities it needs to learn are those deﬁned by the optimal policy; they obey the Bellman equations given on page 652, which we repeat here for convenience: U (s) = R(s) + γ max a X s′ P (s′ | s, a)U (s′) .",Safety,4
2329,"Having obtained a utility function U that is optimal for the learned model, the agent can extract an optimal action by one-step look-ahead to maximize the expected utility; alternatively, if it uses policy iteration, the optimal policy is already available, so it should simply execute the action the optimal policy recommends.",Functional,3
2330,What the greedy agent has overlooked is that actions do more than provide rewards according to the current learned model; they also contribute to learning the true model by af- fecting the percepts that are received.,Interface,4
2331,"By improving the model, the agent will receive greater rewards in the future.2 An agent therefore must make a tradeoff between exploitation to maximize its reward—as reﬂected in its current utility estimates—and exploration to maxi- 2 Notice the direct analogy to the theory of information value in Chapter 16.",Interface,4
2332,"(See sidebar.) Although bandit problems are extremely difﬁcult to solve exactly to obtain an optimal exploration method, it is nonetheless possible to come up with a reasonable scheme that will eventually lead to optimal behavior by the agent.",Constraint,3
2333,"Technically, any such scheme needs to be greedy in the limit of inﬁnite exploration, or GLIE.",Safety,4
2334,A GLIE scheme must try each action in each state an unbounded number of times to avoid having a ﬁnite probability that an optimal action is missed because of an unusually bad series of outcomes.,Interface,4
2335,An ADP agent using such a scheme will eventually learn the true environment model.,Interface,4
2336,"A GLIE scheme must also eventually become greedy, so that the agent’s actions become optimal with respect to the learned (and hence the true) model.",Constraint,4
2337,This can be implemented by altering the constraint equation (21.4) so that it assigns a higher utility estimate to relatively BANDIT PROBLEM GLIE Section 21.3.,Safety,4
2338,"The gambler must choose which lever to play on each successive coin—the one that has paid off best, or maybe one that has not been tried?",Safety,4
2339,"To formulate a bandit problem properly, one must deﬁne exactly what is meant by optimal behavior.",Constraint,4
2340,"These deﬁ- nitions require that the expectation be taken over the possible worlds that the agent could be in, as well as over the possible results of each action sequence in any given world.",Safety,4
2341,"The function f (u, n) should be increasing in u and decreasing in n.",Functional,3
2342,This will have the effect of making the agent try each action–state pair at least Ne times.,Safety,4
2343,"The most obvious change from the passive case is that the agent is no longer equipped with a ﬁxed policy, so, if it learns a utility function U , it will need to learn a model in order to be able to choose an action based on U via one-step look-ahead.",Functional,3
2344,"The TD update rule will take this as seriously as if the outcome had been the normal result of the action, whereas one might suppose that, because the outcome was a ﬂuke, the agent should not worry about it too much.",Safety,4
2345,"In fact, of course, the unlikely outcome will occur only infrequently in a large set of training sequences; hence in the long run its effects will be weighted proportionally to its probability, as we would hope.",Safety,4
2346,"Once again, it can be shown that the TD algorithm will converge to the same values as ADP as the number of training sequences tends to inﬁnity.",Safety,3
2347,"We will use the notation Q(s, a) to denote the value of doing action a in state s.",Safety,4
2348,"As with utilities, we can write a constraint equation that must hold at equilibrium when the Q-values are correct: MODEL FREE Q(s, a) = R(s) + γ X s′ P (s′ | s, a) max a′ Q(s′, a′) .",Constraint,4
2349,"This does, however, require that a model also be learned, because the equation uses P (s′ | s, a).",Safety,4
2350,"The temporal-difference approach, on the other hand, requires no model of state transitions—all 844 Chapter 21.",Safety,4
2351,The complete agent design for an exploratory Q-learning agent using TD is shown in Figure 21.8.,Interface,4
2352,"On the other hand, SARSA is more realistic: for example, if the overall policy is even partly controlled by other agents, it is better to learn a Q-function for what will actually happen rather than what the agent would like to happen.",Functional,3
2353,It would be absurd to suppose that one must visit all these states many times in order to learn how to play the game!,Safety,4
2354,"That is, the most important aspect of function approximation is not that it requires less space, but that it allows for induc- tive generalization over input states.",Functional,3
2355,"This suggests that ˆUθ(1, 1), currently 0.8, is too large and must be reduced.",Constraint,4
2356,How should the parameters be adjusted to achieve this?,Safety,4
2357,"We expect that the agent will learn faster if it uses a function approximator, provided that the hypothesis space is not too large, but includes some functions that are a reasonably good ﬁt to the true utility function.",Functional,3
2358,"(See Exercise 21.8.) If we put the +1 reward at (5,5), the true utility is more like a pyramid and the function approximator in Equation (21.10) will fail miserably.",Functional,3
2359,21.5 POLICY SEARCH POLICY SEARCH STOCHASTIC POLICY SOFTMAX FUNCTION The ﬁnal approach we will consider for reinforcement learning problems is called policy search.,Functional,3
2360,Policy search will then ad- just the parameters θ to improve the policy.,Safety,4
2361,"(For a continuous action space, the policy can be a smooth function of the parameters.) That is, there will be values of θ such that an inﬁnitesimal change in θ causes the policy to switch from one action to another.",Functional,3
2362,We can follow the policy gradient vector ∇θρ(θ) provided ρ(θ) is differentiable.,Safety,4
2363,"With the usual caveats, this process will converge to a local optimum in policy space.",Constraint,4
2364,"Suppose we are trying to do hill climbing, which requires comparing ρ(θ) and ρ(θ + ∆θ) for some small ∆θ.",Constraint,4
2365,"The problem is that the total reward on each trial may vary widely, so estimates of the policy value from a small number of trials will be quite unreliable; trying to compare two such estimates will be even more unreliable.",Safety,4
2366,"For simplicity, we will derive this estimate for the simple case of a nonsequential environment in which the reward R(a) is obtained immediately after doing action a in the start state s0.",Safety,4
2367,"The resulting algorithm is called REINFORCE (Williams, 1992); it is usually much more effective than hill climbing using lots of trials at each value of θ.",Safety,3
2368,"It can be shown that the number of random sequences required to ensure that the value of every policy is well estimated depends only on the complexity of the policy space, and not at all on the complexity of the underlying domain.",Safety,4
2369,"This means that it is theoretically possible for Samuel’s program not to converge, or to converge on a strategy designed to lose rather than to win.",Constraint,4
2370,He managed to avoid this fate by insisting that the weight for material advantage should always be positive.,Safety,4
2371,"Moreover, unlike many subsequent systems, BOXES was implemented with a 852 Chapter 21.",Constraint,1
2372,The major points are: • The overall agent design dictates the kind of information that must be learned.,Safety,4
2373,"The three main designs we covered were the model-based design, using a model P and a utility function U ; the model-free design, using an action-utility function Q ; and the reﬂex design, using a policy π.",Functional,3
2374,"With TD, Q-learning requires no model in either the learning or action- selection phase.",Constraint,4
2375,"• When the learning agent is responsible for selecting actions while it learns, it must trade off the estimated value of those actions against the potential for learning useful new information.",Safety,4
2376,"• In large state spaces, reinforcement learning algorithms must use an approximate func- tional representation in order to generalize over states.",Interface,3
2377,Applications in robotics promise to be particularly valuable; these will require methods for handling con- 854 Chapter 21.,Constraint,1
2378,The psychological literature on reinforcement learning is much older; Hilgard and Bower (1975) provide a good survey.,Interface,4
2379,"Direct evidence for the operation of reinforcement learning in animals has been provided by investigations into the foraging behavior of bees; there is a clear neural correlate of the reward signal in the form of a large neuron mapping from the nectar intake sensors directly to the motor cortex (Mon- tague et al., 1995).",Safety,3
2380,"Research using single-cell recording suggests that the dopamine system in primate brains implements something resembling value function learning (Schultz et al., 1997).",Functional,1
2381,The paper by Sutton (1988) provides a good historical overview.,Safety,4
2382,The idea of prioritized sweeping was introduced independently by Moore and Atkeson (1993) and Bibliographical and Historical Notes 855 CMAC Peng and Williams (1993).,Safety,4
2383,"Bayesian reinforcement learning (Dearden et al., 1998, 1999) provides another angle on both model uncertainty and exploration.",Constraint,4
2384,"Papavassiliou and Russell (1999) describe a new type of reinforcement learning that converges with any form of function ap- proximator, provided that a best-ﬁt approximation can be found for the observed data.",Functional,3
2385,"Policy search methods were brought to the fore by Williams (1992), who developed the REINFORCE family of algorithms.",Safety,3
2386,"The method of correlated sampling for comparing different conﬁgurations of a system was described formally by Kahn and Marshall (1953), but seems to have been known long before that.",Safety,1
2387,Real-world environments also exhibit enormous complexity in terms of the number of primitive actions required to achieve signiﬁcant reward.,Safety,4
2388,"Such rewards can speed up learning enormously and are simple to provide, but there is a risk that the agent will learn to maximize the pseudorewards rather than the true rewards; for example, standing next to the ball and “vibrating” causes many contacts with the ball.",Constraint,4
2389,"(1999) show that the agent will still learn the optimal policy provided that the pseudoreward F (s, a, s′) satisﬁes F (s, a, s′) = γΦ(s′) − Φ(s), where Φ is an arbitrary function of the state.",Functional,3
2390,The partial-programming language for agent programs extends an ordinary pro- gramming language by adding primitives for unspeciﬁed choices that must be ﬁlled in by learning.,Interface,4
2391,"One key result (Dietterich, 2000) is that the hierarchical structure provides a natural additive decomposition of the overall utility function into terms that depend on small subsets of the variables deﬁning the state space.",Functional,3
2392,"For example, Bibliographical and Historical Notes 857 SUBAGENT APPRENTICESHIP LEARNING INVERSE REINFORCEMENT LEARNING RELATIONAL REINFORCEMENT LEARNING can we devise methods whereby separate subagents for robot navigation and robot obstacle avoidance could cooperatively achieve a combined control system that is globally optimal?",Safety,1
2393,"The consequent requirement for ran- domized policies is not a signiﬁcant complication, as we saw on page 848.",Safety,4
2394,There are extreme states (such as crashing the car) that clearly should have a large penalty.,Safety,4
2395,(2009) show how this technique works for learning to ﬂy a helicopter; see Figure 25.25 on page 1007 for an example of the acrobatics the resulting policy is capable of.,Interface,4
2396,Russell (1998) describes the task of inverse reinforcement learning—ﬁguring out what the reward function must be from an example path through that state space.,Functional,3
2397,"This is useful as a part of apprenticeship learning, or as a part of doing science—we can understand an animal or robot by working backwards from what it does to what its reward function must be.",Functional,3
2398,(1996) provides a good entry point to the literature.,Safety,4
2399,"Re- inforcement learning papers are published frequently in Machine Learning, in the Journal of Machine Learning Research, and in the International Conferences on Machine Learning and the Neural Information Processing Systems meetings.",Safety,1
2400,"Reinforcement Learning Implement a passive learning agent in a simple environment, such as the 4 × 3 world.",Safety,4
2401,Implement a priority queue for adjustments to the utility estimates.,Safety,4
2402,"Whenever a state is adjusted, all of its predecessors also become candidates for adjustment and should be added to the queue.",Safety,4
2403,Implement an exploring reinforcement learning agent that uses direct utility estima- 21.6 tion.,Interface,4
2404,"Implement the REINFORCE and PEGASUS algorithms and apply them to the 4 × 3 21.9 world, using a policy family of your own choosing.",Safety,3
2405,"An agent that wants to do knowledge acquisition needs to understand (at least partially) the ambiguous, messy languages that humans use.",Interface,4
2406,"Computer systems identify languages with greater than 99% accuracy; occasionally, closely related languages, such as Swedish and Norwegian, are confused.",Constraint,1
2407,"We may have some estimate of these values; for example, if we are selecting a random Web page we know that English is the most likely language and that the probability of Macedonian will be less than 1%.",Safety,4
2408,"Sopersteen was prescribed aciphex,” we should recognize that “Mr.",Constraint,4
2409,22.1.2 Smoothing n-gram models The major complication of n-gram models is that the training corpus provides only an esti- mate of the true probability distribution.,Interface,4
2410,For common character sequences such as “ th” any English corpus will give a good estimate: about 1.5% of all trigrams.,Constraint,4
2411,Does that mean we should as- sign P (“ th”) = 0?,Constraint,4
2412,Just because we have never seen “ http” before does not mean that our model should claim that it is impossi- ble.,Constraint,4
2413,"Thus, we will adjust our language model so that sequences that have a count of zero in the training corpus will be assigned a small nonzero probability (and the other counts will be adjusted downward slightly so that the probability still sums to 1).",Safety,4
2414,"The simplest type of smoothing was suggested by Pierre-Simon Laplace in the 18th cen- tury: he said that, in the lack of further information, if a random Boolean variable X has been false in all n observations so far then the estimate for P (X = true) should be 1/(n + 2).",Safety,4
2415,"The evaluation can be a task-speciﬁc metric, such as measuring accuracy on language identiﬁcation.",Safety,4
2416,"This metric is inconvenient because the probability of a large corpus will be a very small number, and ﬂoating-point underﬂow becomes an issue.",Interface,4
2417,"Then for a sequence of any length, the perplexity will be 100.",Safety,4
2418,"If some characters are more likely than others, and the model reﬂects that, then the model will have a perplexity less than 100.",Safety,4
2419,"In English a sequence of letters surrounded by spaces is a word, but in some languages, like Chinese, words are not separated by spaces, and even in English many decisions must be made to have a clear policy on word boundaries: how many words are in “ne’er-do-well”?",Constraint,4
2420,Bigram: systems are very similar computational approach would be represented .,Constraint,1
2421,"Even with this small sample, it should be clear that the unigram model is a poor approximation of either English or the content of an AI textbook, and that the bigram and trigram models are 1 With the possible exception of the groundbreaking work of T.",Constraint,4
2422,Designer watches for cheap ...,Constraint,4
2423,Ham: Abstract: We will motivate the problem of social identity clustering: ...,Safety,4
2424,A character model should detect this.,Constraint,4
2425,"If there are 100,000 words in the language model, then the feature vector has length 100,000, but for a short email message almost all the features will have count zero.",Safety,4
2426,"All of these have been applied to spam detection, usually with accuracy in the 98%–99% range.",Constraint,4
2427,"With a carefully designed feature set, accuracy can exceed 99.9%.",Constraint,4
2428,The idea is that a spam message will tend to share dictionary entries with other spam messages and thus will compress better when appended to a collection that already contains the spam dictionary.,Constraint,4
2429,"Experiments with compression-based classiﬁcation on some of the standard corpora for text classiﬁcation—the 20-Newsgroups data set, the Reuters-10 Corpora, the Industry Sector corpora—indicate that whereas running off-the-shelf compression algorithms like gzip, RAR, and LZW can be quite slow, their accuracy is comparable to traditional classiﬁcation algo- rithms.",Constraint,3
2430,The best-known examples of information retrieval systems are search engines on the World Wide Web.,Safety,1
2431,"In this section, we will see how such systems are built.",Constraint,1
2432,An information retrieval (henceforth IR) system can be characterized by QUERY LANGUAGE RESULT SET RELEVANT PRESENTATION BOOLEAN KEYWORD MODEL 1.,Interface,1
2433,"Each system must decide what it wants to treat as a document: a paragraph, a page, or a multipage text.",Safety,1
2434,"The query language can be just a list of words, such as [AI book]; or it can specify a phrase of words that must be adjacent, as in [“AI book”]; it can contain Boolean operators as in [AI AND book]; it can include non-Boolean operators such as [AI NEAR book] or [AI book site:www.aaai.org].",Safety,4
2435,This is the subset of documents that the IR system judges to be relevant to the query.,Safety,1
2436,The earliest IR systems worked on a Boolean keyword model.,Safety,1
2437,This model has the advantage of being simple to explain and implement.,Safety,4
2438,22.3.1 IR scoring functions Most IR systems have abandoned the Boolean model and use models based on the statistics of word counts.,Safety,1
2439,"For the query [farming in Kansas], documents that mention “farming” frequently will have higher scores.",Safety,4
2440,"A million-word document will probably mention all the query words, but may not actually be about the query.",Safety,4
2441,"Instead, systems create an index ahead of time that lists, for each vocabulary word, the documents that contain the word.",Interface,1
2442,PRECISION RECALL 22.3.2 IR system evaluation How do we know whether an IR system is performing well?,Interface,1
2443,We undertake an experiment in which the system is given a set of queries and the result sets are scored with respect to human relevance judgments.,Constraint,1
2444,"Imagine that an IR system has returned a result set for a single query, for which we know which documents are and are not relevant, out of a corpus of 100 documents.",Interface,1
2445,"In the extreme, a system that returns every document in the document collection is guaranteed a recall of 100%, but will have low precision.",Safety,1
2446,"Alternately, a system could return a single document and have low recall, but a decent chance at 100% precision.",Safety,1
2447,"22.3.3 IR reﬁnements There are many possible reﬁnements to the system described here, and indeed Web search engines are continually updating their algorithms as they discover new approaches and as the Web grows and changes.",Safety,1
2448,"The BM25 scoring function uses a word model that treats all words as completely in- dependent, but we know that some words are correlated: “couch” is closely related to both “couches” and “sofa.” Many IR systems attempt to account for these correlations.",Functional,1
2449,"For example, if the query is [couch], it would be a shame to exclude from the result set those documents that mention “COUCH” or “couches” but not “couch.” Most IR systems do case folding of “COUCH” to “couch,” and some use a stemming algorithm to reduce “couches” to the stem form “couch,” both in the query and the documents.",Safety,1
2450,"For example, stemming “stocking” to “stock” will tend to decrease precision for queries about either foot coverings or ﬁnancial instruments, although it could improve recall for queries about warehousing.",Constraint,4
2451,"The idea is that ibm.com has many in-links (links to the page), so it should be ranked higher: each in-link is a vote for the quality of the linked-to page.",Safety,4
2452,"Therefore, the PageRank algorithm is designed to weight links from high-quality sites more heavily.",Safety,3
2453,"The deﬁnition is recursive, but we will see that the recursion bottoms out properly.",Safety,4
2454,The PageRank of page p is then the probability that the random surfer will be at page p at any point in time.,Safety,4
2455,That means that it must be computed anew for each query—a computational burden that most search engines have elected not to take on.,Constraint,4
2456,"If we then normalize the scores and repeat k times, the process will converge.",Safety,4
2457,"There have been question-answering NLP (natural language processing) systems since the 1960s, but only since 2001 have such systems used Web information retrieval to radically increase their breadth of coverage.",Safety,1
2458,"The ASKMSR system (Banko et al., 2002) is a typical Web-based question-answering system.",Safety,1
2459,"It is based on the intuition that most questions will be answered many times on the Web, so question answering should be thought of as a problem in precision, not recall.",Safety,4
2460,"For example, consider the query [Who killed Abraham Lincoln?] Suppose a system had to answer that question with access only to a single encyclopedia, whose entry on Lincoln said John Wilkes Booth altered history with a bullet.",Constraint,1
2461,He will forever be known as the man who ended Abraham Lincoln’s life.,Safety,4
2462,"To use this passage to answer the question, the system would have to know that ending a life can be a killing, that “He” refers to Booth, and several other linguistic and semantic facts.",Safety,1
2463,There is also a ﬁlter that says the answer should not be part of the question; together these should allow us to return “John Wilkes Booth” (and not “Abraham Lincoln”) as the highest-scoring response.,Safety,4
2464,"In some cases the answer will be longer than three words; since the components re- sponses only go up to 3-grams, a longer response would have to be pieced together from shorter pieces.",Safety,4
2465,"For example, in a system that used only bigrams, the answer “John Wilkes Booth” could be pieced together from high-scoring pieces “John Wilkes” and “Wilkes Booth.” At the Text Retrieval Evaluation Conference (TREC), ASKMSR was rated as one of the top systems, beating out competitors with the ability to do far more complex language understanding.",Constraint,1
2466,"In a limited domain, this can be done with high accuracy.",Constraint,4
2467,We will see in Chapter 23 how to deﬁne complex language models of the phrase structure (noun phrases and verb phrases) of English.,Safety,4
2468,22.4.1 Finite-state automata for information extraction The simplest type of information extraction system is an attribute-based extraction system that assumes that the entire text refers to a single object and the task is to extract attributes of that object.,Interface,1
2469,"That will select $78.00 as the target from the text “List price $99.00, special sale price $78.00, shipping $3.00.” One step up from attribute-based extraction systems are relational extraction systems, which deal with multiple objects and the relations among them.",Constraint,1
2470,"Thus, when these systems see the text “$249.99,” they need to determine not just that it is a price, but also which object has that price.",Safety,1
2471,"A typical relational-based extraction system is FASTUS, which handles news stories about corporate mergers and acquisitions.",Safety,1
2472,A relational extraction system can be built as a series of cascaded ﬁnite-state transducers.,Safety,1
2473,"That is, the system consists of a series of small, efﬁcient ﬁnite-state automata (FSAs), where each automaton receives text as input, transduces the text into a different format, and passes it along to the next automaton.",Safety,1
2474,The idea is to chunk these into units that will be managed by the later stages.,Safety,4
2475,"We will see how to write a complex description of noun and verb phrases in Chapter 23, but here we have simple rules that only approximate the complexity of English, but have the advantage of being rep- resentable by ﬁnite state automata.",Safety,4
2476,"If the next sentence says “The joint venture will start production in January,” then this step will notice that there are two references to a joint venture, and that they should be merged into one.",Safety,4
2477,"In general, ﬁnite-state template-based information extraction works well for a restricted domain in which it is possible to predetermine what subjects will be discussed, and how they will be mentioned.",Constraint,4
2478,"The cascaded transducer model helps modularize the necessary knowl- edge, easing construction of the system.",Safety,1
2479,These systems work especially well when they are reverse-engineering text that has been generated by a program.,Constraint,1
2480,"22.4.2 Probabilistic models for information extraction When information extraction must be attempted from noisy or varied input, simple ﬁnite-state approaches fare poorly.",Constraint,4
2481,The “-” indicates a background state: Text: Speaker: - - Date: There will be a seminar by Dr.,Safety,4
2482,"HMMs can be trained from data; they don’t require laborious engineering of templates, and thus they can more easily be kept up to date as text changes over time.",Constraint,4
2483,"Note that we have assumed a certain level of structure in our HMM templates: they all consist of one or more target states, and any preﬁx states must precede the targets, postﬁx states most follow the targets, and other states must be background.",Performance,4
2484,"If there are multiple ﬁllers, we need to decide which to choose, as we discussed with template-based systems.",Constraint,1
2485,"This is another way of saying “the CRF model should prefer the target state SPEAKER for the word ANDREW.” If on the other hand λ1 < 0, the CRF model will try to avoid this association, and if λ1 = 0, this feature is ignored.",Safety,4
2486,"Here the bold words and commas must appear literally in the text, but the parentheses are for grouping, the asterisk means repetition of zero or more, and the question mark means optional.",Safety,4
2487,"Similar templates can be constructed with the key words “including,” “especially,” and “or other.” Of course these templates will fail to match many relevant passages, like “Rabies is a disease.” That is intentional.",Constraint,4
2488,"In one of the ﬁrst experiments of this kind, Brin (1999) started with a data set of just ﬁve examples: (“Isaac Asimov”, “The Robots of Dawn”) (“David Brin”, “Startide Rising”) (“James Gleick”, “Chaos—Making a New Science”) (“Charles Dickens”, “Great Expectations”) (“William Shakespeare”, “The Comedy of Errors”) Clearly these are examples of the author–title relation, but the learning system had no knowl- edge of authors or titles.",Constraint,1
2489,"The language of templates was designed to have a close mapping to the matches themselves, to be amenable to automated learning, and to emphasize high precision Section 22.4.",Safety,4
2490,"Given a good set of templates, the system can collect a good set of examples.",Safety,1
2491,"Given a good set of examples, the system can build a good set of templates.",Safety,1
2492,"22.4.6 Machine reading Automated template construction is a big step up from handcrafted template construction, but it still requires a handful of labeled examples of each relation to get started.",Constraint,4
2493,"To build a large ontology with many thousands of relations, even that amount of work would be onerous; we would like to have an extraction system with no human input of any kind—a system that could read on its own and build up its own database.",Constraint,1
2494,Such a system would be relation-independent; would work for any relation.,Safety,1
2495,"In practice, these systems work on all relations in parallel, because of the I/O demands of large corpora.",Safety,1
2496,They behave less like a traditional information- extraction system that is targeted at a few relations and more like a human reader who learns from the text itself; because of this the ﬁeld has been called machine reading.,Safety,1
2497,"A representative machine-reading system is T EXTRUNNER (Banko and Etzioni, 2008).",Safety,1
2498,"In the case of Hearst (1992), speciﬁc patterns (e.g., such as) provided the bootstrap, and for Brin (1998), it was a set of ﬁve author–title pairs.",Safety,4
2499,"Questionable answers include “wa- ter,” which came from the sentence “Boiling water for at least 10 minutes will kill bacteria.” It would be better to attribute this to “boiling water” rather than just “water.” With the techniques outlined in this chapter and continual new inventions, we are start- ing to get closer to the goal of machine reading.",Constraint,4
2500,"Bibliographical and Historical Notes 883 • Information retrieval systems use a very simple language model based on bags of words, yet still manage to perform well in terms of recall and precision on very large corpora of text.",Functional,1
2501,• Information-extraction systems use a more complex model that includes limited no- tions of syntax and semantics in the form of templates.,Safety,1
2502,"• In building a statistical language system, it is best to devise a model that can make good use of available data, even if the model seems overly simplistic.",Safety,1
2503,"Chomsky (1956, 1957) pointed out the limitations of ﬁnite-state models compared with context-free models, concluding, “Probabilistic models give no particular insight into some of the basic problems of syntactic structure.” This is true, but probabilistic models do provide insight into some other basic problems—problems that context-free models ignore.",Constraint,4
2504,Joachims (2001) uses statistical learning theory and support vector machines to give a theo- retical analysis of when classiﬁcation will be successful.,Constraint,4
2505,(1994) report an accuracy of 96% in classifying Reuters news articles into the “Earnings” category.,Interface,4
2506,"Koller and Sahami (1997) report accuracy up to 95% with a naive Bayes classiﬁer, and up to 98.6% with a Bayes classiﬁer that accounts for some dependencies among features.",Constraint,4
2507,Schapire and Singer (2000) show that simple linear classiﬁers can often achieve accuracy almost as good as more complex models and are more efﬁcient to evaluate.,Constraint,4
2508,"government’s National Institute of Standards and Technology (NIST), hosts an annual competition for IR systems and publishes proceedings with results.",Constraint,1
2509,Brin and Page (1998) describe the PageRank algorithm and the implementation of a Web search engine.,Safety,3
2510,The FASTUS ﬁnite-state system was done by Hobbs et al.,Safety,1
2511,"Surveys of template-based systems are given by Roche and Schabes (1997), Appelt (1999), Exercises 885 and Muslea (1999).",Constraint,1
2512,(2002) present the ASKMSR question-answering system; a similar sys- tem is due to Kwok et al.,Safety,1
2513,Pasca and Harabagiu (2001) discuss a contest-winning question-answering system.,Safety,1
2514,"Two early inﬂuential approaches to automated knowledge engi- neering were by Riloff (1993), who showed that an automatically constructed dictionary per- formed almost as well as a carefully handcrafted domain-speciﬁc dictionary, and by Yarowsky (1995), who showed that the task of word sense classiﬁcation (see page 756) could be accom- plished through unsupervised training on a corpus of unlabeled text with accuracy as good as supervised methods.",Constraint,3
2515,"The method was advanced by the QXTRACT (Agichtein and Gravano, 2003) and KNOWITALL (Etzioni et al., 2005) systems.",Safety,1
2516,"(Adapted from Jurafsky and Martin (2000).) In this exercise you will develop a classi- 22.3 ﬁer for authorship: given a text, the classiﬁer predicts which of two candidate authors wrote the text.",Safety,4
2517,Assess the accuracy of this technique.,Safety,4
2518,How does accuracy change as you alter the set of features?,Safety,4
2519,"Then train a classiﬁcation algorithm (decision tree, naive Bayes, SVM, logistic regression, or some other algorithm of your choosing) on a training set and report its accuracy on a test set.",Constraint,3
2520,22.8 Consider the problem of trying to evaluate the quality of an IR system that returns a ranked list of answers (like most Web search engines).,Interface,1
2521,"The searcher will look at the ﬁrst twenty answers returned, with the objective of getting as much relevant information as possible.",Constraint,4
2522,"The searcher needs only one relevant document, and will go down the list until she ﬁnds the ﬁrst one.",Safety,4
2523,"(E.g., a lawyer wants to be sure that she has found all relevant precedents, and is willing to spend considerable resources on that.) d.",Constraint,4
2524,The assistant will charge the searcher for the full hour regardless of whether he ﬁnds it immediately or at the end of the hour.,Safety,4
2525,The searcher will look through all the answers.,Safety,4
2526,"If the documents she has looked at so far are mostly good, she will continue; otherwise, she will stop.",Safety,4
2527,COMMUNICATION SIGN Communication is the intentional exchange of information brought about by the production and perception of signs drawn from a shared system of conventional signs.,Safety,1
2528,"The big issue for these models is data sparsity—with a vocabulary of, say, 105 words, there are 1015 trigram probabilities to estimate, and so a corpus of even a trillion words will not be able to supply reliable estimates for all of them.",Constraint,4
2529,Context-sensitive grammars are restricted only in that the right-hand side must contain at least as many symbols as the left-hand side.,Safety,4
2530,"Natural Language for Communication There have been many competing language models based on the idea of phrase struc- ture; we will describe a popular model called the probabilistic context-free grammar, or PCFG.1 A grammar is a collection of rules that deﬁnes a language as a set of allowable strings of words.",Safety,4
2531,"The E0 grammar generates a wide range of English sentences such as the following: John is in the pit The wumpus that stinks is in 2 2 Mary is in Boston and the wumpus is near 3 2 OVERGENERATION UNDERGENERATION Unfortunately, the grammar overgenerates: that is, it generates sentences that are not gram- matical, such as “Me go Boston” and “I smell pits wumpus John.” It also undergenerates: there are many sentences of English that it rejects, such as “I think the wumpus is smelly.” We will see how to learn a better grammar later; for now we concentrate on what we can do with the grammar we have.",Constraint,4
2532,"A left-to-right parsing algorithm would have to guess whether the ﬁrst word is part of a command or a question and will not be able to tell if the guess is correct until at least the eleventh word, take or taken.",Safety,3
2533,"If the algorithm guesses wrong, it will have to backtrack all the way to the ﬁrst word and reanalyze the whole sentence under the other interpretation.",Safety,3
2534,"Note that it requires a grammar with all rules in one of two very speciﬁc formats: lexical rules of the form X → word, and syntactic rules of the form X → Y Z .",Constraint,4
2535,"With the A∗ algorithm we don’t have to search the entire state space, and we are guaranteed that the ﬁrst parse found will be the most probable.",Constraint,3
2536,"Xn and use cross-validation to pick the best value of n.) We can then assume that the grammar includes every possible (X → Y Z ) or (X → word) rule, although many of these rules will have probability 0 or close to 0.",Safety,4
2537,"Lari and Young (1990) conclude that inside–outside is “computationally intractable for realistic problems.” However, progress can be made if we are willing to step outside the bounds of learning solely from unparsed text.",Safety,4
2538,"The latter is used for the pronoun “she,” the former for the pronoun “her.” We will explore this issue in Section 23.6; for now let us just say that there are many ways in which it would be useful to split a category like NP—grammar induction systems that use treebanks but automatically split categories do better than those that stick with the original category set (Petrov and Klein, 2007c).",Constraint,1
2539,"A Markov model of order two or more, given a sufﬁciently large corpus, will know that “eat Section 23.3.",Constraint,4
2540,"But a PCFG will usually assign fairly high probability to many short sentences, such as “He slept,” whereas in the Journal we’re more likely to see something like “It has been reported by a reliable source that the allegation that he slept is credible.” It seems that the phrases in the Journal really are not context-free; instead the writers have an idea of the expected sentence length and use that length as a soft global constraint on their sentences.",Interface,4
2541,"Natural Language for Communication DEFINITE CLAUSE GRAMMAR Note that since we are considering only heads, the distinction between “eat a banana” and “eat a rancid banana” will not be caught by these probabilities.",Safety,4
2542,Only a few percent of these can come from a corpus; the rest will have to come from smoothing (see Section 22.1.2).,Safety,4
2543,"These objectless probabilities are still very useful; they can capture the distinction between a transitive verb like “eat”—which will have a high value for P1 and a low value for P2—and an intransitive verb like “sleep,” which will have the reverse.",Interface,4
2544,"23.3.2 Formal deﬁnition of augmented grammar rules Augmented rules are complicated, so we will give them a formal deﬁnition by showing how an augmented rule can be translated into a logical sentence.",Interface,4
2545,"The sentence will have the form of a deﬁnite clause (see page 256), so the result is called a deﬁnite clause grammar, or DCG.",Safety,4
2546,"This works for toy examples, but serious language-generation systems need more control over the process than is afforded by the DCG rules alone.",Safety,1
2547,"Notice that all the NP rules must be duplicated, once for NP S and once for NP O.",Safety,4
2548,English requires subject–verb agreement for person and number of the subject and main verb of a sentence.,Safety,4
2549,"Note the use of the {test} notation to deﬁne logical predicates that must be satisﬁed, but that are not constituents.",Safety,4
2550,"We use the simple example sentence “John loves Mary.” The NP “John” should have as its semantic interpreta- tion the logical term John, and the sentence as a whole should have as its interpretation the logical sentence Loves(John, Mary).",Safety,4
2551,Applying ILP directly to learn this predicate results in poor performance: the induced parser has only about 20% accuracy.,Performance,4
2552,"With this additional back- ground knowledge, CHILL can learn to achieve 70% to 85% accuracy on various database query tasks.",Constraint,4
2553,We will brieﬂy mention some examples.,Constraint,4
2554,"This suggests that our two lexical rules for the words “loves” and “loved” should be these: Verb(λy λx e ∈ Loves(x, y) ∧ During(Now , e)) → loves Verb(λy λx e ∈ Loves(x, y) ∧ After(Now , e)) → loved .",Safety,4
2555,"In “Who did the agent tell you to give the gold to?” the ﬁnal word “to” should be parsed as [PP to ], where the “ ” denotes a gap or trace where an NP is missing; the missing NP is licensed by the ﬁrst word of the sentence, “who.” A complex system of augmentations is used to make sure that the missing NP s match up with the licensing words in just the right way, and prohibit gaps in the wrong places.",Constraint,1
2556,Safety experts say school bus passengers should be belted.,Safety,4
2557,A system with a large grammar and lexicon might ﬁnd thousands of interpretations for a perfectly ordinary sentence.,Constraint,1
2558,"The language model: the likelihood that a certain string of words will be chosen, given that the speaker has the intention of communicating a certain fact.",Safety,4
2559,"The acoustic model: for spoken communication, the likelihood that a particular se- quence of sounds will be generated, given that the speaker has chosen a given string of words.",Safety,4
2560,"This is typical accuracy: of the two sentences, one has an error that would not be made by a native speaker, yet the meaning is clearly conveyed.",Interface,4
2561,"Rough translation, as provided by free online services, gives the “gist” of a foreign sentence or document, but contains errors.",Safety,2
2562,"Translation is difﬁcult because, in the fully general case, it requires in-depth understand- ing of the text.",Safety,4
2563,"An English parsing system could use predicates like Open(x), but for translation, the representation language would have to make more distinctions, perhaps with Open 1(x) representing the “Offen” sense and Open 2(x) representing the “Neu Er¨offnet” sense.",Constraint,1
2564,"A translator (human or machine) often needs to understand the actual situation de- scribed in the source, not just the individual words.",Safety,4
2565,"For example, to translate the English word “him,” into Korean, a choice must be made between the humble and honoriﬁc form, a choice that depends on the social relationship between the speaker and the referent of “him.” In Japanese, the honoriﬁcs are relative, so the choice depends on the social relationships be- tween the speaker, the referent, and the listener.",Safety,4
2566,"It broke.” into French, we must choose the feminine “elle” or the masculine “il” for “it,” so we must decide whether “it” refers to the baseball or the window.",Safety,4
2567,"To get the translation right, one must understand physics as well as language.",Safety,4
2568,"For example, an Italian love poem that uses the masculine “il sole” (sun) and feminine “la luna” (moon) to symbolize two lovers will necessarily be altered when translated into German, where the genders are reversed, and further altered when translated into a language where the genders are the same.7 23.4.1 Machine translation systems All translation systems must model the source and target languages, but systems vary in the type of models they use.",Interface,1
2569,Some systems attempt to analyze the source language text all the way into an interlingua knowledge representation and then generate sentences in the target lan- guage from that representation.,Interface,1
2570,Other systems are based on a transfer model.,Constraint,1
2571,"Machine Translation 909 Interlingua Semantics Attraction(NamedJohn, NamedMary, High) English Semantics Loves(John, Mary) French Semantics Aime(Jean, Marie) English Syntax S(NP(John), VP(loves, NP(Mary))) French Syntax S(NP(Jean), VP(aime, NP(Marie))) English Words John loves Mary French Words Jean aime Marie The Vauquois triangle: schematic diagram of the choices for a machine Figure 23.12 translation system (Vauquois, 1968).",Safety,1
2572,"An interlingua- based system follows the solid lines, parsing English ﬁrst into a syntactic form, then into a semantic representation and an interlingua representation, and then through generation to a semantic, syntactic, and lexical form in French.",Interface,1
2573,A transfer-based system uses the dashed lines as a shortcut.,Safety,1
2574,Different systems make the transfer at different points; some make it at multiple points.,Safety,1
2575,"23.4.2 Statistical machine translation Now that we have seen how complex the translation task can be, it should come as no sur- prise that the most successful machine translation systems are built by training a probabilistic model using statistics gathered from a large corpus of text.",Safety,1
2576,"Should we work directly on P (f | e), or apply Bayes’ rule and work on P (e | f ) P (f )?",Constraint,4
2577,"But of course our resources are ﬁnite, and most of the sentences we will be asked to translate will be novel.",Safety,4
2578,"However, they will be composed of phrases that we have seen before (even if some phrases are as short as one word).",Constraint,4
2579,"For example, in this book, common phrases include “in this exercise we will,” “size of the state space,” “as a function of the” and “notes at the end of the chapter.” If asked to translate the novel sentence “In this exercise we will compute the size of the state space as a function of the number of actions.” into French, we should be able to break the sentence into phrases, ﬁnd the phrases in the English corpus (this book), ﬁnd the corresponding French phrases (from the French translation of the book), and then reassemble the French phrases into an order that makes sense in French.",Functional,3
2580,"We will specify this permutation in a way that seems a little complicated, but is designed to have a simple probability dis- tribution: For each fi, we choose a distortion di, which is the number of words that phrase fi has moved with respect to fi−1; positive for moving to the right, negative for moving to the left, and zero if fi immediately follows fi−1.",Constraint,4
2581,"The probability distribution provides a summary of the volatility of the permutations; how likely a distortion of P (d = 2) is, compared to P (d = 0), for example.",Safety,4
2582,We will have to search for a good solution.,Constraint,4
2583,"Bilingual text is also available online; some Web sites publish parallel content with parallel URLs, for 9 Named after William Hansard, who ﬁrst published the British parliamentary debates in 1811.",Constraint,4
2584,The leading statistical translation systems train on hundreds of millions of words of parallel text and billions of words of monolingual text.,Safety,1
2585,"Segment into sentences: The unit of translation is a sentence, so we will have to break the corpus into sentences.",Safety,4
2586,This approach achieves about 98% accuracy.,Constraint,4
2587,"Usually, the next sentence of English corre- sponds to the next sentence of French in a 1:1 match, but sometimes there is variation: one sentence in one language will be split into a 2:1 match, or the order of two sentences will be swapped, resulting in a 2:2 match.",Safety,4
2588,"short sentences should align with short sentences), it is possible to align them (1:1, 1:2, or 2:2, etc.) with accuracy in the 90% to 99% range using a variation on the Viterbi algorithm.",Constraint,3
2589,"For example, if the 3rd English and 4th French sentences contain the string “1989” and neighboring sentences do not, that is good evidence that the sentences should be aligned together.",Safety,4
2590,"Speech Recognition 913 SEGMENTATION COARTICULATION HOMOPHONES ACOUSTIC MODEL LANGUAGE MODEL NOISY CHANNEL MODEL people interact with speech recognition systems every day to navigate voice mail systems, search the Web from mobile phones, and other applications.",Constraint,1
2591,"Most speech recognition systems use a language model that makes the Markov assumption—that the cur- rent state Word t depends only on a ﬁxed number n of previous states—and represent Word t as a single random variable taking on a ﬁnite set of values, which makes it a Hidden Markov Model (HMM).",Safety,1
2592,"That means that a low-end system, sampling at 8 kHz with 8-bit quantization, would require nearly half a megabyte per minute of speech.",Constraint,1
2593,"Therefore, speech systems summarize the properties of the signal over time slices called frames.",Safety,1
2594,"A frame length of about 10 milliseconds (i.e., 80 samples at 8 kHz) is short enough to ensure that few short-duration phenomena will be missed.",Safety,4
2595,Picking out features from a speech signal is like listening to an orchestra and saying “here the French horns are playing loudly and the violins are playing softly.” We’ll give a brief overview of the features in a typical system.,Interface,1
2596,In this diagram Figure 23.15 each frame is described by the discretized values of three acoustic features; a real system would have dozens of features.,Safety,1
2597,Figure 23.17(a) shows a transition model that provides for this dialect variation.,Constraint,4
2598,"For task-speciﬁc speech recognition, the corpus should be task-speciﬁc: to build your airline reservation system, get transcripts of prior calls.",Safety,1
2599,"Part of the design of a voice user interface is to coerce the user into saying things from a limited set of options, so that the speech recognizer will have a tighter probability distribution to deal with.",Constraint,2
2600,"23.5.3 Building a speech recognizer The quality of a speech recognition system depends on the quality of all of its components— the language model, the word-pronunciation models, the phone models, and the signal- processing algorithms used to extract spectral features from the acoustic signal.",Safety,1
2601,"Large pronunciation dictionaries are now avail- able for English and other languages, although their accuracy varies greatly.",Constraint,4
2602,"As usual, we will acquire the probabilities from a corpus, this time a corpus of speech.",Safety,4
2603,"In the early days of speech recognition, the hidden variables were provided by laborious hand-labeling of spectrograms.",Safety,4
2604,Recent systems use expectation–maximization to automatically supply the missing data.,Safety,1
2605,"The method is guaranteed to increase the ﬁt between model and data on each iteration, and it generally converges to a much better set of parameter values than those provided by the initial, hand-labeled estimates.",Safety,3
2606,"The systems with the highest accuracy work by training a different model for each speaker, thereby capturing differences in dialect as well as male/female and other variations.",Constraint,1
2607,"This training can require several hours of interaction with the speaker, so the systems with the most widespread adoption do not create speaker-speciﬁc models.",Constraint,1
2608,The accuracy of a system depends on a number of factors.,Safety,1
2609,"First, the quality of the signal matters: a high-quality directional microphone aimed at a stationary mouth in a padded room will do much better than a cheap microphone transmitting a signal over phone lines from a car in trafﬁc with the radio playing.",Constraint,4
2610,"The vocabulary size matters: when recognizing digit strings with a vocabulary of 11 words (1-9 plus “oh” and “zero”), the word error rate will be below 0.5%, whereas it rises to about 10% on news stories with a 20,000-word vocabulary, and 20% on a corpus with a 64,000-word vocabulary.",Constraint,4
2611,The task matters too: when the system is trying to accomplish a speciﬁc task—book a ﬂight or give directions to a restaurant—the task can often be accomplished perfectly even with a word error rate of 10% or more.,Constraint,1
2612,"Unlike most other areas of AI, natural language understanding requires an empirical investigation of actual human behavior—which turns out to be complex and interesting.",Interface,4
2613,"Bibliographical and Historical Notes 919 • Sentences in a context-free language can be parsed in O(n3) time by a chart parser such as the CYK algorithm, which requires grammar rules to be in Chomsky Normal Form.",Safety,3
2614,"• Machine translation systems have been implemented using a range of techniques, from full syntactic and semantic analysis to statistical techniques based on phrase fre- quencies.",Constraint,1
2615,• Speech recognition systems are also primarily based on statistical principles.,Constraint,1
2616,"Speech systems are popular and useful, albeit imperfect.",Constraint,1
2617,Stolcke and Omohundro (1994) show how to learn grammar rules with Bayesian model merging; Haghighi and Klein (2006) describe a learning system based on prototypes.,Constraint,1
2618,"Leading parsers today include those by Petrov and Klein (2007b), which achieved 90.6% accuracy on the Wall Street Journal corpus, Charniak and Johnson (2005), which achieved 92.0%, and Koo et al.",Safety,4
2619,"The ﬁrst NLP system to solve an actual task was probably the BASEBALL question answering system (Green et al., 1961), which handled questions about a database of baseball Bibliographical and Historical Notes 921 UNIVERSAL GRAMMAR statistics.",Interface,1
2620,"Modern approaches to semantic interpretation usually assume that the mapping from syntax to semantics will be learned from examples (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005).",Safety,4
2621,"Prominent linguists, such as Chomsky (1957) and Pinker (2003), have used Gold’s result to argue that there must be an innate universal grammar that all children have from birth.",Interface,4
2622,"The so-called Poverty of the Stimulus argument says that children aren’t given enough input to learn a CFG, so they must already “know” the grammar and be merely tuning some of its parameters.",Safety,4
2623,"The Penn Treebank (Marcus et al., 1993) provides parse trees for a 3-million-word corpus of English.",Safety,4
2624,"Natural Language for Communication In the 1930s Petr Troyanskii applied for a patent for a “translating machine,” but there were no computers available to implement his ideas.",Safety,4
2625,"I will now proceed to decode.”’ For the next decade, the community tried to decode in this way.",Safety,4
2626,IBM exhibited a rudimentary system in 1954.,Safety,1
2627,"At AT&T Bell Labs, a system was built for recognizing isolated digits (Davis et al., 1952) by means of simple pattern matching of acoustic features.",Safety,1
2628,"Starting in 1971, the Defense Advanced Research Projects Agency (DARPA) of the United States Department of Defense funded four competing ﬁve-year projects to develop high-performance speech recognition systems.",Performance,1
2629,"The winner, and the only system to meet the goal of 90% accuracy with a 1000-word vocabulary, was the HARPY system at CMU (Lowerre and Reddy, 1980).",Constraint,1
2630,The ﬁnal version of HARPY was derived from a system called DRAGON built by CMU graduate student James Baker (1975); DRAGON was the ﬁrst to use HMMs for speech.,Safety,1
2631,"Almost simultaneously, Je- linek (1976) at IBM had developed another HMM-based system.",Safety,1
2632,"In 1997, Bill Gates predicted, “The PC ﬁve years from now—you won’t recognize it, because speech will come into the interface.” That didn’t quite happen, but in 2008 he predicted “In ﬁve years, Microsoft expects more Internet searches to be done through speech than through typing on a keyboard.” History will tell if he is right this time around.",Interface,2
2633,"The linguist John Firth (1957) proclaimed “You shall know a word by the company it keeps,” and linguis- tics of the 1940s and early 1950s was based largely on word frequencies, although without the computational power we have available today.",Safety,4
2634,There will be a test later.,Constraint,4
2635,At ﬁrst the whole procedure will seem complicated.,Safety,4
2636,"Soon, however, it will become just another facet of life.",Constraint,4
2637,Eventually they will be used once more and the whole cycle will have to be repeated.,Safety,4
2638,"Each sentence should be signiﬁcantly different, should be at least six words long, and should include some new lexical entries (which you should deﬁne).",Constraint,4
2639,In this exercise you will transform E0 into Chomsky Normal Form (CNF).,Constraint,4
2640,A rule (X → Y ) is not allowed in CNF; it must be (X → Y Z ) or (X → word).,Constraint,4
2641,Will that be true of every translation pair?,Constraint,4
2642,"is the, communication, exchange of, intentional, information brought, by, about, the production, perception of, and signs, from, drawn, a, of, system, signs, conventional, shared d.",Safety,1
2643,Report on the accuracy of this model.,Safety,4
2644,PERCEPTION SENSOR OBJECT MODEL RENDERING MODEL Perception provides agents with information about the world they inhabit by interpreting the response of sensors.,Constraint,3
2645,"Rather than trying to cover all of these, this chapter will cover one modality in depth: vision.",Constraint,4
2646,"We saw in our description of POMDPs (Section 17.4, page 658) that a model-based decision-theoretic agent in a partially observable environment has a sensor model—a prob- ability distribution P(E | S) over the evidence that its sensors provide, given a state of the world.",Interface,3
2647,"The object model could include a precise 3D geometric model taken from a computer-aided design (CAD) system, or it could be vague constraints, such as the fact that human eyes are usually 5 to 7 cm apart.",Constraint,1
2648,"Ambiguity can be managed with prior knowledge—we know Godzilla is not real, so the image must be a toy—or by selectively choosing to ignore the ambiguity.",Constraint,4
2649,"For example, the vision system for an autonomous car may not be able to interpret objects that are far in the distance, but the agent can choose to ignore the problem, because it is unlikely to crash into an object that is miles away.",Interface,1
2650,"For example, fruit ﬂies (Drosophila) are in part reﬂex agents: they have cervical giant ﬁbers that form a direct pathway from their visual system to the wing muscles that initiate an escape response—an immediate reaction, without deliberation.",Interface,1
2651,"The visual system extracts an estimate of the distance to the object, and the control system adjusts the wing muscles accordingly, allowing very fast changes of direction, with no need for a detailed model of the object.",Constraint,1
2652,"The problem for a vision-capable agent then is: Which aspects of the rich visual stimulus should be considered to help the agent make good action choices, and which aspects should be ignored?",Safety,4
2653,"Recognition could mean labeling each image with a yes or no as to whether it contains food that we should forage, or contains Grandma’s face.",Constraint,4
2654,Understanding these methods requires an understanding of the processes by which images are formed.,Interface,3
2655,"For example, a picture taken looking down a long straight set of railway tracks will suggest that the rails converge and meet.",Performance,4
2656,"As you move your hand back and forth or tilt it, your hand will seem to shrink and grow in the image, but it is not doing so in reality (Figure 24.1).",Safety,4
2657,"To see a focused image, we must ensure that all the photons from approximately the same spot in the scene arrive at approximately the same point in the image plane.",Safety,4
2658,"Photons from the scene must pass through the pinhole, so if it is small enough then nearby photons in the scene will be nearby in the image plane, and the image will be in focus.",Safety,4
2659,"We use a three-dimensional coordinate system with the origin at the pinhole, and consider a point P in the scene, with coordinates (X, Y, Z).",Constraint,1
2660,"This means that two parallel lines leaving different points in space will converge in the image—for large λ, the image points are nearly the same, whatever the value of (X0, Y0, Z0) (again, think railway tracks, Figure 24.1).",Safety,4
2661,VANISHING POINT 24.1.2 Lens systems MOTION BLUR The drawback of the pinhole camera is that we need a small pinhole to keep the image in focus.,Safety,1
2662,"But the smaller the pinhole, the fewer photons get through, meaning the image will be dark.",Safety,4
2663,"We can gather more photons by keeping the pinhole open longer, but then we will get motion blur—objects in the scene that move will appear blurred because they send photons to multiple locations on the image plane.",Safety,4
2664,"More light will enter, but light from a small patch of object in the scene will now be spread over a patch on the image plane, causing a blurred image.",Safety,4
2665,"Perception Image plane Light Source Iris Cornea Lens System Lens Fovea Visual Axis Optical Axis Retina Optic Nerve Lenses collect the light leaving a scene point in a range of directions, and steer Figure 24.3 it all to arrive at a single point on the image plane.",Safety,1
2666,Focusing works for points lying close to a focal plane in space; other points will not be focused properly.,Constraint,4
2667,"In cameras, elements of the lens system move to change the focal plane, whereas in the eye, the shape of the lens is changed by specialized muscles.",Safety,1
2668,LENS DEPTH OF FIELD FOCAL PLANE Vertebrate eyes and modern cameras use a lens system to gather sufﬁcient light while keeping the image in focus.,Safety,1
2669,"However, lens systems have a limited depth of ﬁeld: they can focus light only from points that lie within a range of depths (centered around a focal plane).",Constraint,1
2670,Objects outside this range will be out of focus in the image.,Safety,4
2671,"For example, spots on a distant leopard may look small because the leopard is far away, but two spots that are next to each other will have about the same size.",Safety,4
2672,"We will assume a linear model (current cameras have non- linearities at the extremes of light and dark, but are linear in the middle).",Safety,4
2673,"For example, a streak of dark makeup under a cheekbone will often look like a shading effect, making the face look thinner.",Safety,4
2674,A diffuse surface patch illuminated by a distant point light source will reﬂect some fraction of the light it collects; this fraction is called the diffuse albedo.,Safety,4
2675,"This model predicts, correctly, that the same surface will produce different colored image patches under different-colored lights.",Safety,4
2676,"With all sensors there will be noise in the image, and in any case there is a lot of data to deal with.",Constraint,3
2677,"In this section we will study three useful image-processing operations: edge detection, texture analysis, and computation of optical ﬂow.",Constraint,4
2678,This makes the low-level operations good candidates for implementation in parallel hardware—either in a graphics processor unit (GPU) or an eye.,Interface,4
2679,We will then look at one mid-level operation: segmenting the image into regions.,Safety,4
2680,"Edge detection is concerned only with the image, and thus does not distinguish between these different types of scene discontinuities; later processing will.",Constraint,4
2681,Later stages of processing will have to correct for these errors.,Constraint,4
2682,The measurement of brightness at a pixel in a CCD camera is based on a physical process involving the absorption of photons and the release of electrons; inevitably there will be statistical ﬂuctuations of the measurement—noise.,Safety,4
2683,"But how many neighbors should we consider—one pixel away, or two, or more?",Constraint,4
2684,"A σ of 1 pixel is enough to smooth over a small amount of noise, whereas 2 pixels will smooth a larger amount, but at the loss of some detail.",Safety,4
2685,"Edges correspond to locations in images where the brightness undergoes a sharp change, and so the magnitude of the gradient, k∇Ik, should be large at an edge point.",Interface,4
2686,"To tell whether a point is an edge point, we must look at other points a small distance forward and back along the direction of the gradient.",Interface,4
2687,This can be done by assuming that any two neighboring edge pixels with consistent orientations must belong to the same edge curve.,Constraint,4
2688,"A patch on a tiger and a patch on the grassy background will have very different orientation histograms, allowing us to ﬁnd the boundary curve between them.",Safety,4
2689,Optical ﬂow also enables us to recognize actions.,Constraint,4
2690,"Note that for this to work, there needs to be some texture or variation in the scene.",Safety,4
2691,"A boundary curve passing through a pixel (x, y) will have an orientation θ, so one way to formalize the problem of detecting boundary curves is as a machine learning classiﬁcation problem.",Interface,4
2692,"From there, knowledge-based algorithms can take over; they will ﬁnd it easier to deal with hundreds of superpixels rather than millions of raw pixels.",Constraint,3
2693,"For the moment, we will consider only faces where the nose is oriented vertically; we will deal with rotated faces below.",Constraint,4
2694,"Thus, we will likely have several overlapping windows that each report a match for a face.",Constraint,4
2695,All this yields a system whose architecture is sketched in Figure 24.12.,Safety,1
2696,"Perception Non-maximal suppresion Image Responses Detections Estimate orientation Correct illumination Rotate window Features Classifier Face ﬁnding systems vary, but most follow the architecture illustrated in Figure 24.12 two parts here.",Safety,1
2697,Classiﬁer outputs are then postprocessed to ensure that only one face is placed at each location in the image.,Safety,4
2698,This is because some structure will be present in the images produced by the object.,Safety,4
2699,"For example, if the pattern elements are color pixels, the French, UK, and Netherlands ﬂags will get confused because they have approximately the same color histograms, though the colors are arranged in very different ways.",Safety,4
2700,Histogram-based features have been successful in a wide variety of recognition applications; we will survey pedestrian detection.,Constraint,1
2701,"We expect to see some evidence of arms and legs, and the curve around the shoulders and head also tends to visible and quite distinctive.",Safety,4
2702,"Pedestrians can move their arms and legs around, so we should use a histogram to suppress some spatial detail in the feature.",Safety,4
2703,"Doing so will produce a feature that can tell whether the head-and- shoulders curve is at the top of the window or at the bottom, but will not change if the head moves slightly.",Safety,4
2704,One further trick is required to make a good feature.,Constraint,4
2705,"We will write || ∇Ix || for the gradient magnitude at point x in the image, write C for the cell whose histogram we wish to compute, and write wx,C for the weight that we will use for the Section 24.4.",Safety,4
2706,Non-maximum suppression needs to be applied to the output.,Safety,4
2707,"Note that although the instantaneous optical ﬂow ﬁeld cannot provide either the distance Z or the velocity component Tz, it can provide the ratio of the two and can therefore be used to control the landing approach.",Interface,4
2708,Most prey have eyes on the side of the head to enable a wider ﬁeld of vision.,Safety,4
2709,"Because a given feature in the scene will be in a different place relative to the z-axis of each image plane, if we superpose the two images, there will be a disparity in the location of the image feature in the two images.",Safety,4
2710,We will need to work out the geometrical relationship between disparity and depth.,Safety,4
2711,"First, we will consider the case when both the eyes (or cameras) are looking forward with their optical axes parallel.",Constraint,4
2712,"For convenience, we will compute the angular disparity, measured in radians.",Safety,4
2713,"For some other point P in the scene that is δZ farther away, we can compute the angular displacements of the left and right images of P , which we will call PL and PR, respectively.",Safety,4
2714,"• The relative orientation problem, i.e., determining the transformation (rotation and translation) between the coordinate systems ﬁxed to the different cameras.",Safety,1
2715,"However, the accuracy and applicability of these algorithms is not anywhere as general as those based on using multiple views.",Safety,3
2716,"The key to this situation seems to be that nearby normals will be similar, because most surfaces are smooth—they do not have sharp changes.",Safety,4
2717,"(Courtesy of Isha Malik.) metry provide cues to solving the ﬁgure-ground problem—assigning which side of the contour is ﬁgure (nearer), and which is ground (farther).",Interface,4
2718,"Reconstructing the 3D World 955 Image plane Horizon Ground plane C B B C A A In an image of people standing on a ground plane, the people whose feet Figure 24.22 are closer to the horizon in the image must be farther away (top drawing).",Interface,4
2719,This means they must look smaller in the image (left lower drawing).,Safety,4
2720,Notice that pedestrians who are higher in the scene must be smaller.,Safety,4
2721,Pedestrians who are farther away from the camera must also be smaller in the image.,Safety,4
2722,"A reasonably reliable pedestrian detector is capable of producing estimates of the horizon, if there are several pedestrians in the scene at different distances from the camera.",Safety,4
2723,These are measured in some coordinate system that is natural for the object.,Safety,1
2724,The net result is a transformation Q that will bring the model point mi into alignment with the image point pi.,Constraint,4
2725,"Although we do not know Q initially, we do know (for rigid objects) that Q must be the same for all the model points.",Safety,4
2726,"We will not give a proof here; we merely state the following result: Given three noncollinear points m1, m2, and m3 in the model, and their scaled orthographic projections p1, p2, and p3 on the image plane, there exist exactly two transformations from the three-dimensional model coordinate frame to a two- dimensional image coordinate frame.",Safety,4
2727,"The shape of an object is relevant for some manipulation tasks (e.g., deciding where to grasp an object), but its most signiﬁcant role is in object recognition, where geometric shape along with color and texture provide the most signiﬁcant cues to enable us to identify objects, classify what is in the image as an example of some class one has seen before, and so on.",Interface,4
2728,"We have seen that we can ﬁnd a box by pooling the evidence provided by orientations, using histogram methods to suppress potentially confusing spatial detail.",Safety,3
2729,"If we want to know more about what someone is doing, we will need to know where their arms, legs, body, and head lie in the picture.",Safety,4
2730,We will search the image for the best match to this cardboard person using inference methods for a tree-structured Bayes net (see Chapter 14).,Safety,3
2731,"First, an image rectangle should look like its segment.",Interface,4
2732,"For the moment, we will remain vague about precisely what that means, but we assume we have a function φi that scores how well an image rectangle matches a body segment.",Functional,3
2733,"All the functions will be larger if the match is better, so we can think of them as being like a log probability.",Safety,3
2734,"It is inconvenient to search a continuous space, and we will discretize the space of image rectangles.",Safety,4
2735,We must now ﬁnd the best allocation of rectangles to segments.,Safety,4
2736,"This will be slow, because there are many image rectangles and, for the model we have given, choosing the right torso will be O(M 6) if there are M image rectangles.",Safety,4
2737,"Generally, we don’t know what a person looks like, and must build a model of segment appearances.",Constraint,4
2738,"If we must report the conﬁguration of a person in a single image, we can start with a poorly tuned appearance model, estimate conﬁguration with this, then re-estimate appearance, and so on.",Constraint,4
2739,"In video, we have many frames of the same person, and this will reveal their appearance.",Safety,4
2740,"If we could reliably report the location of arms, legs, torso, and head in video sequences, we could build much improved game interfaces and surveillance systems.",Safety,1
2741,"The detector does not need to be very accurate, but should produce few false positives.",Safety,4
2742,"They will appear at least once in most of the frames of video; such segments can be found by It is best to start with the torso, because it is big and clustering the detector responses.",Constraint,4
2743,"Once we have a torso appearance model, upper leg segments should appear near the torso, and so on.",Safety,4
2744,"A good choice of conﬁguration is one that is easy to detect reliably, and where there is a strong chance the person will appear in that conﬁguration even in a short sequence (lateral walking is a good choice).",Safety,4
2745,"24.6 USING VISION BACKGROUND SUBTRACTION If vision systems could analyze video and understood what people are doing, we would be able to: design buildings and public places better by collecting and using data about what people do in public; build more accurate, more secure, and less intrusive surveillance systems; build computer sports commentators; and build human-computer interfaces that watch people and react to their behavior.",Constraint,1
2746,Applications for reactive interfaces range from computer games that make a player get up and move around to systems that save energy by managing heat and light in a building to match where the occupants are and what they are doing.,Safety,1
2747,"Let’s suppose the user enters a text query, such as “bicycle race.” Some of the images will have keywords or captions attached, or will come from Web pages that contain text near the image.",Safety,4
2748,Many images of a set of points should reveal their positions unambiguously.,Constraint,4
2749,"If the locations of points in the images are known with some accuracy and the viewing directions are reasonable, very high accuracy camera and point information can be obtained.",Constraint,4
2750,"Some applications are • Model-building: For example, one might build a modeling system that takes a video sequence depicting an object and produces a very detailed three-dimensional mesh of textured polygons for use in computer graphics and virtual reality applications.",Interface,1
2751,"24.6.3 Using vision for controlling movement One of the principal uses of vision is to provide information both for manipulating objects— picking them up, grasping them, twirling them, and so on—and for navigating while avoiding obstacles.",Interface,4
2752,"The ability to use vision for these purposes is present in the most primitive of In many cases, the visual system is minimal, in the sense that it animal visual systems.",Safety,1
2753,extracts from the available light ﬁeld just the information the animal needs to inform its behavior.,Safety,4
2754,"Quite probably, modern vision systems evolved from early, primitive organisms that used a photosensitive spot at one end to orient themselves toward (or away from) the light.",Safety,1
2755,We saw in Section 24.4 that ﬂies use a very simple optical ﬂow detection system to land on walls.,Safety,1
2756,"A classic study, What the Frog’s Eye Tells the Frog’s Brain (Lettvin et al., 1959), observes of a frog that, “He will starve to death surrounded by food if it is not moving.",Safety,4
2757,His choice of food is determined only by size and movement.” Let us consider a vision system for an automated vehicle driving on a freeway.,Interface,1
2758,"Figure 24.26 This ﬁgure outlines a system built by Michael Goesele and colleagues from the University of Washington, TU Darmstadt, and Microsoft Research.",Safety,1
2759,"From a collection of pictures of a monument taken by a large community of users and posted on the Internet (a), their system can determine the viewing directions for those pictures, shown by the small black pyramids in (b) and a comprehensive 3D reconstruction shown in (c).",Safety,1
2760,Lateral control—ensure that the vehicle remains securely within its lane or changes lanes smoothly when required.,Safety,4
2761,Longitudinal control—ensure that there is a safe distance to the vehicle in front.,Safety,4
2762,"For lateral control, one needs to maintain a representation of the position and orientation of the car relative to the lane.",Safety,4
2763,"This information, along with information about the dynamics of the car, is all that is needed by the steering-control system.",Constraint,1
2764,"If we have good detailed maps of the road, then the vision system serves to conﬁrm our position (and to watch for obstacles that are not on the map).",Safety,1
2765,"For longitudinal control, one needs to know distances to the vehicles in front.",Safety,4
2766,A group at Sarnoff has developed a system based on two cameras looking forward that track feature points in 3D and use that to reconstruct the Section 24.7.,Safety,1
2767,"In fact, they have two stereoscopic camera systems, one looking front and one looking back—this gives greater robustness in case the robot has to go through a featureless patch due to dark shadows, blank walls, and the like.",Safety,1
2768,"Now of course, that could happen, so a backup is provided by using an inertial motion unit (IMU) somewhat akin to the mechanisms for sensing acceleration that we humans have in our inner ears.",Interface,4
2769,"The solution for this is to use landmarks to provide absolute position ﬁxes: as soon as the robot passes a location in its internal map, it can adjust its estimate of its position appropriately.",Interface,4
2770,"Instead, a vision system should compute just what is needed to accomplish the task.",Safety,1
2771,"24.7 SUMMARY Although perception appears to be an effortless activity for humans, it requires a signiﬁcant amount of sophisticated computation.",Interface,4
2772,"• There are various cues in the image that enable one to obtain three-dimensional in- formation about the scene: motion, stereopsis, texture, shading, and contour analysis.",Safety,4
2773,Each of these cues relies on background assumptions about physical scenes to provide nearly unambiguous interpretations.,Interface,4
2774,Systematic attempts to understand human vision can be traced back to ancient times.,Constraint,1
2775,"The other was the development of point descriptors, which enable one to construct feature vectors from parts of objects.",Safety,4
2776,(2001) provide a comprehensive treatment of the geometry of multiple views.,Interface,4
2777,"For the reader interested in human vision, Palmer (1999) provides the best comprehen- sive treatment; Bruce et al.",Safety,4
2778,"While many of his speciﬁc models haven’t stood the test of time, the theoretical perspective from which each task is analyzed at an informa- tional, computational, and implementation level is still illuminating.",Interface,4
2779,"Research with a machine learning component is also published in the NIPS (Neural Informa- tion Processing Systems) conference, and work on the interface with computer graphics often appears at the ACM SIGGRAPH (Special Interest Group in Graphics) conference.",Constraint,1
2780,What will you expect to see in the image if the cylinder is illuminated by a point source at inﬁnity located on the positive x-axis?,Safety,4
2781,24.4 A stereoscopic system is being contemplated for terrain mapping.,Safety,1
2782,"It will consist of two CCD cameras, each having 512 × 512 pixels on a 10 cm × 10 cm square sensor.",Constraint,3
2783,"If the nearest distance to be measured is 16 meters, what is the largest disparity that will occur (in pixels)?",Safety,4
2784,"In stereo views of the same scene, greater accuracy is obtained in the depth calculations if the two camera positions are farther apart.",Safety,4
2785,Top view of a two-camera vision system observing a bottle with a wall (Courtesy of Pietro Perona.) Figure 24.27 shows two cameras at X and Y observing a 24.6 scene.,Constraint,1
2786,All AI researchers should be concerned with the ethical implications of their work.,Constraint,4
2787,"How- ever, neither the questions nor the answers have any relevance to the design or capabilities of airplanes and submarines; rather they are about the usage of words in English.",Safety,4
2788,"The practical possibility of “thinking machines” has been with us for only 50 years or so, not long enough for speakers of English to settle on a meaning for the word “think”—does it require “a brain” or just “brain-like parts.” Alan Turing, in his famous paper “Computing Machinery and Intelligence” (1950), sug- gested that instead of asking whether machines can think, we should ask whether machines can pass a behavioral intelligence test, which has come to be called the Turing Test.",Constraint,4
2789,We will look at some of them.,Constraint,4
2790,"Computer chess expert David Levy predicts that by 2050 people will routinely fall in love with humanoid robots (Levy, 2007).",Constraint,4
2791,Each of these required performance at the level of a human expert.,Performance,4
2792,"It is clear that computers can do many things as well as or better than humans, including things that people believe require great human insight and understanding.",Constraint,4
2793,"This does not mean, of course, that computers use insight and understanding in performing these tasks—those are not part of behavior, and we address such questions elsewhere—but the point is that one’s ﬁrst guess about the mental processes required to produce a given behavior is often wrong.",Safety,4
2794,"26.1.2 The mathematical objection It is well known, through the work of Turing (1936) and G¨odel (1931), that certain math- ematical questions are in principle unanswerable by particular formal systems.",Safety,1
2795,"Brieﬂy, for any formal axiomatic system F powerful enough to do arithmetic, it is possible to construct a so-called G¨odel sentence G(F ) with the following properties: • G(F ) is a sentence of F , but cannot be proved within F .",Constraint,1
2796,"Lucas (1961) have claimed that this theorem shows that machines are mentally inferior to humans, because machines are formal systems that are limited by the incompleteness theorem—they cannot establish the truth of their own G¨odel sentence—while humans have no such limitation.",Safety,1
2797,We will examine only three of the problems with the claim.,Constraint,4
2798,"First, G¨odel’s incompleteness theorem applies only to formal systems that are powerful enough to do arithmetic.",Constraint,1
2799,"Turing machines are inﬁnite, whereas computers are ﬁnite, and any computer can therefore be described as a (very large) system in propositional logic, which is not subject to G¨odel’s incompleteness theorem.",Safety,1
2800,"Second, an agent should not be too ashamed that it cannot establish the truth of some sentence while other agents can.",Interface,4
2801,"If Lucas asserted this sentence, then he would be contradicting himself, so therefore Lucas cannot consistently assert it, and hence it must be true.",Constraint,4
2802,"It is all too easy to show rigorously that a formal system cannot do X, and then claim that hu- mans can do X using their own informal method, without giving any evidence for this claim.",Safety,1
2803,"Indeed, it is impossible to prove that humans are not subject to G¨odel’s incompleteness theo- rem, because any rigorous proof would require a formalization of the claimed unformalizable human talent, and hence refute itself.",Safety,4
2804,"This appeal is expressed with arguments such as “we must assume our own consistency, if thought is to be possible at all” (Lucas, 1976).",Constraint,4
2805,GOFAI is supposed to claim that all intelligent behavior can be captured by a system that reasons logically from a set of facts and rules describing the domain.,Safety,1
2806,"As we saw in Chapter 13, probabilistic reasoning systems are more appropriate for open-ended domains.",Constraint,1
2807,"It is reasonable to suppose, however, that a book called What First-Order Logical Rule-Based Systems Without Learning Can’t Do might have had less impact.",Constraint,1
2808,"In our view, this is a good reason for a serious redesign of current models of neural processing so that they can take advantage of previously learned knowledge in the way that other learning algorithms do.",Safety,3
2809,"Learning algorithms do not perform well with many features, and if we pick a subset of features, “there is no known way of adding new features should the current set prove inadequate to account for the learned facts.” In fact, new methods such as support vector machines handle large feature sets very well.",Functional,3
2810,STANLEY’s 132-mile trip through the desert (page 28) was made possible in large part by an active sensing system of this kind.,Interface,1
2811,"In sum, many of the issues Dreyfus has focused on—background commonsense knowledge, the qualiﬁcation problem, uncertainty, learning, compiled forms of decision making—are indeed important issues, and have by now been incorporated into standard intelligent agent design.",Safety,4
2812,"As philosopher Andy Clark (1998) says, “Biological brains are ﬁrst and foremost the control systems for biological bodies.",Safety,1
2813,"We need to study the system as a whole; the brain augments its reasoning by referring to the environment, as the reader does in perceiving (and creating) marks on paper to transfer knowledge.",Safety,1
2814,"Instead, he maintains that the question is just as ill-deﬁned as asking, “Can machines think?” Besides, why should we insist on a higher standard for machines than we do for humans?",Safety,4
2815,"Nevertheless, Turing says, “Instead of arguing continually over this point, it is usual to have the polite convention that everyone thinks.” Turing argues that Jefferson would be willing to extend the polite convention to ma- chines if only he had experience with ones that act intelligently.",Constraint,4
2816,"He cites the following dialog, which has become such a part of AI’s oral tradition that we simply have to include it: HUMAN: In the ﬁrst line of your sonnet which reads “shall I compare thee to a summer’s day,” would not a “spring day” do as well or better?",Safety,4
2817,"For thinking, we have not yet reached our 1848 and there are those who believe that artiﬁcial thinking, no matter how impressive, will never be real.",Constraint,4
2818,"For example, the philosopher John Searle (1980) argues as follows: No one supposes that a computer simulation of a storm will leave us all wet .",Safety,4
2819,"In fact, we typically speak of an implementation of addition or chess, not a simulation.",Interface,4
2820,Turing’s answer—the polite convention—suggests that the issue will eventually go away by itself once machines reach a certain level of sophistication.,Safety,4
2821,"His Meditations on First Philosophy (1641) considered the mind’s activity of thinking (a process with no spatial extent or material prop- erties) and the physical processes of the body, concluding that the two must exist in separate realms—what we would now call a dualist theory.",Constraint,4
2822,"If physicalism is correct, it must be the case that the proper description of a person’s mental state is determined by that person’s brain state.",Safety,4
2823,"Imag- ine, if you will, that your brain was removed from your body at birth and placed in a mar- velously engineered vat.",Constraint,4
2824,"On the other hand, if one is concerned with the question of whether AI systems are really thinking and really do have mental states, then narrow content is appropriate; it simply doesn’t make sense to say that whether or not an AI system is really thinking depends on conditions outside that system.",Constraint,1
2825,"Narrow content is also relevant if we are thinking about designing AI systems or understanding their operation, because it is the narrow content of a brain state that determines what will be the (narrow content of the) next brain state.",Safety,1
2826,"Under functionalist theory, any two systems with isomorphic causal processes would have the same mental states.",Constraint,1
2827,"Of course, we have not yet said what “isomorphic” really means, but the assumption is that there is some level of abstraction below which the speciﬁc implementation does not matter.",Safety,4
2828,"By the deﬁnition of the experiment, the subject’s external behavior must remain unchanged compared with what would be observed if the operation were not carried out.3 Now although the presence or absence of consciousness cannot easily be ascertained by a third party, the subject of the experiment ought at least to be able to record any changes in his or her own conscious experience.",Constraint,4
2829,"First, note that, for the external behavior to re- main the same while the subject gradually becomes unconscious, it must be the case that the subject’s volition is removed instantaneously and totally; otherwise the shrinking of aware- ness would be reﬂected in external behavior—“Help, I’m shrinking!” or words to that effect.",Safety,4
2830,"By the conditions of the experiment, we will get responses such as “I feel ﬁne.",Safety,4
2831,"I must say I’m a bit surprised because I believed Searle’s argument.” Or we might poke the subject with a pointed stick and observe the response, “Ouch, that hurt.” Now, in the normal course of affairs, the skeptic can dismiss such outputs from AI programs as mere contrivances.",Constraint,4
2832,Then we must have an explanation of the manifestations of consciousness produced by the electronic brain that appeals only to the functional properties of the neurons.,Interface,3
2833,"And this explanation must also apply to the real brain, which has the same functional properties.",Safety,3
2834,"Instead, the brain must contain a second, unconscious mechanism that is responsible for the “Ouch.” Patricia Churchland (1986) points out that the functionalist arguments that operate at the level of the neuron can also operate at the level of any larger functional unit—a clump of neurons, a mental module, a lobe, a hemisphere, or the whole brain.",Safety,2
2835,"That means that if you accept the notion that the brain replacement experiment shows that the replacement brain is conscious, then you should also believe that consciousness is maintained when the entire brain is replaced by a circuit that updates its state and maps from inputs to outputs via a huge lookup table.",Safety,4
2836,"1031 BIOLOGICAL NATURALISM system that might be described (even in a simple-minded, computational sense) as accessing and generating beliefs, introspections, goals, and so on.",Safety,1
2837,"Thus, mental states cannot be duplicated just on the basis of some pro- gram having the same functional structure with the same input–output behavior; we would require that the program be running on an architecture with the same causal power as neurons.",Constraint,3
2838,"To support his view, Searle describes a hypothetical system that is clearly running a program and passes the Turing Test, but that equally clearly (according to Searle) does not understand anything of its inputs and outputs.",Safety,1
2839,"The system consists of a human, who understands only English, equipped with a rule book, written in English, and various stacks of paper, some blank, some with indecipherable inscriptions.",Constraint,1
2840,"(The human therefore plays the role of the CPU, the rule book is the program, and the stacks of paper are the storage device.) The system is inside a room with a small opening to the outside.",Constraint,1
2841,"Eventually, the instructions will cause one or more symbols to be transcribed onto a piece of paper that is passed back to the outside world.",Safety,4
2842,"But from the outside, we see a system that is taking input in the form of Chinese sentences and generating answers in Chinese that are as “intelligent” as those in the conversation imagined by Turing.4 Searle then argues: the person in the room does not understand Chinese (given).",Safety,1
2843,"Several commentators, including John McCarthy and Robert Wilensky, proposed what Searle calls the systems reply.",Safety,1
2844,"In both cases, the answer is no, and in both cases, according to the systems reply, the entire system does have the capacity in question.",Safety,1
2845,"By Turing’s polite convention, this should be enough.",Constraint,4
2846,He seems to be relying on the argument that a property of the whole must reside in one of the parts.,Safety,4
2847,"From the fourth axiom he concludes “Any other system capable of causing minds would have to have causal powers (at least) equivalent to those of brains.” From there he infers that any artiﬁcial brain would have to duplicate the causal powers of brains, not just run a particular program, and that human brains do not produce mental phenomena solely by virtue of running a program.",Safety,1
2848,"According to Searle, the point of the Chinese Room argument is to provide intuitions for axiom 3.",Interface,4
2849,"It should be noted, however, that neurons evolved to fulﬁll functional roles—creatures INTUITION PUMP Section 26.2.",Constraint,3
2850,"The aspect we will focus on is that of subjective experience: why it is that it feels like something to have certain brain states (e.g., while eating a hamburger), whereas it presumably does not feel like anything to have other physical states (e.g., while being a rock).",Safety,4
2851,This explanatory gap has led some philosophers to conclude that humans are simply incapable of forming a proper understanding of their own consciousness.,Constraint,4
2852,"Philosophical Foundations 26.3 THE ETHICS AND RISKS OF DEVELOPING ARTIFICIAL INTELLIGENCE So far, we have concentrated on whether we can develop AI, but we must also consider whether we should.",Safety,4
2853,"All scientists and engineers face ethical considerations of how they should act on the job, what projects should or should not be done, and how they should be handled.",Safety,4
2854,• AI systems might be used toward undesirable ends.,Constraint,1
2855,• The use of AI systems might result in a loss of accountability.,Safety,1
2856,We will look at each issue in turn.,Constraint,4
2857,"Now that the canonical AI program is an “intelligent agent” designed to assist a human, loss of jobs is less of a concern than it was when AI focused on “expert systems” designed to replace humans.",Interface,1
2858,It is not out of the way to predict that it will be slashed in half again by 2000.” Arthur C.,Safety,4
2859,"Instead, people working in knowledge-intensive industries have found themselves part of an integrated computerized system that operates 24 hours a day; to keep up, they have been forced to work longer hours.",Interface,1
2860,"Humanity has survived other setbacks to our sense of uniqueness: De Revolutionibus Orbium Coelestium (Copernicus, 1543) moved the Earth away from the center of the solar system, and Descent of Man (Darwin, 1871) put Homo sapiens at the same level as other species.",Safety,1
2861,AI systems might be used toward undesirable ends.,Constraint,1
2862,Autonomous AI systems are now commonplace on the battleﬁeld; the U.S.,Safety,1
2863,He didn’t foresee a world with terrorist threats that would change the balance of how much surveillance people are willing to 1036 Chapter 26.,Constraint,4
2864,Some accept that computerization leads to a loss of privacy—Sun Microsystems CEO Scott McNealy has said “You have zero privacy anyway.,Constraint,1
2865,The use of AI systems might result in a loss of accountability.,Safety,1
2866,"When a physician relies on the judgment of a medical expert system for a diagnosis, who is at fault if the diagnosis is wrong?",Safety,1
2867,"The question should therefore be “Who is at fault if the diagnosis is unreasonable?” So far, courts have held that medical expert systems play the same role as medical textbooks and reference books; physicians are responsible for understanding the rea- soning behind any decision and for using their own judgment in deciding whether to accept the system’s recommendations.",Safety,1
2868,"In designing medical expert systems as agents, therefore, the actions should be thought of not as directly affecting the patient but as inﬂuencing the physician’s behavior.",Safety,1
2869,"If expert systems become reliably more accurate than human diagnosti- cians, doctors might become legally liable if they don’t use the recommendations of an expert system.",Interface,1
2870,"In California law, at least, there do not seem to be any legal sanctions to prevent an automated vehicle from exceeding the speed limits, although the designer of the vehicle’s control mechanism would be liable in the case of an accident.",Interface,4
2871,The question is whether an AI system poses a bigger risk than traditional software.,Interface,1
2872,We will look at three sources of risk.,Constraint,4
2873,"First, the AI system’s state estimation may be incorrect, causing it to do the wrong thing.",Safety,1
2874,"More seriously, a missile defense system might erroneously detect an attack and launch a counterattack, leading to the death of billions.",Interface,1
2875,These risks are not really risks of AI systems—in both cases the same mistake could just as easily be made by a human as by a computer.,Safety,1
2876,The correct way to mitigate these risks is to design a system with checks and balances so that a single state-estimation error does not propagate through the system unchecked.,Constraint,1
2877,"Second, specifying the right utility function for an AI system to maximize is not so easy.",Functional,1
2878,"For example, we might propose a utility function designed to minimize human suffering, expressed as an additive reward function over time as in Chapter 17.",Functional,3
2879,"Given the way humans are, however, we’ll always ﬁnd a way to suffer even in paradise; so the optimal decision for the AI system is to terminate the human race as soon as possible—no humans, no suffering.",Safety,1
2880,"With AI systems, then, we need to be very careful what we ask for, whereas humans would have no trouble realizing that the proposed utility function cannot be taken literally.",Functional,1
2881,"The machines we build need not be innately aggressive, unless we decide to build them that way (or unless they emerge as the end product of a mechanism design that encourages aggressive behavior).",Safety,4
2882,"Third, the AI system’s learning function may cause it to evolve into a system with unintended behavior.",Functional,1
2883,"This scenario is the most serious, and is unique to AI systems, so we will cover it in more depth.",Safety,1
2884,"Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind.",Interface,4
2885,"Thus the ﬁrst ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.",Safety,4
2886,"Shortly after, the human era will be ended.” Good and Vinge (and many others) correctly note that the curve of technological progress (on many measures) is growing exponentially at present (consider Moore’s Law).",Safety,4
2887,"However, it is a leap to extrapolate that the curve will continue to a singularity of near-inﬁnite growth.",Safety,4
2888,"If the problem of deﬁning ultraintelligent machines (or even approximations to them) happens to fall in the class of, say, NEXPTIME-complete problems, and if there are no heuristic shortcuts, then even exponential progress in technology won’t help—the speed of light puts a strict upper bound on how much computing can be done; problems beyond that limit will not be solved.",Safety,4
2889,"Ray Kurzweil is currently the most visible advocate for the singularity view, writing in The Singularity is Near (2005): The Singularity will allow us to transcend these limitations of our biological bodies and brain.",Safety,4
2890,We will gain power over our fates.,Constraint,4
2891,Our mortality will be in our own hands.,Constraint,4
2892,We will be able to live as long as we want (a subtly different statement from saying we will live forever).,Constraint,4
2893,We will fully understand human thinking and will vastly extend and expand its reach.,Constraint,4
2894,"By the end of this century, the nonbiological portion of our intelligence will be trillions of trillions of times more powerful than unaided human intelligence.",Safety,4
2895,"Kurzweil also notes the potential dangers, writing “But the Singularity will also amplify the ability to act on our destructive inclinations, so its full story has not yet been written.” If ultraintelligent machines are a possibility, we humans would do well to make sure that we design their predecessors in such a way that they design themselves to treat us well.",Safety,4
2896,"A robot must obey orders given to it by human beings, except where such orders would conﬂict with the First Law.",Constraint,4
2897,A robot must protect its own existence as long as such protection does not conﬂict with the First or Second Law.,Constraint,4
2898,"These laws seem reasonable, at least to us humans.6 But the trick is how to implement these laws.",Safety,4
2899,"That means that the negative utility for harm to a human must be much greater than for disobeying, but that each of the utilities is ﬁnite, not inﬁnite.",Safety,4
2900,Yudkowsky (2008) goes into more detail about how to design a Friendly AI.,Constraint,4
2901,"He asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be ﬂawed, and that the robot will learn and evolve over time.",Safety,4
2902,"Thus the challenge is one of mechanism design—to deﬁne a mechanism for evolving AI systems under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes.",Safety,1
2903,"For example, if technology had allowed us to design a super-powerful AI agent in 1800 and endow it with the prevailing morals of the time, it would be ﬁghting today to reestablish slavery and abolish women’s right to vote.",Constraint,4
2904,"But human brains are primitive compared to my powers, so it must be moral for me to kill humans.” Omohundro (2008) hypothesizes that even an innocuous chess program could pose a risk to society.",Interface,4
2905,"Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.",Interface,4
2906,"6 A robot might notice the inequity that a human is allowed to kill another in self-defense, but a robot is required to sacriﬁce its own life to save a human.",Safety,4
2907,Philosophical Foundations We should note that the idea of safeguards against change in utility function is not a new one.,Functional,3
2908,It is interesting to think how similar safeguards could be built into AI systems.,Constraint,1
2909,"Few AI researchers pay attention to the Turing Test, preferring to concentrate on their systems’ performance on practical tasks, rather than the ability to imitate humans.",Performance,1
2910,"Such consid- erations lead inevitably to the conclusion that we must weigh carefully, and soon, the possible consequences of AI research.",Safety,4
2911,"Gilbert Harman (1983), in analyzing belief revision, makes connections with AI research on truth maintenance systems.",Constraint,1
2912,"He explicitly conceived of animals as automata, and he anticipated the Turing Test, writing “it is not conceivable [that a machine] should produce different arrangements of words so as to give an appropriately meaningful answer to whatever is said in its presence, as even the dullest of men can do” (Descartes, 1637).",Interface,4
2913,"Frank Jack- son (1982) designed an inﬂuential thought experiment involving Mary, a color scientist who has been brought up in an entirely black-and-white world.",Interface,4
2914,"Exercises EXERCISES 1043 26.1 Go through Turing’s list of alleged “disabilities” of machines, identifying which have been achieved, which are achievable in principle by a program, and which are still problem- atic because they require conscious mental states.",Constraint,4
2915,"Can the skeptic reasonably object that this would require updating those neurophysiological properties of the neurons relating to conscious experience, as distinct from those involved in the functional behavior of the neurons?",Safety,3
2916,"He also wrote, in a letter to Philip Davis, that one of the central dreams of computer science is that “through the performance of computers and their programs we will remove all doubt that there is only a chemical distinction between the living and nonliving world.” To what extent does the progress made so far in artiﬁcial intelligence shed light on these issues?",Performance,4
2917,"Suppose that at some future date, the AI endeavor has been completely success- ful; that is, we have build intelligent agents capable of carrying out any human cognitive task at human levels of ability.",Safety,4
2918,"Good claims that intelligence is the most important quality, and that building 26.7 ultraintelligent machines will change everything.",Safety,4
2919,"In Chapter 2, we suggested that it would be helpful to view the AI task as that of designing rational agents—that is, agents whose actions maximize their expected utility given their percept histories.",Safety,4
2920,"We showed that the design problem depends on the percepts and actions available to the agent, the utility function that the agent’s behavior should satisfy, and the nature of the environment.",Functional,3
2921,"A variety of different agent designs are possible, ranging from reﬂex agents to fully deliberative, knowledge-based, decision-theoretic agents.",Constraint,4
2922,"Moreover, the components of these designs can have a number of different instantiations—for example, logical or probabilistic reasoning, and atomic, factored, or structured representations of states.",Safety,4
2923,"For all the agent designs and components, there has been tremendous progress both in our scientiﬁc understanding and in our technological capabilities.",Safety,4
2924,"In this chapter, we stand back from the details and ask, “Will all this progress lead to a general-purpose intelligent agent that can perform well in a wide variety of environments?” Section 27.1 looks at the components of an intelligent agent to assess what’s known and what’s missing.",Functional,4
2925,Section 27.3 asks whether designing rational agents is the right goal in the ﬁrst place.,Safety,4
2926,27.1 AGENT COMPONENTS Chapter 2 presented several agent designs and their components.,Constraint,4
2927,"To focus our discussion here, we will look at the utility-based agent, which we show again in Figure 27.1.",Safety,4
2928,"When en- dowed with a learning component (Figure 2.15), this is the most general of our agent designs.",Constraint,4
2929,"Agent Components 1045 State How the world evolves What my actions do Utility Agent Sensors What the world is like now What it will be like if I do action A How happy I will be in such a state What action I should do now Actuators E n v i r o n m e n t Figure 27.1 A model-based, utility-based agent, as ﬁrst presented in Figure 2.14.",Safety,3
2930,while robotic systems focused on low-level tasks in which high-level reasoning and plan- ning were largely absent.,Constraint,1
2931,This was due in part to the great expense and engineering effort required to get real robots to work at all.,Safety,4
2932,"MEMS (micro-electromechanical systems) technology has supplied miniaturized accelerometers, gy- roscopes, and actuators for an artiﬁcial ﬂying insect (Floreano et al., 2009).",Interface,1
2933,"Thus, we see that AI systems are at the cusp of moving from primarily software-only systems to embedded robotic systems.",Safety,1
2934,Keeping track of the state of the world: This is one of the core capabilities required for an intelligent agent.,Interface,4
2935,It requires both perception and updating of internal representations.,Constraint,4
2936,We expect that the application of these ideas for tracking complex environments will yield huge beneﬁts.,Safety,1
2937,It is possible that a new focus on probabilistic rather than logical representation coupled with aggressive machine learning (rather than hand- encoding of knowledge) will allow for progress.,Constraint,4
2938,"Projecting, evaluating, and selecting future courses of action: The basic knowledge- representation requirements here are the same as for keeping track of the world; the primary difﬁculty is coping with courses of action—such as having a conversation or a cup of tea— that consist eventually of thousands or millions of primitive steps for a real agent.",Constraint,4
2939,"As yet, however, there has been very little work on constructing realistic utility functions—imagine, for example, the complex web of interacting preferences that must be understood by an agent operating as an ofﬁce assistant for a human being.",Interface,3
2940,"In computer vision, for example, learning complex concepts such as Classroom and Cafeteria would be made unnecessarily difﬁcult if the agent were forced to work from pixels as the input representation; instead, the agent needs to be able to form intermediate concepts ﬁrst, such as Desk and Tray , without explicit human supervision.",Safety,4
2941,"Perhaps this will incorporate some of the ideas of deep belief networks—Bayesian networks that have multiple layers of hidden variables, as in the work of Hinton et al.",Safety,4
2942,"Learning researchers will need to adapt their very successful techniques for factored repre- sentations to structured representations, particularly hierarchical representations.",Constraint,4
2943,"27.2 AGENT ARCHITECTURES It is natural to ask, “Which of the agent architectures in Chapter 2 should an agent use?” The answer is, “All of them!” We have seen that reﬂex responses are needed for situations in which time is of the essence, whereas knowledge-based deliberation allows the agent to plan ahead.",Interface,4
2944,"A complete agent must be able to do both, using a hybrid architecture.",Constraint,4
2945,"A less studied problem is the reversal of this process: when the environment changes, learned reﬂexes may no longer be appropriate and the agent must return to the deliberative level to produce new behaviors.",Safety,4
2946,"They must be able to cease deliberating when action is demanded, and they must be able to use the time available for deliberation to execute the most proﬁtable computations.",Safety,4
2947,"For example, a taxi-driving agent that sees an accident ahead must decide in a split second either to brake or to take evasive action.",Interface,4
2948,"It should also spend that split second thinking about the most important questions, such as whether the lanes to the left and right are clear and whether there is a large truck close behind, rather than worrying about wear and tear on the tires or where to pick up the next passenger.",Safety,4
2949,"AI: The Present and Future Knowledge-based deliberation C o m p i l a t i o n ANYTIME ALGORITHM DECISION THEORETIC METAREASONING REFLECTIVE ARCHITECTURE Percepts Reflex system Actions Figure 27.2 cient, reﬂexive mechanisms.",Safety,1
2950,"Compilation serves to convert deliberative decision making into more efﬁ- systems move into more complex domains, all problems will become real-time, because the agent will never have long enough to solve the decision problem exactly.",Safety,1
2951,Metareasoning techniques can be used to design better search algorithms and to guarantee that the algorithms have the anytime prop- erty.,Safety,3
2952,"Metalevel reinforcement learning may provide another way to acquire effective policies for controlling deliberation: in essence, computations that lead to better decisions are reinforced, while those that turn out to have no effect are penalized.",Interface,4
2953,"Metareasoning is one speciﬁc example of a reﬂective architecture—that is, an archi- tecture that enables deliberation about the computational entities and actions occurring within the architecture itself.",Interface,4
2954,Decision-making and learning algorithms can be designed that operate over this joint state space and thereby serve to implement and improve the agent’s compu- tational activities.,Safety,3
2955,"Eventually, we expect task-speciﬁc algorithms such as alpha–beta search and backward chaining to disappear from AI systems, to be replaced by general methods that direct the agent’s computations toward the efﬁcient generation of high-quality decisions.",Safety,1
2956,"For most of the book, however, we will adopt the working hypothesis that perfect rationality is a good starting point for analysis.",Safety,4
2957,"This is an interesting property for a system to exhibit, but in most environments, the right answer at the wrong time is of no value.",Interface,1
2958,"In practice, AI system designers are forced to compromise on decision quality to obtain reason- able overall performance; unfortunately, the theoretical basis of calculative rationality does not provide a well-founded way to make such compromises.",Performance,1
2959,"He wrote, The capacity of the human mind for formulating and solving complex problems is very small compared with the size of the problems whose solution is required for objectively rational behavior in the real world—or even for a reasonable approximation to such ob- jective rationality.",Constraint,4
2960,"If the problems imposed by the constraints are minor, one would expect the ﬁnal design to be similar to a BO agent design.",Constraint,4
2961,"But as the resource constraints become more critical—for example, as the environment becomes more complex—one would expect the two designs to diverge.",Constraint,4
2962,"If there is to be a constructive theory of bounded optimality, we have to hope that the design of bounded op- timal programs does not depend too strongly on the details of the computer being used.",Safety,4
2963,It would make scientiﬁc research very difﬁcult if adding a few kilobytes of memory to a giga- byte machine made a signiﬁcant difference to the design of the BO program.,Safety,4
2964,"We can hazard a guess that BO or ABO programs for powerful computers in complex environments will not necessarily have a simple, elegant structure.",Constraint,4
2965,We have already seen that general-purpose intelligence requires some reﬂex capability and some deliberative capability; a variety of forms of knowledge and decision making; learning and compilation mechanisms for all of those forms; methods for controlling reasoning; and a large store of domain-speciﬁc knowledge.,Constraint,3
2966,"A bounded optimal agent must adapt to the environment in which it ﬁnds itself, so that eventually its internal organization will reﬂect optimizations that are speciﬁc to the particular environment.",Safety,4
2967,"This is only to be expected, and it is similar to the way in which racing cars restricted by engine capacity have evolved into extremely complex designs.",Safety,4
2968,1051 suspect that a science of artiﬁcial intelligence based on bounded optimality will involve a good deal of study of the processes that allow an agent program to converge to bounded optimality and perhaps less concentration on the details of the messy programs that result.,Interface,4
2969,"Actions are, after all, generated by programs, and it is over programs that designers have control.",Constraint,4
2970,"Intelligent computers are more powerful than dumb ones, but will that power be used for good or ill?",Constraint,4
2971,The scope of the impact will depend on the degree of success of AI.,Safety,4
2972,"AI has made possible new applications such as speech recognition systems, inventory control systems, surveillance systems, robots, and search engines.",Constraint,1
2973,"AI systems at this level of capability could threaten hu- man autonomy, freedom, and even survival.",Constraint,1
2974,AI: The Present and Future Which way will the future go?,Safety,4
2975,A MATHEMATICAL BACKGROUND A.1 COMPLEXITY ANALYSIS AND O() NOTATION BENCHMARKING ANALYSIS OF ALGORITHMS Computer scientists are often faced with the task of comparing algorithms to see how fast they run or how much memory they require.,Constraint,3
2976,"From the single result that the benchmark provides, it can be difﬁcult to predict how well the algorithm would do on a different compiler, com- puter, or data set.",Safety,3
2977,"The second approach relies on a mathematical analysis of algorithms, independently of the particular implementation and input, as discussed below.",Safety,3
2978,"A.1.1 Asymptotic analysis We will consider algorithm analysis through the following example, a program to compute the sum of a sequence of numbers: function SUMMATION(sequence) returns a number sum ← 0 for i = 1 to LENGTH(sequence) do sum ← sum + sequence[i] return sum The ﬁrst step in the analysis is to abstract over the input, in order to ﬁnd some parameter or parameters that characterize the size of the input.",Functional,3
2979,"In this example, the input can be charac- terized by the length of the sequence, which we will call n.",Safety,4
2980,"The second step is to abstract over the implementation, to ﬁnd some measure that reﬂects the running time of the algorithm but is not tied to a particular compiler or computer.",Safety,3
2981,We will call this characterization T (n).,Constraint,4
2982,Computing an average means that the analyst must assume some distribution of inputs.,Interface,4
2983,"For example, an O(n2) algorithm will always be worse than an O(n) in the long run, but if the two algorithms are T (n2 + 1) and T (100n + 1000), then the O(n2) algorithm is actually better for n < 110.",Interface,3
2984,"But it also contains those with time O(n1000), so the name “easy” should not be taken too literally.",Constraint,4
2985,"Another class is the class of PSPACE problems—those that require a polynomial amount of space, even on a nondeterministic machine.",Safety,4
2986,"A.2 VECTORS, MATRICES, AND LINEAR ALGEBRA VECTOR Mathematicians deﬁne a vector as a member of a vector space, but we will use a more con- crete deﬁnition: a vector is an ordered sequence of values.",Interface,4
2987,−1 such that A Matrices are used to solve systems of linear equations in O(n3) time; the time is domi- nated by inverting a matrix of coefﬁcients.,Safety,1
2988,"Probability Distributions 1057 We can represent this system as the matrix equation A x = b, where A =   2 −3 −1 1 −2 1 −1 2 2   , x =   x y z   , b =   8 −11 −3   .",Safety,1
2989,Mathematical background The density function must be nonnegative for all x and must have ∞ Z P (x) dx = 1 .,Functional,3
2990,"In one dimension, we can deﬁne the cumulative distribution function F (x) as the probability that a random variable will be less than x.",Functional,3
2991,"Classic works on the analysis and design of algorithms include those by Knuth (1973) and Aho, Hopcroft, and Ullman (1974); more recent contributions are by Tarjan (1983) and Cormen, Leiserson, and Rivest (1990).",Safety,3
2992,These books place an emphasis on designing and analyzing algorithms to solve tractable problems.,Interface,3
2993,"Most of the pseudocode should be familiar to users of languages like Java, C++, or Lisp.",Safety,4
2994,A few idiosyncrasies should be noted.,Constraint,4
2995,"Programs with persistent variables can be implemented as objects in object-oriented languages such as C++, Java, Python, and Smalltalk.",Constraint,4
2996,"In functional languages, they can be implemented by functional closures over an environment containing the required variables.",Interface,3
2997,"• Destructuring assignment: The notation “x , y ← pair ” means that the right-hand side must evaluate to a two-element tuple, and the ﬁrst element is assigned to x and the second to y.",Safety,4
2998,"B.3 ONLINE HELP Most of the algorithms in the book have been implemented in Java, Lisp, and Python at our online code repository: aima.cs.berkeley.edu The same Web site includes instructions for sending comments, corrections, or suggestions for improving the book, and for joining discussion lists.",Safety,3
2999,A knowledge-based planning and scheduling system for spacecraft AIV.,Safety,1
3000,The Design and Analysis of Computer Algorithms.,Safety,3
3001,"IEEE Intelligent Systems, 19(1), 8–12.",Constraint,1
3002,"Dynamic Systems, Measurement, and Control, 97, 270–277.",Constraint,1
3003,"Intelligent Systems, 6, 341–355.",Constraint,1
3004,"(Eds.), Systems and Computer Science.",Constraint,1
3005,HUGIN—A shell for building Bayesian belief universes for expert systems.,Constraint,1
3006,"Franklin Institute, servable Markov systems.",Constraint,1
3007,"ACM Transactions on Information Systems, 12, 233–251.",Constraint,1
3008,The Dragon system—An (1975).,Interface,1
3009,Magic sets and other strange ways to implement logic programs.,Constraint,4
3010,"IEEE Transac- tions on Systems, Man and Cybernetics, 13, 834– 846.",Constraint,1
3011,"(Ed.), Neural Information Pro- cessing Systems, pp.",Constraint,1
3012,"IEEE Intelli- gent Systems, 24(4), 21–23.",Constraint,1
3013,"Invited paper presented at the IEEE Systems Science and Cybernetics Conference, Miami.",Safety,1
3014,Handbook on Schedul- ing: Models and Methods for Advanced Planning (International Handbooks on Information Systems).,Constraint,1
3015,"In Neural Information Processing Systems, Vol.",Constraint,1
3016,Natural language input for a computer problem solving system.,Safety,1
3017,"GUS, a frame driven dialog system.",Safety,1
3018,Navigating Mobile Robots: Systems and Techniques.,Constraint,1
3019,Dis- covering the hidden structure of complex dynamic systems.,Safety,1
3020,From one to many: Planning for loosely coupled multi-agent systems.,Constraint,1
3021,"IEEE Transactions on Systems, Man and Cybernetics, 15(2), 224–233.",Constraint,1
3022,Programming expert systems in OPS5: An introduction to rule-based programming.,Interface,1
3023,"(Eds.), Pattern- Directed Inference Systems, pp.",Constraint,1
3024,Models of learning systems.,Constraint,1
3025,Rule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuristic Programming Project.,Safety,1
3026,Stochas- tic Hybrid Systems.,Constraint,1
3027,"IEEE Intelligent Systems, 22(4), 12–19.",Constraint,1
3028,"Computer and System Sciences, 21(2), 156–178.",Safety,1
3029,Improving the design of intelligent acquisition interfaces for col- lecting world knowledge from web contributors.,Safety,2
3030,"Principles of Robotic Motion: Theory, Algorithms, and Implementation.",Constraint,3
3031,Probabilistic Networks and Ex- pert Systems.,Constraint,1
3032,"IEEE Transactions on Systems, Man and Cybernet- ics (SMC), 11.",Constraint,1
3033,"Mathematics of Con- trols, Signals, and Systems, 2, 303–314.",Constraint,1
3034,Knowledge- Based Systems in Artiﬁcial Intelligence.,Constraint,1
3035,Theoretical Neu- roscience: Computational and Mathematical Mod- eling of Neural Systems.,Constraint,1
3036,The space shuttle ground processing scheduling system.,Safety,1
3037,Coping with uncertainty in a control system for navigation and exploration.,Constraint,1
3038,An overview of the FRUMP system.,Interface,1
3039,"Feynman–Kac Formulae, Ge- nealogical and Interacting Particle Systems with Ap- plications.",Constraint,1
3040,Modern Con- trol Systems (10th edition).,Constraint,1
3041,A truth maintenance system.,Safety,1
3042,"Intelligent Systems, 9(1), 61–100.",Constraint,1
3043,Model design in the Prospector consultant system for mineral exploration.,Safety,1
3044,"(Ed.), Ex- pert Systems in the Microelectronic Age, pp.",Safety,1
3045,"The KR system dlv: Progress report, comparisons and benchmarks.",Safety,1
3046,"S., Draper, D., Lesh, N., and Williamson, M.",Constraint,4
3047,A planning system for robot construction tasks.,Safety,1
3048,In Discrete event modeling and simulation technologies: a tapestry of systems and AI-based theories and methodologies.,Constraint,1
3049,"(Ed.), Qualitative Reasoning About Physical Systems, pp.",Constraint,1
3050,"General Systems Yearbook, 4, 171–184.",Constraint,1
3051,"(Eds.), AI-based Mobile Robots: Case Studies of Successful Robot Systems, pp.",Constraint,1
3052,The use of design de- scriptions in automated diagnosis.,Safety,4
3053,S¨atze der Principia mathematica und verwandter Systeme I.,Constraint,1
3054,Theorem-proving by resolu- tion as a basis for question-answering systems.,Constraint,1
3055,The use of theorem-proving techniques in question-answering systems.,Safety,1
3056,"IEEE Intelligent Systems, March/April, 8–12.",Constraint,1
3057,"ACM Transac- tions on Information and System Security, 11(4).",Safety,1
3058,"IEEE Transactions on Systems Sci- ence and Cybernetics, SSC-4(2), 100–107.",Constraint,1
3059,"(Ed.), Expert Systems in the Microelec- tronic Age.",Safety,1
3060,The fast downward planning system.,Safety,1
3061,Hy- brid systems: Computation and control.,Constraint,1
3062,"Complex Systems, 1(3), 495–502.",Constraint,1
3063,FF: The fast-forward planning system.,Safety,1
3064,The FF plan- ning system: Fast plan generation through heuristic search.,Safety,1
3065,Adaption in Natural and Ar- tiﬁcial Systems.,Constraint,1
3066,Functional programming of behavior-based systems.,Constraint,1
3067,"Problem-solving design: Rea- soning about computational value, trade-offs, and re- sources.",Constraint,4
3068,Decision theory in expert systems and artiﬁcial intel- ligence.,Constraint,1
3069,"IEEE Transactions on Systems Science and Cyber- netics, SSC-2, 22–26.",Constraint,1
3070,Conference on Multi-Agent Systems (ICMAS-2000).,Constraint,1
3071,The design of mechanisms for resource allocation.,Safety,4
3072,The CLP(R) language and system.,Safety,1
3073,"ACM Transactions on Programming Languages and Systems, 14(3), 339–395.",Constraint,1
3074,Artiﬁcial Intelligence: A Systems Approach.,Constraint,1
3075,Neural Network Design and the Complexity of Learning.,Safety,4
3076,"Robotics and Autonomous Systems, 6(1–2), 35–48.",Constraint,1
3077,Verbmobil: A Translation System for Face-To-Face Dialog.,Safety,1
3078,A computational model for combined causal and diagnostic reasoning in inference systems.,Constraint,1
3079,What really matters in auc- tion design.,Constraint,4
3080,"Mathematical Systems Theory, 2(2), 127– 145.",Constraint,1
3081,Probabilistic frame-based systems.,Constraint,1
3082,A ﬁrst order formalization of knowledge and action for a multi-agent planning system.,Safety,1
3083,"(Ed.), Qualitative Reasoning About Physi- cal Systems, pp.",Constraint,1
3084,"EURISKO: A program that learns new heuristics and domain concepts: The na- ture of heuristics, III: Program design and results.",Safety,4
3085,"Bell Systems Technical Journal, 44(10), 2245–2269.",Constraint,1
3086,Building Large Knowledge-Based Systems: Representation and In- ference in the CYC Project.,Safety,1
3087,On a measure of the infor- mation provided by an experiment.,Interface,4
3088,Local computations with probabilities on graphical struc- tures and their application to expert systems.,Constraint,1
3089,Sequential Monte Carlo methods for dynamic systems.,Constraint,1
3090,"Fourth International Conference on Evolvable Systems, pp.",Constraint,1
3091,The HARPY Speech Recog- nition System.,Safety,1
3092,The HARPY speech recognition system.,Safety,1
3093,Knowledge acquisition for decision-theoretic expert systems.,Constraint,1
3094,"Technical report LIDS-P-2411, Laboratory for Infor- mation and Decision Systems, Massachusetts Insti- tute of Technology.",Constraint,1
3095,R1: A rule-based conﬁgurer of computer systems.,Constraint,1
3096,"IEEE Intelligent Systems, 16(2), 46–53.",Constraint,1
3097,The multi-purpose incremental learn- ing system AQ15 and its testing application to three medical domains.,Safety,1
3098,"A System of Logic, Ratiocinative and Inductive: Being a Connected View of the Prin- ciples of Evidence, and Methods of Scientiﬁc Inves- tigation.",Safety,1
3099,When will a genetic algorithm outperform hill climbing?,Constraint,3
3100,"38th Design Automation Conference (DAC 2001), pp.",Constraint,4
3101,"S., Williamson, J., and Willighagen, E.",Constraint,4
3102,"Muscettola, N., Nayak, P., Pell, B., and Williams, B.",Constraint,4
3103,Remote agent: To boldly go where no AI system has gone before.,Safety,1
3104,Optimal auction design.,Constraint,4
3105,Smodels: A system for answer set program- ming.,Safety,1
3106,Learning Machines: Foun- dations of Trainable Pattern-Classifying Systems.,Constraint,1
3107,The design of a high- performance cache controller: A case study in asyn- chronous synthesis.,Performance,4
3108,A systematic compar- ison of various statistical alignment model.,Constraint,1
3109,Systems That Learn: An Introduction to Learning Theory for Cognitive and Computer Sci- entists.,Interface,1
3110,Developing Intelligent Agent Systems: A Practical Guide.,Constraint,1
3111,On the design of behavior- based multi-robot teams.,Safety,4
3112,Probabilistic Reasoning in Intelli- gent Systems: Networks of Plausible Inference.,Constraint,1
3113,"Complex Systems, 1(5), 995–1019.",Constraint,1
3114,SPOOK: A system for probabilistic In UAI- object-oriented knowledge representation.,Safety,1
3115,Probabilistic Reasoning for Complex Systems.,Constraint,1
3116,The design and implementation of IBAL: A general-purpose probabilistic language.,Safety,4
3117,"Scheduling: Theory, Algorithms, and Systems.",Constraint,1
3118,A design for an understand- ing machine.,Interface,4
3119,"(Ed.), Expert Systems in the Microelectronic Age.",Safety,1
3120,A library hierarchy for implementing scal- able parallel search algorithms.,Constraint,3
3121,Maximum likelihood estimates of linear dynamic systems.,Constraint,1
3122,Knowledge in Action: Logical Foundations for Specifying and Implementing Dy- namical Systems.,Constraint,1
3123,Genetic algo- rithms in computer aided design.,Constraint,4
3124,"Computer Aided Design, 35(8), 709–726.",Constraint,4
3125,The design and implementation of VAMPIRE.,Safety,4
3126,"E., and Williams, R.",Constraint,4
3127,"E., and Williams, R.",Constraint,4
3128,On- line Q-learning using connectionist systems.,Constraint,1
3129,Quantita- tive evaluation of explanation-based learning as an optimization tool for a large-scale natural language system.,Interface,1
3130,Boostexter: A boosting-based system for text categorization.,Safety,1
3131,Classi- ﬁcation in the KL-ONE representation system.,Safety,1
3132,System Description: E 0.81.,Safety,1
3133,Shallow parsing with conditional random ﬁelds.,Constraint,4
3134,"Bell Systems Technical Journal, 27, 379–423, 623–656.",Constraint,1
3135,"Mul- tiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations.",Constraint,1
3136,"1998-014, Digital Systems Re- search Center.",Constraint,1
3137,"First International Conference on On- tologies, Databases, and Applications of Semantics for Large Scale Information Systems.",Constraint,1
3138,Do you have free will?,Constraint,4
3139,Forward reasoning and dependency-directed backtracking in a system for computer-aided circuit analysis.,Safety,1
3140,A prolog technology theorem prover: a new exposition and implementation in pro- log.,Constraint,4
3141,Layered Learning in Multi-Agent Systems: A Winning Approach to Robotic Soccer.,Constraint,1
3142,Sketchpad: A man-machine graphical communication system.,Safety,1
3143,"IEEE Trans- actions on Systems, Man and Cybernetics, 20(2), 365–379.",Constraint,1
3144,"Om anvendelse af mindste kvadraters methode i nogle tilfælde, hvor en kom- plikation af visse slags uensartede tilfældige fejlk- ilder giver fejlene en ‘systematisk’ karakter.",Constraint,1
3145,Implementation of logical query languages for databases.,Constraint,4
3146,"ACM Transactions on Database Systems, 10(3), 289–321.",Constraint,1
3147,The design and experimental analysis of algorithms for temporal reasoning.,Safety,3
3148,"Design, implementation, and evaluation of the constraint language cc(FD).",Safety,4
3149,Economic mechanism design for computerized agents.,Constraint,4
3150,A Concise Introduction to Multi- agent Systems and Distributed Artiﬁcial Intelligence.,Constraint,1
3151,WARPLAN: A System for Generating Plans.,Safety,1
3152,Readings in Qualitative Reasoning about Physical Systems.,Constraint,1
3153,"General Systems Yearbook, 22, 25–38.",Constraint,1
3154,Williams and Northgate.,Constraint,4
3155,"In Self- Organizing Systems 1962, pp.",Constraint,1
3156,A complete algorithm for designing passive fences to orient parts.,Constraint,3
3157,"Williams, B., Ingham, M., Chung, S., and Elliott, P.",Constraint,4
3158,Model-based programming of intelligent embedded systems and robotic space explorers.,Constraint,1
3159,"IEEE: Special Issue on Modeling and Design of Embedded Software, pp.",Constraint,4
3160,Au- tomated design of multiple-valued logic circuits by automated theorem proving techniques.,Constraint,4
3161,An Introduction to MultiA- gent Systems.,Interface,1
3162,"Wos, L., Robinson, G., Carson, D., and Shalla, L.",Constraint,4
3163,KAM: A System for Intelli- gently Guiding Numerical Experimentation by Com- puter.,Safety,1
3164,"Fuzzy Sets and Systems, 1, 3–28.",Constraint,1
3165,"A systematic comparison of phrase-based, hierarchical and syntax-augmented statistical MT.",Constraint,1
3166,"G., 60, 1068 Casteran, P., 359, 1066 Castro, R., 553, 1068 categorization, 865 category, 440, 440–445, 453 causal network, see Bayesian network causal probability, 496 causal rule, 317, 517 causation, 246, 498 caveman, 778 Cazenave, T., 194, 1066 CCD (charge-coupled device), 930, 969 cell decomposition, 986, 989 exact, 990 cell layout, 74 center (in mechanism design), 679 central limit theorem, 1058 cerebral cortex, 11 certainty effect, 620 certainty equivalent, 618 certainty factor, 23, 548, 557 Cesa-Bianchi, N., 761, 1068 1100 Cesta, A., 28, 1068 CGP, 433 CHAFF, 277 Chaﬁn, B., 28, 1064 chain rule (for differentiation), 726 chain rule (for probabilities), 514 Chakrabarti, P.",Constraint,4
3167,"C., 1024, 1032, 1033, 1042, 1070 Denney, E., 360, 1071 density estimation, 806 nonparametric, 814 DeOliveira, J., 469, 1081 depth-ﬁrst search, 85, 85–87, 108, 408 DEPTH-LIMITED-SEARCH, 88 depth limit, 173 depth of ﬁeld, 932 derivational analogy, 799 derived sentences, 242 Descartes, R., 6, 966, 1027, 1041, 1071 descendant (in Bayesian networks), 517 Descotte, Y., 432, 1071 description logic, 454, 456, 456–458, 468, 471 descriptive theory, 619 detachment, 547 detailed balance, 537 detection failure (in data association), 602 determination, 784, 799, 801 minimal, 787 deterministic environment, 43 deterministic node, 518 Detwarasiti, A., 639, 1071 Deville, Y., 228, 1091 DEVISER, 431 Devroye, L., 827, 1071 Dewey Decimal system, 440 de Bruin, A., 191, 1085 de Dombal, F.",Safety,1
3168,"F., 111, 1071 Dimopoulos, Y., 395, 1078 Dinh, H., 111, 1071 Diophantine equations, 227 Diophantus, 227 Diorio, C., 604, 1086 DiPasquo, D., 885, 1069 Diplomacy, 166 directed acyclic graph (DAG), 511, 552 1103 directed arc consistency, 223 direct utility estimation, 853 Dirichlet distribution, 811 Dirichlet process, 827 disabilities, 1043 disambiguation, 904–912, 919 discontinuities, 936 discount factor, 649, 685, 833 discovery system, 800 discrete event, 447 discretization, 131, 519 discriminative model, 878 disjoint sets, 441 disjunct, 244 disjunction, 244 disjunctive constraint, 205 disjunctive normal form, 283 disparity, 949 Dissanayake, G., 1012, 1071 distant point light source, 934 distortion, 910 distribute ∨ over ∧, 254, 347 distributed constraint satisfaction, 230 distribution beta, 592, 811 conditional, nonparametric, 520 cumulative, 564, 623, 1058 mixture, 817 divide-and-conquer, 606 Dix, J., 472, 1067 DLV, 472 DNF (disjunctive normal form), 283 Do, M.",Safety,1
3169,"V., 885, 1071 Durrant-Whyte, H., 1012, 1071, 1080 Dyer, M., 23, 1071 dynamical systems, 603 dynamic backtracking, 229 dynamic Bayesian network (DBN), 566, 590, 590–599, 603, 604, 646, 664 dynamic decision network, 664, 685 dynamic environment, 44 dynamic programming, 60, 106, 110, 111, 342, 575, 685 adaptive, 834, 834–835, 853, 858 nonserial, 553 dynamic state, 975 dynamic weighting, 111 Dyson, G., 1042, 1071 dystopia, 1052 Duzeroski, S., 796, 800, 1071, 1078, 1080 E E, 359 E0 (English fragment), 890 Earley, J., 920, 1071 early stopping, 706 earthquake, 511 Eastlake, D.",Constraint,1
3170,"H., 1035, 1072 Frankenstein, 1037 Franz, A., 883, 921, 1072 Fratini, S., 28, 1068 FREDDY, 74, 156, 1012 Fredkin Prize, 192 Freeman, W., 555, 1091, 1092 free space, 988 free will, 6 Frege, G., 8, 276, 313, 357, 1072 Freitag, D., 877, 885, 1069, 1072 frequentism, 491 Freuder, E.",Constraint,4
3171,"C., 841, 855, 1074 Gittins index, 841, 855 Giunchiglia, E., 433, 1072 Givan, R., 857, 1090 Glanc, A., 1011, 1074 Glass, J., 604, 1080 GLAUBER, 800 Glavieux, A., 555, 1065 GLIE, 840 global constraint, 206, 211 Global Positioning System (GPS), 974 Glover, F., 154, 1074 Glymour, C., 314, 826, 1074, 1089 Go (game), 186, 194 goal, 52, 64, 65, 108, 369 based agent, 52–53, 59, 60 formulation of, 65 goal-based agent, 52–53, 59 goal-directed reasoning, 259 inferential, 301 serializable, 392 goal clauses, 256 goal monitoring, 423 goal predicate, 698 goal test, 67, 108 God, existence of, 504 G¨odel, K., 8, 276, 358, 1022, 1074 Goebel, J., 826, 1074 Goebel, R., 2, 59, 1085 Goel, A., 682, 1064 Goertzel, B., 27, 1074 GOFAI, 1024, 1041 gold, 237 Gold, B., 922, 1074 Gold, E.",Safety,1
3172,"A., 505, 1074 Gottlob, G., 230, 1074 Gotts, N., 473, 1069 GP-CSP, 390 GPS (General Problem Solver), 3, 7, 18, 393 GPS (Global Positioning System), 974 graceful degradation, 666 gradient, 131 empirical, 132, 849 gradient descent, 125, 719 batch, 720 stochastic, 720 Graham, S.",Safety,1
3173,"C., 505, 1070 horse, 1028 Horswill, I., 1013, 1076 Horvitz, E.",Constraint,4
3174,"A., 394, 432, 1068, 1073 KNOWITALL, 885 knowledge acquisition, 860 and action, 7, 453 background, 235, 349, 777, 1024, 1025 base (KB), 235, 274, 315 commonsense, 19 diagnostic, 497 engineering, 307, 307–312, 514 for decision-theoretic systems, 634 level, 236, 275 model-based, 497 prior, 39, 768, 778, 787 knowledge-based agents, 234 knowledge-based system, 22–24, 845 knowledge acquisition, 23, 307, 860 knowledge compilation, 799 knowledge map, see Bayesian network knowledge representation, 2, 16, 19, 24, 234, 285–290, 437–479 analogical, 315 everything, 437 language, 235, 274, 285 uncertain, 510–513 Knuth, D.",Safety,1
3175,"B., 27, 439, 469, 474, 800, 1070, 1075, 1080 lens system, 931 Lenstra, J.",Safety,1
3176,"T., 275, 1080 LOOCV, 708 Look ma, no hands, 18 lookup table, 736 Loomes, G., 637, 1086 loosely coupled system, 427 Lorenz, U., 193, 1071 loss function, 710 Lotem, A., 396, 1091 lottery, 612 standard, 615 love, 1021 Love, N., 195, 1080 Lovejoy, W.",Functional,1
3177,"E., 505, 884, 1081 Marr, D., 968, 1081 Marriott, K., 228, 1081 Marshall, A.",Constraint,4
3178,"L., 395, 1082 McNealy, S., 1036 McPhee, N., 156, 1085 MDL, 713, 759, 805 MDP, 10, 647, 684, 686, 830 mean-ﬁeld approximation, 554 measure, 444 measurement, 444 mechanism, 679 strategy-proof, 680 mechanism design, 679, 679–685 medical diagnosis, 23, 505, 517, 548, 629, 1036 Meehan, J., 358, 1068 Meehl, P., 1022, 1074, 1082 Meek, C., 553, 1092 M eet (interval relation), 448 Megarian school, 275 megavariable, 578 Meggido, N., 677, 687, 1078 Mehlhorn, K., 112, 1069 mel frequency cepstral coefﬁcient (MFCC), 915 Mellish, C.",Constraint,4
3179,"S., 359, 1068 memoization, 343, 357, 780 memory requirements, 83, 88 MEMS, 1045 Mendel, G., 130, 1082 meningitis, 496–508 mental model, in disambiguation, 906 mental objects, 450–453 mental states, 1028 Mercer’s theorem, 747 Mercer, J., 747, 1082 Mercer, R.",Constraint,4
3180,"S., 799, 1082 Michaylov, S., 359, 1077 Michie, D., 74, 110, 111, 156, 191, 763, 851, 854, 1012, 1071, 1082 micro-electromechanical systems (MEMS), 1045 micromort, 616, 637, 642 Microsoft, 553, 874 microworld, 19, 20, 21 Middleton, B., 519, 552, 1086 Miikkulainen, R., 435, 1067 Milch, B., 556, 639, 1078, 1082, 1085 Milgrom, P., 688, 1082 Milios, E., 1012, 1081 military uses of AI, 1035 Mill, J.",Constraint,1
3181,"J., 314, 1074 MIN-CONFLICTS, 221 min-conﬂicts heuristic, 220, 229 MIN-VALUE, 166, 170 mind, 2, 1041 dualistic view, 1041 and mysticism, 12 philosophy of, 1041 as physical system, 6 theory of, 3 mind–body problem, 1027 minesweeper, 283 MINIMAL-CONSISTENT-DET, 786 minimal model, 459 MINIMAX-DECISION, 166 minimax algorithm, 165, 670 minimax decision, 165 minimax search, 165–168, 188, 189 minimax value, 164, 178 minimum global, 121 local, 122 Index minimum-remaining-values, 216, 333 minimum description length (MDL), 713, 759, 805 minimum slack, 405 minimum spanning tree (MST), 112, 119 MINISAT, 277 Minker, J., 358, 473, 1073, 1082 Minkowski distance, 738 Minsky, M.",Safety,1
3182,"M., 61, 288, 763, 776, 798, 799, 884, 885, 1047, 1066, 1067, 1069, 1082, 1084 Mitra, M., 870, 1089 mixed strategy, 667 mixing time, 573 mixture distribution, 817 mixture distribution, 817 mixture of Gaussians, 608, 817, 820 Mizoguchi, R., 27, 1075 ML, see maximum likelihood modal logic, 451 model, 50, 240, 274, 289, 313, 451 causal, 517 (in representation), 13 sensor, 579, 586, 603 theory, 314 transition, 67, 108, 134, 162, 266, 566, 597, 603, 646, 684, 832, 979 MODEL-BASED-REFLEX-AGENT, 51 model-based reﬂex agents, 59 model checking, 242, 274 model selection, 709, 825 Modus Ponens, 250, 276, 356, 357, 361 Generalized, 325, 326 Moffat, A., 884, 1092 MOGO, 186, 194 Mohr, R., 210, 228, 968, 1082, 1088 Mohri, M., 889, 1082 Molloy, M., 277, 1064 monism, 1028 monitoring, 145 Index monkey and bananas, 397 monotone condition, 110 monotonicity of a heuristic, 95 of a logical system, 251, 458 of preferences, 613 Montague, P.",Safety,1
3183,"S., 761, 1083 Moutarlier, P., 1012, 1083 movies movies 2001: A Space Odyssey, 552 movies A.I., 1040 movies The Matrix, 1037 movies The Terminator, 1037 Mozetic, I., 799, 1082 MPI (mutual preferential independence), 625 MRS (metalevel reasoning system), 345 MST, 112, 119 Mueller, E.",Safety,1
3184,"B., 154, 229, 1082 Philo of Megara, 275 philosophy, 5–7, 59, 1020–1043 phone (speech sound), 914 phoneme, 915 phone model, 915 phonetic alphabet, 914 photometry, 932 photosensitive spot, 963 phrase structure, 888, 919 physicalism, 1028, 1041 physical symbol system, 18 Pi, X., 604, 1083 Piccione, C., 687, 1093 Pickwick, Mr., 1026 pictorial structure model, 958 PID controller, 999 Pieper, G., 360, 1092 pigeons, 13 Pijls, W., 191, 1085 pineal gland, 1027 Pineau, J., 686, 1013, 1085 Pinedo, M., 432, 1085 ping-pong, 32, 830 Index pinhole camera, 930 Pinkas, G., 229, 1085 Pinker, S., 287, 288, 314, 921, 1085, 1087 Pinto, D., 885, 1085 Pipatsrisawat, K., 277, 1085 Pippenger, N., 434, 1080 Pisa, tower of, 56 Pistore, M., 275, 1088 pit, bottomless, 237 Pitts, W., 15, 16, 20, 278, 727, 731, 761, 963, 1080, 1082 pixel, 930 PL-FC-ENTAILS?, 258 PL-RESOLUTION, 255 Plaat, A., 191, 1085 Place, U.",Safety,1
3185,"W., 1041, 1086 Q Q(s, a) (value of action in state), 843 Q-function, 627, 831 Q-learning, 831, 843, 844, 848, 973 Q-LEARNING-AGENT, 844 QA3, 314 QALY, 616, 637 Qi, R., 639, 1093 QUACKLE, 187 quadratic dynamical systems, 155 quadratic programming, 746 qualia, 1033 qualiﬁcation problem, 268, 481, 1024, 1025 qualitative physics, 444, 472 qualitative probabilistic network, 557, 624 quantiﬁcation, 903 quantiﬁer, 295, 313 existential, 297 in logic, 295–298 nested, 297–298 universal, 295–296, 322 quantization factor, 914 quasi-logical form, 904 Qubic, 194 query (logical), 301 query language, 867 query variable, 522 question answering, 872, 883 queue, 79 FIFO, 80, 81 LIFO, 80, 85 priority, 80, 858 Quevedo, T., 190 quiescence, 174 Quillian, M.",Functional,1
3186,"T., 554, 605, 1087 Rowland, J., 797, 1078 Rowley, H., 968, 1087 Roy, N., 1013, 1087 Rozonoer, L., 760, 1064 RPM, 541, 552 RSA (Rivest, Shamir, and Adelman), 356 RSAT, 277 Rubik’s Cube, 105 Rubin, D., 604, 605, 826, 827, 1070, 1073, 1087 Rubinstein, A., 688, 1084 rule, 244 causal, 317, 517 condition–action, 48 default, 459 diagnostic, 317, 517 if–then, 48, 244 implication, 244 situation–action, 48 uncertain, 548 rule-based system, 547, 1024 with uncertainty, 547–549 Rumelhart, D.",Constraint,1
3187,"D., 517, 553, 554, 559, 615, 634, 639, 687, 1071, 1088, 1090 shading, 933, 948, 952–953 shadow, 934 Shafer, G., 557, 1088 shaft decoder, 975 Shah, J., 967, 1083 Shahookar, K., 110, 1088 Shaked, T., 885, 1072 Shakey, 19, 60, 156, 393, 397, 434, 1011 Shalla, L., 359, 1092 Shanahan, M., 470, 1088 Shankar, N., 360, 1088 Shannon, C.",Constraint,4
3188,"H., 23, 557, 1067, 1088 shoulder (in state space), 123 Shpitser, I., 556, 1085 SHRDLU, 20, 23, 370 Shreve, S.",Constraint,4
3189,"J., 686, 1089 space complexity, 80, 108 spacecraft assembly, 432 spam detection, 865 spam email, 886 Sparck Jones, K., 505, 868, 884, 1087 sparse model, 721 sparse system, 515 SPASS, 359 spatial reasoning, 473 Index spatial substance, 447 specialization, 771, 772 species, 25, 130, 439–441, 469, 817, 860, 888, 948, 1035, 1042 spectrophotometry, 935 specularities, 933 specular reﬂection, 933 speech act, 904 speech recognition, 25, 912, 912–919, 922 sphex wasp, 39, 425 SPI (Symbolic Probabilistic Inference), 553 Spiegelhalter, D.",Safety,1
3190,"S., 192, 1084 Subramanian, D., 278, 472, 799, 1050, 1068, 1087, 1089 substance, 445 spatial, 447 temporal, 447 substitutability (of lotteries), 612 substitution, 301, 323 subsumption in description logic, 456 in resolution, 356 subsumption architecture, 1003 subsumption lattice, 329 successor-state axiom, 267, 279, 389 successor function, 67 Sudoku, 212 Sulawesi, 223 SUMMATION, 1053 summer’s day, 1026 summing out, 492, 527 sum of squared differences, 940 Sun Microsystems, 1036 Sunstein, C., 638, 1090 Sunter, A., 556, 1072 Superman, 286 superpixels, 942 supervised learning, 695, 846, 1025 support vector machine, 744, 744–748, 754 sure thing, 617 1128 surveillance, 1036 survey propagation, 278 survival of the ﬁttest, 605 Sussman, G.",Functional,1
3191,"B., 605, 1092 Svore, K., 884, 1090 Swade, D., 14, 1090 Swartz, R., 1022, 1067 Swedish, 33 Swerling, P., 604, 1090 Swift, T., 359, 1090 switching Kalman ﬁlter, 589, 608 syllogism, 4, 275 symbolic differentiation, 364 symbolic integration, 776 symmetry breaking (in CSPs), 226 synapse, 11 synchro drive, 976 synchronization, 427 synonymy, 465, 870 syntactic ambiguity, 905, 920 syntactic categories, 888 syntactic sugar, 304 syntactic theory (of knowledge), 470 syntax, 23, 240, 244 of logic, 274 of natural language, 888 of probability, 488 synthesis, 356 deductive, 356 synthesis of algorithms, 356 Syrj¨anen, T., 472, 1084, 1090 systems reply, 1031 Szafron, D., 678, 687, 1066, 1091 Szathm´ary, E., 155, 1089 Szepesvari, C., 194, 1078 T T (ﬂuent holds), 446 T-SCHED, 432 T4, 431 TABLE-DRIVEN-AGENT, 47 table lookup, 737 table tennis, 32 tabu search, 154, 222 tactile sensors, 974 Tadepalli, P., 799, 857, 1090 Tait, P.",Constraint,1
3192,"E., 189, 431, 434, 1092 Williams, B., 60, 278, 432, 472, 1083, 1092 Williams, C.",Constraint,4
3193,"I., 827, 1086 Williams, R., 640 Williams, R.",Constraint,4
3194,"J., 685, 761, 849, 855, 1085, 1087, 1092 Williamson, J., 469, 1083 Williamson, M., 433, 1072 Willighagen, E.",Constraint,4
